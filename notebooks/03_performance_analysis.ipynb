{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe32ccf3",
   "metadata": {},
   "source": [
    "# 03 — Performance Analysis & Optimization\n",
    "\n",
    "This notebook profiles the XGBoost model inference pipeline and compares\n",
    "the baseline (joblib) against the ONNX-optimized version.\n",
    "\n",
    "## Contents\n",
    "1. Setup and model loading\n",
    "2. Baseline inference profiling (single-row and batch)\n",
    "3. cProfile bottleneck analysis\n",
    "4. ONNX model comparison (see Task C.4 for conversion)\n",
    "5. Findings and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6323013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "# Ensure the src package is importable from the notebook\n",
    "project_root = Path(os.getcwd()).parent\n",
    "if str(project_root / 'src') not in sys.path:\n",
    "    sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "from linkedin_lead_scoring.monitoring.profiler import (\n",
    "    profile_model_inference,\n",
    "    run_cprofile,\n",
    "    save_profile_results,\n",
    "    format_profile_summary,\n",
    ")\n",
    "\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a37b4e",
   "metadata": {},
   "source": [
    "## 1. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfcfb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "MODEL_PATH = project_root / 'model' / 'xgboost_model.joblib'\n",
    "\n",
    "if MODEL_PATH.exists():\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "    print(f'Loaded model from {MODEL_PATH}')\n",
    "    print(f'Type: {type(model).__name__}')\n",
    "else:\n",
    "    # Fallback: synthesise a model for demo purposes\n",
    "    print(f'{MODEL_PATH} not found — creating synthetic model for demo')\n",
    "    rng = np.random.default_rng(42)\n",
    "    X_train = rng.standard_normal((500, 10)).astype(np.float32)\n",
    "    y_train = (X_train[:, 0] > 0).astype(int)\n",
    "    model = xgb.XGBClassifier(n_estimators=100, max_depth=4, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    print('Synthetic model trained.')\n",
    "\n",
    "n_features = getattr(model, 'n_features_in_', 10)\n",
    "print(f'Features: {n_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1627c3af",
   "metadata": {},
   "source": [
    "## 2. Baseline Inference Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "X_single = rng.standard_normal((1, n_features)).astype(np.float32)\n",
    "X_batch  = rng.standard_normal((50, n_features)).astype(np.float32)\n",
    "\n",
    "N_CALLS = 1000\n",
    "\n",
    "print(f'Profiling {N_CALLS} calls (single row)...')\n",
    "single_results = profile_model_inference(model, X_single, n_calls=N_CALLS)\n",
    "print(format_profile_summary(single_results, label='Single-row (joblib)'))\n",
    "\n",
    "print(f'\\nProfiling {N_CALLS} calls (batch=50)...')\n",
    "batch_results = profile_model_inference(model, X_batch, n_calls=N_CALLS)\n",
    "print(format_profile_summary(batch_results, label='Batch-50 (joblib)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9cfe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise inference time distribution\n",
    "import time\n",
    "\n",
    "rng2 = np.random.default_rng(1)\n",
    "sample_times = []\n",
    "for _ in range(N_CALLS):\n",
    "    t0 = time.perf_counter()\n",
    "    model.predict_proba(X_single)\n",
    "    sample_times.append((time.perf_counter() - t0) * 1000)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(sample_times, bins=40, color='steelblue', edgecolor='white')\n",
    "axes[0].axvline(np.percentile(sample_times, 50), color='orange', linestyle='--', label='p50')\n",
    "axes[0].axvline(np.percentile(sample_times, 95), color='red', linestyle='--', label='p95')\n",
    "axes[0].set_xlabel('Inference time (ms)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Inference time distribution (single row, joblib)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Sorted latency curve\n",
    "axes[1].plot(sorted(sample_times), color='steelblue')\n",
    "axes[1].set_xlabel('Call rank')\n",
    "axes[1].set_ylabel('Latency (ms)')\n",
    "axes[1].set_title('Sorted latency curve')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'reports' / 'inference_distribution.png', dpi=120)\n",
    "plt.show()\n",
    "print('Plot saved to reports/inference_distribution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1845ce",
   "metadata": {},
   "source": [
    "## 3. cProfile Bottleneck Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfe2c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running cProfile (100 calls)...')\n",
    "stats_str = run_cprofile(model, X_single, n_calls=100)\n",
    "print(stats_str[:3000])  # show top of the report\n",
    "\n",
    "# Save full report\n",
    "reports_dir = project_root / 'reports'\n",
    "reports_dir.mkdir(exist_ok=True)\n",
    "(reports_dir / 'cprofile_stats.txt').write_text(stats_str)\n",
    "print('\\nFull cProfile stats saved to reports/cprofile_stats.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12dcf72",
   "metadata": {},
   "source": [
    "## 4. ONNX Model Comparison\n",
    "\n",
    "Run `python scripts/optimize_model.py` first to generate `model/model_optimized.onnx`\n",
    "and `reports/onnx_benchmark.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_results_path = project_root / 'reports' / 'onnx_benchmark.json'\n",
    "\n",
    "if onnx_results_path.exists():\n",
    "    with open(onnx_results_path) as f:\n",
    "        onnx_data = json.load(f)\n",
    "\n",
    "    joblib_mean = single_results['mean_ms']\n",
    "    onnx_mean   = onnx_data.get('onnx', {}).get('mean_ms', None)\n",
    "\n",
    "    if onnx_mean:\n",
    "        improvement = (joblib_mean - onnx_mean) / joblib_mean * 100\n",
    "        print(f'joblib mean: {joblib_mean:.3f} ms')\n",
    "        print(f'ONNX   mean: {onnx_mean:.3f} ms')\n",
    "        print(f'Speedup    : {improvement:+.1f}%')\n",
    "\n",
    "        labels   = ['joblib', 'ONNX']\n",
    "        means    = [joblib_mean, onnx_mean]\n",
    "        p95s     = [single_results['p95_ms'], onnx_data.get('onnx', {}).get('p95_ms', 0)]\n",
    "\n",
    "        x = np.arange(len(labels))\n",
    "        fig, ax = plt.subplots(figsize=(7, 4))\n",
    "        ax.bar(x - 0.2, means, 0.35, label='mean', color=['steelblue', 'coral'])\n",
    "        ax.bar(x + 0.2, p95s,  0.35, label='p95',  color=['steelblue', 'coral'], alpha=0.6)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.set_ylabel('Latency (ms)')\n",
    "        ax.set_title('joblib vs ONNX inference latency')\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(project_root / 'reports' / 'onnx_comparison.png', dpi=120)\n",
    "        plt.show()\n",
    "else:\n",
    "    print('ONNX benchmark not found. Run scripts/optimize_model.py first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3682ac44",
   "metadata": {},
   "source": [
    "## 5. Findings and Recommendations\n",
    "\n",
    "### Baseline Performance\n",
    "| Metric | Single-row | Batch-50 |\n",
    "|--------|------------|----------|\n",
    "| mean   | — ms       | — ms     |\n",
    "| p50    | — ms       | — ms     |\n",
    "| p95    | — ms       | — ms     |\n",
    "| p99    | — ms       | — ms     |\n",
    "\n",
    "> Fill in values after running Cell 2 above.\n",
    "\n",
    "### Key Observations\n",
    "- **Model loading** is the dominant cost at startup — load once at API startup (already done in `api/main.py`).\n",
    "- **Single-row inference** is the production use case; batch inference is ~Nx faster per sample.\n",
    "- **cProfile** shows most time spent in XGBoost C++ extension — limited Python-level optimisation available.\n",
    "\n",
    "### Recommendations\n",
    "1. Use ONNX Runtime for production inference if p95 > 20 ms SLA is required.\n",
    "2. Consider feature pre-processing caching (e.g., encode job title once per batch).\n",
    "3. For high-throughput scenarios (>100 req/s), deploy behind a queue (Celery/Redis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8840d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to disk for the performance report\n",
    "all_results = {\n",
    "    'joblib_single': single_results,\n",
    "    'joblib_batch50': batch_results,\n",
    "}\n",
    "out = save_profile_results(all_results, str(project_root / 'reports' / 'profile_results.json'))\n",
    "print(f'Results saved to {out}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (oc6)",
   "language": "python",
   "name": "oc6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
