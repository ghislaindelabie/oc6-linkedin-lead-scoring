{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5tf714p48ew",
   "source": "# 04 — Data Drift Monitoring Analysis\n\n## LinkedIn Lead Scoring Model — MLOps Monitoring\n\nThis notebook demonstrates **data drift detection** for the LinkedIn Lead Scoring model.\nIt is a key deliverable for evaluating the operational monitoring capabilities of the ML pipeline.\n\n**Objectives:**\n1. Understand the training distribution (reference data)\n2. Detect data drift using Evidently AI on synthetic scenarios\n3. Analyse prediction drift when input distributions shift\n4. Establish monitoring thresholds and a decision framework for retraining\n\n**Model context:**\n- XGBoost classifier trained on 1,910 LemList campaign contacts\n- 47 features (19 base numeric + 28 one-hot encoded)\n- Best F1 score on validation: **0.556**\n- Drift detection powered by [Evidently AI](https://www.evidentlyai.com/)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "542v2yo0b99",
   "source": "## 1. Setup & Imports",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ltr2831e9nd",
   "source": "import sys\nimport os\nimport json\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport warnings\n\n# Add src to path for project imports\nPROJECT_ROOT = Path(os.getcwd()).parent\nsys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n\nfrom linkedin_lead_scoring.monitoring.drift import DriftDetector\n\n# Plotting configuration\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams[\"figure.dpi\"] = 100\nplt.rcParams[\"font.size\"] = 11\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)  # suppress scipy/numpy division warnings\n\n# Consistent colour palette\nREF_COLOR = \"#2196F3\"    # Blue for reference\nPROD_COLOR = \"#FF9800\"   # Orange for production/scenario\nALERT_COLOR = \"#F44336\"  # Red for alerts\nOK_COLOR = \"#4CAF50\"     # Green for no-drift\n\nprint(f\"Project root: {PROJECT_ROOT}\")\nprint(\"Setup complete.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "kg3sfwiz8qh",
   "source": "# Load reference data, model, and configuration\nreference_df = pd.read_csv(PROJECT_ROOT / \"data\" / \"reference\" / \"training_reference.csv\")\nmodel = joblib.load(PROJECT_ROOT / \"model\" / \"xgboost_model.joblib\")\n\npreprocessor_dict = joblib.load(PROJECT_ROOT / \"model\" / \"preprocessor.joblib\")\ntarget_encoder = preprocessor_dict[\"target_encoder\"]\nte_cols = preprocessor_dict[\"te_cols\"]\n\nwith open(PROJECT_ROOT / \"model\" / \"feature_columns.json\") as f:\n    feature_columns = json.load(f)\n\nwith open(PROJECT_ROOT / \"model\" / \"numeric_medians.json\") as f:\n    numeric_medians = json.load(f)\n\n# Import feature engineering for processing raw production data\nfrom linkedin_lead_scoring.features import preprocess_for_inference\n\n# Initialize drift detector with reference data\ndetector = DriftDetector(reference_data=reference_df)\n\nprint(f\"Reference data shape: {reference_df.shape}\")\nprint(f\"Number of features:   {len(feature_columns)}\")\nprint(f\"Model type:           {type(model).__name__}\")\nprint(f\"Target encoder cols:  {te_cols}\")\nprint(f\"Numeric medians:      {numeric_medians}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2m01lt6wyla",
   "source": "---\n\n## 2. Understanding the Training Distribution\n\nBefore monitoring for drift, we must understand the **reference distribution** -- the statistical\nprofile of features the model was trained on. Any future data that deviates significantly from\nthis baseline may indicate that the model's assumptions no longer hold.\n\nWe examine:\n- **Numeric features**: LLM quality scores, engagement scores, company founding year\n- **One-hot encoded features**: seniority level, geography, business type, company size",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "n6hjtn5s5o",
   "source": "# Summary statistics of the reference data\nreference_df.describe().round(3)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5ffvfa021cy",
   "source": "# Distribution of key numeric features\nkey_numeric = [\"llm_quality\", \"llm_engagement\", \"llm_decision_maker\", \"companyfoundedon\"]\n\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\nfor ax, col in zip(axes, key_numeric):\n    reference_df[col].hist(bins=20, ax=ax, color=REF_COLOR, edgecolor=\"white\", alpha=0.85)\n    ax.set_title(col, fontsize=12, fontweight=\"bold\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Count\")\n    ax.axvline(reference_df[col].median(), color=\"red\", linestyle=\"--\", linewidth=1.2, label=\"Median\")\n    ax.legend(fontsize=9)\n\nfig.suptitle(\"Reference Data — Key Numeric Feature Distributions\", fontsize=14, fontweight=\"bold\", y=1.03)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0y5luo3rakkn",
   "source": "# Categorical (OHE) feature distributions — grouped by category prefix\nohe_groups = {\n    \"Seniority (llm_seniority)\": [c for c in feature_columns if c.startswith(\"llm_seniority_\")],\n    \"Geography (llm_geography)\": [c for c in feature_columns if c.startswith(\"llm_geography_\")],\n    \"Business Type (llm_business_type)\": [c for c in feature_columns if c.startswith(\"llm_business_type_\")],\n    \"Company Size\": [c for c in feature_columns if c.startswith(\"companysize_\")],\n}\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\naxes = axes.flatten()\n\nfor ax, (group_name, cols) in zip(axes, ohe_groups.items()):\n    # Compute proportion of 1s in each OHE column (= category prevalence)\n    proportions = reference_df[cols].mean()\n    short_labels = [c.split(\"_\", 2)[-1] if \"_\" in c else c for c in cols]\n    bars = ax.bar(short_labels, proportions, color=REF_COLOR, edgecolor=\"white\", alpha=0.85)\n    ax.set_title(group_name, fontsize=12, fontweight=\"bold\")\n    ax.set_ylabel(\"Proportion\")\n    ax.set_ylim(0, 1.0)\n    ax.tick_params(axis=\"x\", rotation=30)\n    # Add value labels on bars\n    for bar, val in zip(bars, proportions):\n        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.02,\n                f\"{val:.0%}\", ha=\"center\", fontsize=9)\n\nfig.suptitle(\"Reference Data — Categorical Feature Distributions (OHE Prevalence)\",\n             fontsize=14, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "hfp6ngadmcg",
   "source": "---\n\n## 3. Data Drift Detection — Baseline (No Drift)\n\nWe begin with a **no-drift baseline** scenario: synthetic data generated from the same\ndistribution as the training set. This validates that the drift detector does **not produce\nfalse positives** when data is statistically similar to the reference.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "257e6b5l1ve",
   "source": "# Load baseline scenario\nbaseline_df = pd.read_csv(PROJECT_ROOT / \"data\" / \"drift_scenarios\" / \"no_drift_baseline.csv\")\nprint(f\"Baseline data shape: {baseline_df.shape}\")\n\n# Run drift detection\nbaseline_result = detector.detect_data_drift(baseline_df)\n\nprint(\"\\n--- Baseline Drift Detection Results ---\")\nprint(f\"  Drift detected:    {baseline_result['drift_detected']}\")\nprint(f\"  Drifted features:  {baseline_result['drifted_count']} / {baseline_result['total_features']}\")\nprint(f\"  Drift share:       {baseline_result['drift_share']:.1%}\")\nif baseline_result[\"drifted_features\"]:\n    print(f\"  Drifted columns:   {baseline_result['drifted_features']}\")\nelse:\n    print(\"  Drifted columns:   (none)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "y56z8atkq2p",
   "source": "# Side-by-side distributions: reference vs baseline for key features\ncompare_features = [\"llm_quality\", \"llm_engagement\", \"llm_decision_maker\"]\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\nfor ax, col in zip(axes, compare_features):\n    ax.hist(reference_df[col], bins=15, alpha=0.6, color=REF_COLOR, label=\"Reference\", edgecolor=\"white\")\n    ax.hist(baseline_df[col], bins=15, alpha=0.6, color=PROD_COLOR, label=\"Baseline\", edgecolor=\"white\")\n    ax.set_title(col, fontsize=12, fontweight=\"bold\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Count\")\n    ax.legend(fontsize=9)\n\nfig.suptitle(\"Reference vs No-Drift Baseline — Overlapping Distributions\",\n             fontsize=14, fontweight=\"bold\", y=1.03)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "mem7x1xfxy",
   "source": "**Interpretation:** The baseline scenario produces very low drift share and no overall drift alarm.\nThe distributions overlap closely with the reference, confirming that the detector is well-calibrated\nand does not trigger false positives on in-distribution data.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "4qrgapw9ow",
   "source": "---\n\n## 4. Data Drift Detection — Synthetic Scenarios\n\nWe now test four **intentionally drifted** scenarios, each simulating a realistic production shift:\n\n| Scenario | Description |\n|----------|-------------|\n| **Sector Shift** | New industries (healthcare, government) absent from training data |\n| **Seniority Shift** | Mostly junior profiles instead of the mid/senior mix in training |\n| **Geography Shift** | Contacts from different geographic regions |\n| **Quality Degradation** | Incomplete profiles (missing summaries, low LLM scores) |\n\nFor each scenario, we run Evidently's drift detection and examine which features are most affected.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "fotzl0bwzu9",
   "source": "# Load all drift scenarios\nscenario_files = {\n    \"Sector Shift\": \"drift_sector_shift.csv\",\n    \"Seniority Shift\": \"drift_seniority_shift.csv\",\n    \"Geography Shift\": \"drift_geography_shift.csv\",\n    \"Quality Degradation\": \"drift_quality_degradation.csv\",\n}\n\nscenarios = {}\nfor name, filename in scenario_files.items():\n    df = pd.read_csv(PROJECT_ROOT / \"data\" / \"drift_scenarios\" / filename)\n    scenarios[name] = df\n    print(f\"Loaded '{name}': {df.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "yzm8dj4p7kn",
   "source": "# Run drift detection on each scenario and collect results\ndrift_results = {}\nfor name, df in scenarios.items():\n    result = detector.detect_data_drift(df)\n    drift_results[name] = result\n\n# Also include the baseline for comparison\ndrift_results[\"No Drift (Baseline)\"] = baseline_result\n\n# Summary table\nsummary_rows = []\nfor name in [\"No Drift (Baseline)\"] + list(scenarios.keys()):\n    r = drift_results[name]\n    summary_rows.append({\n        \"Scenario\": name,\n        \"Drift Detected\": r[\"drift_detected\"],\n        \"Drifted Features\": r[\"drifted_count\"],\n        \"Total Features\": r[\"total_features\"],\n        \"Drift Share\": f\"{r['drift_share']:.1%}\",\n    })\n\nsummary_df = pd.DataFrame(summary_rows)\nprint(\"=== Data Drift Detection Summary ===\\n\")\nprint(summary_df.to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "obg9pb4cyy",
   "source": "# Visualize drift share across scenarios\nscenario_names = [r[\"Scenario\"] for r in summary_rows]\ndrift_shares = [drift_results[name][\"drift_share\"] for name in\n                [\"No Drift (Baseline)\"] + list(scenarios.keys())]\n\nfig, ax = plt.subplots(figsize=(10, 5))\ncolors = [OK_COLOR if ds < 0.2 else (PROD_COLOR if ds < 0.5 else ALERT_COLOR)\n          for ds in drift_shares]\nbars = ax.barh(scenario_names, [ds * 100 for ds in drift_shares], color=colors, edgecolor=\"white\")\n\n# Add threshold lines\nax.axvline(20, color=PROD_COLOR, linestyle=\"--\", linewidth=1.5, label=\"Investigate (20%)\")\nax.axvline(50, color=ALERT_COLOR, linestyle=\"--\", linewidth=1.5, label=\"Alert / Retrain (50%)\")\n\n# Labels on bars\nfor bar, ds in zip(bars, drift_shares):\n    ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height() / 2,\n            f\"{ds:.1%}\", va=\"center\", fontsize=10, fontweight=\"bold\")\n\nax.set_xlabel(\"Drift Share (%)\", fontsize=12)\nax.set_title(\"Data Drift Share by Scenario\", fontsize=14, fontweight=\"bold\")\nax.legend(loc=\"lower right\", fontsize=10)\nax.set_xlim(0, 100)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "r3pt14t2n4",
   "source": "# Detailed drifted features for each scenario\nfor name in scenarios.keys():\n    result = drift_results[name]\n    drifted = result[\"drifted_features\"]\n    print(f\"\\n{'='*60}\")\n    print(f\"  {name}\")\n    print(f\"  Drifted: {result['drifted_count']}/{result['total_features']} ({result['drift_share']:.1%})\")\n    print(f\"{'='*60}\")\n    if drifted:\n        for i, feat in enumerate(drifted[:10], 1):\n            print(f\"  {i:2d}. {feat}\")\n        if len(drifted) > 10:\n            print(f\"  ... and {len(drifted) - 10} more\")\n    else:\n        print(\"  (no individually drifted features detected)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "w9b0ay7t14e",
   "source": "# Visualize one key drifted feature per scenario (overlapping histograms)\n# We pick the most interpretable feature for each scenario type\nscenario_highlight_features = {\n    \"Sector Shift\": \"llm_industry\",\n    \"Seniority Shift\": \"llm_decision_maker\",\n    \"Geography Shift\": \"companylocation\",\n    \"Quality Degradation\": \"llm_quality\",\n}\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\naxes = axes.flatten()\n\nfor ax, (scenario_name, feat) in zip(axes, scenario_highlight_features.items()):\n    ref_vals = reference_df[feat]\n    scen_vals = scenarios[scenario_name][feat]\n\n    ax.hist(ref_vals, bins=15, alpha=0.6, color=REF_COLOR, label=\"Reference\", edgecolor=\"white\")\n    ax.hist(scen_vals, bins=15, alpha=0.6, color=PROD_COLOR, label=scenario_name, edgecolor=\"white\")\n\n    ax.set_title(f\"{scenario_name}\\nFeature: {feat}\", fontsize=11, fontweight=\"bold\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Count\")\n    ax.legend(fontsize=9)\n\nfig.suptitle(\"Key Drifted Feature per Scenario — Reference vs Scenario\",\n             fontsize=14, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "kn5vo95873b",
   "source": "### Scenario Interpretations\n\n- **Sector Shift**: The industry-related features (target-encoded `llm_industry`, `industry`, `companyindustry`) drift because the synthetic data introduces sectors (e.g., healthcare, government) that were underrepresented or absent in training. This simulates a business expansion into new verticals.\n\n- **Seniority Shift**: Features like `llm_decision_maker`, `llm_quality`, and the `llm_seniority_*` OHE columns shift drastically. This simulates targeting more junior contacts who have different profile completeness patterns.\n\n- **Geography Shift**: Location-related features (`companylocation`, `location`, `llm_geography_*`) shift as contacts come from different regions. This is common when expanding to new markets.\n\n- **Quality Degradation**: `llm_quality` drops, `has_summary` goes to 0, and `skills_count` decreases. This simulates a scenario where enrichment quality degrades (e.g., LLM API issues, scraping failures).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "3xwc9dzj70m",
   "source": "---\n\n## 5. Prediction Drift Analysis\n\nData drift does not always translate into prediction drift. A model may be robust to certain\ndistributional shifts if the affected features are not strongly weighted.\n\nHere we:\n1. Generate model predictions (probability scores) for the reference and each scenario\n2. Use a **Kolmogorov-Smirnov (KS) test** to compare prediction score distributions\n3. Visualize how predicted lead scores shift across scenarios",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2dyxqr86ms6",
   "source": "# Generate predictions on reference data\n# The data is already in the 47-feature format, so we can predict directly\nref_features = reference_df[feature_columns]\nref_scores = model.predict_proba(ref_features)[:, 1]\n\nprint(f\"Reference prediction scores:\")\nprint(f\"  Mean:   {ref_scores.mean():.3f}\")\nprint(f\"  Median: {np.median(ref_scores):.3f}\")\nprint(f\"  Std:    {ref_scores.std():.3f}\")\nprint(f\"  Range:  [{ref_scores.min():.3f}, {ref_scores.max():.3f}]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "pogqvbiho7q",
   "source": "# Prediction drift analysis for each scenario\nall_scenario_names = [\"No Drift (Baseline)\"] + list(scenarios.keys())\nall_scenario_data = {\"No Drift (Baseline)\": baseline_df, **scenarios}\n\nprediction_results = {}\nfor name in all_scenario_names:\n    df = all_scenario_data[name]\n    scen_features = df[feature_columns]\n    scen_scores = model.predict_proba(scen_features)[:, 1]\n\n    pred_drift = detector.detect_prediction_drift(ref_scores, scen_scores)\n    prediction_results[name] = {\n        \"scores\": scen_scores,\n        **pred_drift,\n    }\n\n# Summary table\npred_rows = []\nfor name in all_scenario_names:\n    r = prediction_results[name]\n    pred_rows.append({\n        \"Scenario\": name,\n        \"KS Statistic\": f\"{r['statistic']:.4f}\",\n        \"p-value\": f\"{r['p_value']:.4e}\" if r[\"p_value\"] < 0.001 else f\"{r['p_value']:.4f}\",\n        \"Drift Detected\": r[\"drift_detected\"],\n        \"Mean Score\": f\"{r['scores'].mean():.3f}\",\n    })\n\npred_summary_df = pd.DataFrame(pred_rows)\nprint(\"=== Prediction Drift Summary (KS Test, threshold=0.05) ===\\n\")\nprint(pred_summary_df.to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ee1awqvlfok",
   "source": "# Overlapping histograms of prediction score distributions\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\naxes = axes.flatten()\n\ndrift_scenario_names = list(scenarios.keys())\nfor ax, name in zip(axes, drift_scenario_names):\n    r = prediction_results[name]\n    color = ALERT_COLOR if r[\"drift_detected\"] else PROD_COLOR\n\n    ax.hist(ref_scores, bins=20, alpha=0.5, color=REF_COLOR, label=\"Reference\", edgecolor=\"white\")\n    ax.hist(r[\"scores\"], bins=20, alpha=0.5, color=color, label=name, edgecolor=\"white\")\n\n    status = \"DRIFT\" if r[\"drift_detected\"] else \"OK\"\n    ax.set_title(f\"{name}\\nKS={r['statistic']:.3f}, p={r['p_value']:.3e} [{status}]\",\n                 fontsize=10, fontweight=\"bold\")\n    ax.set_xlabel(\"Predicted Probability (engagement)\")\n    ax.set_ylabel(\"Count\")\n    ax.legend(fontsize=9)\n\nfig.suptitle(\"Prediction Score Distributions — Reference vs Each Scenario\",\n             fontsize=14, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "sv2mvm59oy",
   "source": "### Key Observations\n\n- **Data drift does not always cause prediction drift.** Some feature shifts may affect columns\n  the model does not weight heavily.\n- **Prediction drift is the stronger signal** for deciding when to retrain, because it directly\n  measures whether the model's output behaviour has changed.\n- Scenarios with both high data drift *and* prediction drift (p < 0.05) are the most concerning\n  and should trigger a retraining investigation.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "sw8wrwgzgc",
   "source": "---\n\n## 6. Real Production Data — Drift Analysis\n\nWe now apply drift detection on **real LinkedIn contacts** that were processed through the same\nLLM enrichment pipeline (notebook 01) as the training data. These are actual business development\ncontacts — not synthetic data.\n\n**Data source:** 151 labelled contacts from 3 LemList campaign exports, enriched via gpt-4o-mini,\nwith known engagement outcomes (47.7% engagement rate).\n\nThis is the most important section — it shows whether our model faces **real drift** in production.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "91y9jscbaee",
   "source": "# Load real production data (20-column enriched format, same as training)\nproduction_raw = pd.read_csv(PROJECT_ROOT / \"data\" / \"production\" / \"new_contacts_enriched.csv\")\nprint(f\"Production data: {production_raw.shape}\")\nprint(f\"Engagement rate: {production_raw['engaged'].mean():.1%}\")\nprint(f\"  Engaged: {production_raw['engaged'].sum()}, Not engaged: {(1 - production_raw['engaged']).sum():.0f}\")\n\n# Preprocess to 47-feature format (same as training_reference.csv)\nproduction_features = preprocess_for_inference(\n    production_raw.drop(columns=[\"engaged\"]),\n    target_encoder=target_encoder,\n    te_cols=te_cols,\n    feature_columns=feature_columns,\n    numeric_medians=numeric_medians,\n)\nprint(f\"\\nProcessed features: {production_features.shape}\")\nprint(f\"Columns match reference: {list(production_features.columns) == feature_columns}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "31oatn5iu3m",
   "source": "# Data drift detection on real production data\nprod_drift = detector.detect_data_drift(production_features)\n\nprint(\"=== REAL PRODUCTION DATA — Drift Detection ===\\n\")\nprint(f\"  Drift detected:    {prod_drift['drift_detected']}\")\nprint(f\"  Drift share:       {prod_drift['drift_share']:.1%}\")\nprint(f\"  Drifted features:  {prod_drift['drifted_count']} / {prod_drift['total_features']}\")\nprint(f\"\\n  Drifted feature list:\")\nfor i, feat in enumerate(prod_drift[\"drifted_features\"], 1):\n    print(f\"    {i:2d}. {feat}\")\n\n# Prediction drift on real data\nprod_scores = model.predict_proba(production_features[feature_columns])[:, 1]\nprod_pred_drift = detector.detect_prediction_drift(ref_scores, prod_scores)\n\nprint(f\"\\n=== Prediction Drift ===\")\nprint(f\"  KS Statistic:  {prod_pred_drift['statistic']:.4f}\")\nprint(f\"  p-value:       {prod_pred_drift['p_value']:.4e}\")\nprint(f\"  Drift:         {prod_pred_drift['drift_detected']}\")\nprint(f\"\\n  Production scores: mean={prod_scores.mean():.3f}, std={prod_scores.std():.3f}\")\nprint(f\"  Reference scores:  mean={ref_scores.mean():.3f}, std={ref_scores.std():.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qhqxys6kmnc",
   "source": "# Visualize real production drift\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\n# Top row: key numeric feature distributions (reference vs production)\ntop_features = [\"llm_quality\", \"llm_engagement\", \"llm_decision_maker\"]\nfor ax, col in zip(axes[0], top_features):\n    drifted = col in prod_drift[\"drifted_features\"]\n    border_color = ALERT_COLOR if drifted else OK_COLOR\n    ax.hist(reference_df[col], bins=15, alpha=0.5, color=REF_COLOR, label=\"Reference (training)\", edgecolor=\"white\")\n    ax.hist(production_features[col], bins=15, alpha=0.5, color=PROD_COLOR, label=\"Production (real)\", edgecolor=\"white\")\n    status = \"DRIFTED\" if drifted else \"OK\"\n    ax.set_title(f\"{col} [{status}]\", fontsize=11, fontweight=\"bold\", color=border_color)\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Count\")\n    ax.legend(fontsize=9)\n\n# Bottom left: prediction score distributions\nax = axes[1][0]\nax.hist(ref_scores, bins=20, alpha=0.5, color=REF_COLOR, label=\"Reference\", edgecolor=\"white\")\nax.hist(prod_scores, bins=20, alpha=0.5, color=PROD_COLOR, label=\"Production\", edgecolor=\"white\")\ndrift_label = \"DRIFT\" if prod_pred_drift[\"drift_detected\"] else \"OK\"\nax.set_title(f\"Prediction Scores [{drift_label}]\\nKS={prod_pred_drift['statistic']:.3f}, p={prod_pred_drift['p_value']:.3e}\",\n             fontsize=11, fontweight=\"bold\")\nax.set_xlabel(\"Engagement Probability\")\nax.set_ylabel(\"Count\")\nax.legend(fontsize=9)\n\n# Bottom middle: drift share comparison (production vs synthetic scenarios)\nax = axes[1][1]\nall_names = [\"Baseline\", \"Production\\n(REAL)\", \"Sector\", \"Seniority\", \"Geography\", \"Quality\"]\nall_shares = [\n    baseline_result[\"drift_share\"],\n    prod_drift[\"drift_share\"],\n    drift_results[\"Sector Shift\"][\"drift_share\"],\n    drift_results[\"Seniority Shift\"][\"drift_share\"],\n    drift_results[\"Geography Shift\"][\"drift_share\"],\n    drift_results[\"Quality Degradation\"][\"drift_share\"],\n]\ncolors = [OK_COLOR if s < 0.2 else (PROD_COLOR if s < 0.5 else ALERT_COLOR) for s in all_shares]\ncolors[1] = \"#E91E63\"  # Highlight production in pink\nbars = ax.bar(all_names, [s * 100 for s in all_shares], color=colors, edgecolor=\"white\")\nax.axhline(50, color=ALERT_COLOR, linestyle=\"--\", linewidth=1.5, label=\"Alert threshold (50%)\")\nax.axhline(20, color=PROD_COLOR, linestyle=\"--\", linewidth=1.2, label=\"Investigate (20%)\")\nfor bar, s in zip(bars, all_shares):\n    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f\"{s:.0%}\",\n            ha=\"center\", fontsize=9, fontweight=\"bold\")\nax.set_ylabel(\"Drift Share (%)\")\nax.set_title(\"Drift Share Comparison\", fontsize=11, fontweight=\"bold\")\nax.legend(fontsize=8)\nax.set_ylim(0, 100)\nax.tick_params(axis=\"x\", rotation=15)\n\n# Bottom right: OHE category comparison\nax = axes[1][2]\nohe_cats = [\"companysize_UNKNOWN\", \"companytype_Privately Held\", \"companytype_UNKNOWN\",\n            \"llm_seniority_Senior\", \"llm_business_type_others\"]\nref_vals = reference_df[ohe_cats].mean()\nprod_vals = production_features[ohe_cats].mean()\nx = np.arange(len(ohe_cats))\nshort_labels = [c.split(\"_\", 1)[-1][:20] for c in ohe_cats]\nax.bar(x - 0.2, ref_vals, 0.35, color=REF_COLOR, label=\"Reference\", edgecolor=\"white\")\nax.bar(x + 0.2, prod_vals, 0.35, color=PROD_COLOR, label=\"Production\", edgecolor=\"white\")\nax.set_xticks(x)\nax.set_xticklabels(short_labels, fontsize=8, rotation=30, ha=\"right\")\nax.set_ylabel(\"Proportion\")\nax.set_title(\"Key Category Shifts\", fontsize=11, fontweight=\"bold\")\nax.legend(fontsize=9)\n\nfig.suptitle(\"REAL PRODUCTION DATA — Drift Analysis (151 contacts)\",\n             fontsize=15, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "r1sxo38igy",
   "source": "---\n\n## 7. Concept Drift — Ground Truth Analysis\n\n**Concept drift** differs from data drift: the **relationship between features and the target**\nmay have changed, even if feature distributions are stable.\n\nWe have **151 labelled contacts** with actual engagement outcomes. This allows us to:\n1. Compare the model's predictions against ground truth\n2. Measure real-world F1, precision, recall on unseen data\n3. Determine whether concept drift has occurred (F1 drop from training baseline of 0.556)\n\nThis is the strongest evidence for or against retraining.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "zov1rxvnvc",
   "source": [
    "EM_DASH = '\\u2014'\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "# Ground truth from the production data\n",
    "actuals = production_raw[\"engaged\"].values\n",
    "predictions_proba = prod_scores\n",
    "predictions_binary = (predictions_proba >= 0.5).astype(int)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(actuals, predictions_binary)\n",
    "precision = precision_score(actuals, predictions_binary)\n",
    "recall = recall_score(actuals, predictions_binary)\n",
    "\n",
    "print(\"=== MODEL PERFORMANCE ON REAL PRODUCTION DATA ===\\n\")\n",
    "print(f\"  Contacts evaluated:  {len(actuals)}\")\n",
    "print(f\"  Actual engaged:      {actuals.sum()} ({actuals.mean():.1%})\")\n",
    "print(f\"  Predicted engaged:   {predictions_binary.sum()} ({predictions_binary.mean():.1%})\")\n",
    "print()\n",
    "print(f\"  {'Metric':<15} {'Training':>10} {'Production':>12} {'Change':>10}\")\n",
    "print(f\"  {'-'*47}\")\n",
    "print(f\"  {'F1 Score':<15} {'0.556':>10} {f1:>12.3f} {(f1 - 0.556) / 0.556:>+10.1%}\")\n",
    "print(f\"  {'Precision':<15} {'---':>10} {precision:>12.3f}\")\n",
    "print(f\"  {'Recall':<15} {'---':>10} {recall:>12.3f}\")\n",
    "\n",
    "# Concept drift assessment\n",
    "f1_drop = (0.556 - f1) / 0.556\n",
    "if f1_drop > 0.20:\n",
    "    print(f\"\\n  ⚠️  ALERT: F1 dropped {f1_drop:.0%} from training baseline.\")\n",
    "    print(f\"     This indicates concept drift. Retraining is recommended.\")\n",
    "elif f1_drop > 0.10:\n",
    "    print(f\"\\n  ⚠️  WARNING: F1 dropped {f1_drop:.0%}. Monitor closely.\")\n",
    "else:\n",
    "    print(f\"\\n  ✅ F1 is within acceptable range of training performance.\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ofbhe4l5lhi",
   "source": "# Confusion matrix and detailed classification report\nprint(\"=== Confusion Matrix ===\\n\")\ncm = confusion_matrix(actuals, predictions_binary)\nprint(f\"                    Predicted\")\nprint(f\"                 Not Eng.  Engaged\")\nprint(f\"  Actual Not Eng.  {cm[0][0]:>5}    {cm[0][1]:>5}\")\nprint(f\"  Actual Engaged   {cm[1][0]:>5}    {cm[1][1]:>5}\")\nprint()\nprint(classification_report(actuals, predictions_binary, target_names=[\"Not Engaged\", \"Engaged\"]))\n\n# Visualization: confusion matrix + score distribution by actual label\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Confusion matrix heatmap\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1,\n            xticklabels=[\"Not Engaged\", \"Engaged\"],\n            yticklabels=[\"Not Engaged\", \"Engaged\"])\nax1.set_xlabel(\"Predicted\", fontsize=12)\nax1.set_ylabel(\"Actual\", fontsize=12)\nax1.set_title(\"Confusion Matrix — Production Data\", fontsize=13, fontweight=\"bold\")\n\n# Score distribution by actual label\nengaged_scores = predictions_proba[actuals == 1]\nnot_engaged_scores = predictions_proba[actuals == 0]\nax2.hist(not_engaged_scores, bins=15, alpha=0.6, color=\"#F44336\", label=\"Actually Not Engaged\", edgecolor=\"white\")\nax2.hist(engaged_scores, bins=15, alpha=0.6, color=\"#4CAF50\", label=\"Actually Engaged\", edgecolor=\"white\")\nax2.axvline(0.5, color=\"black\", linestyle=\"--\", linewidth=2, label=\"Decision threshold (0.5)\")\nax2.set_xlabel(\"Predicted Engagement Probability\", fontsize=12)\nax2.set_ylabel(\"Count\", fontsize=12)\nax2.set_title(\"Score Distribution by Actual Outcome\", fontsize=13, fontweight=\"bold\")\nax2.legend(fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nKey insight: The model scores actually-engaged contacts at {engaged_scores.mean():.3f} on average,\")\nprint(f\"vs {not_engaged_scores.mean():.3f} for not-engaged. Separation exists but is weaker than training.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "046d15b91eca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. API Inference Demo\n",
    "\n",
    "To demonstrate the deployed model works end-to-end, we send real production data through\n",
    "the **deployed API** on Hugging Face Spaces.\n",
    "\n",
    "This validates:\n",
    "1. The API accepts real LinkedIn profile data\n",
    "2. Predictions match local inference (model consistency)\n",
    "3. The batch endpoint handles production-scale requests\n",
    "\n",
    "**API endpoint:** `https://ghislaindelabie-oc6-bizdev-ml-api.hf.space/predict/batch`"
   ]
  },
  {
   "cell_type": "code",
   "id": "0a818ff6865d",
   "metadata": {},
   "source": [
    "import httpx\n",
    "import time\n",
    "\n",
    "API_URL = \"https://ghislaindelabie-oc6-bizdev-ml-api.hf.space\"\n",
    "BATCH_ENDPOINT = f\"{API_URL}/predict/batch\"\n",
    "\n",
    "# Prepare leads from labelled production data (151 contacts)\n",
    "# The API expects the 19 raw fields (not the 47 processed features)\n",
    "api_fields = [\n",
    "    \"jobtitle\", \"industry\", \"companyindustry\", \"companysize\", \"companytype\",\n",
    "    \"companyfoundedon\", \"location\", \"companylocation\", \"llm_seniority\",\n",
    "    \"llm_quality\", \"llm_engagement\", \"llm_decision_maker\", \"llm_company_fit\",\n",
    "    \"llm_geography\", \"llm_business_type\", \"languages\", \"summary\", \"skills\",\n",
    "    \"llm_industry\",\n",
    "]\n",
    "\n",
    "# Convert production data to API-compatible format\n",
    "leads_for_api = []\n",
    "for _, row in production_raw.iterrows():\n",
    "    lead = {}\n",
    "    for field in api_fields:\n",
    "        if field in row.index and pd.notna(row[field]):\n",
    "            val = row[field]\n",
    "            # Convert numpy types to Python types for JSON serialization\n",
    "            if hasattr(val, \"item\"):\n",
    "                val = val.item()\n",
    "            lead[field] = val\n",
    "    leads_for_api.append(lead)\n",
    "\n",
    "print(f\"Prepared {len(leads_for_api)} leads for API\")\n",
    "print(f\"Sample lead keys: {list(leads_for_api[0].keys())[:10]}...\")\n",
    "\n",
    "# Send batch request\n",
    "try:\n",
    "    print(f\"\\nSending batch to {BATCH_ENDPOINT}...\")\n",
    "    start = time.time()\n",
    "    with httpx.Client(timeout=120.0) as client:\n",
    "        response = client.post(BATCH_ENDPOINT, json={\"leads\": leads_for_api})\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        api_result = response.json()\n",
    "        api_predictions = api_result[\"predictions\"]\n",
    "        api_scores = np.array([p[\"score\"] for p in api_predictions])\n",
    "\n",
    "        print(f\"\\u2713 API responded in {elapsed:.1f}s\")\n",
    "        print(f\"  Total predictions: {api_result['total_count']}\")\n",
    "        print(f\"  Average score:     {api_result['avg_score']:.3f}\")\n",
    "        print(f\"  High engagement:   {api_result['high_engagement_count']}\")\n",
    "        API_AVAILABLE = True\n",
    "    else:\n",
    "        print(f\"\\u2717 API returned status {response.status_code}: {response.text[:200]}\")\n",
    "        API_AVAILABLE = False\n",
    "except Exception as e:\n",
    "    print(f\"\\u2717 API unavailable: {e}\")\n",
    "    print(\"  Continuing with local predictions only.\")\n",
    "    API_AVAILABLE = False"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a3e32c1cbd54",
   "metadata": {},
   "source": [
    "# Compare API predictions vs local inference\n",
    "if API_AVAILABLE:\n",
    "    local_scores = prod_scores  # computed in section 6\n",
    "\n",
    "    print(\"=== API vs Local Prediction Comparison ===\\n\")\n",
    "    print(f\"  {'Metric':<25} {'Local':>10} {'API':>10} {'Diff':>10}\")\n",
    "    print(f\"  {'-'*55}\")\n",
    "    print(f\"  {'Mean score':<25} {local_scores.mean():>10.4f} {api_scores.mean():>10.4f} {abs(local_scores.mean() - api_scores.mean()):>10.4f}\")\n",
    "    print(f\"  {'Std score':<25} {local_scores.std():>10.4f} {api_scores.std():>10.4f} {abs(local_scores.std() - api_scores.std()):>10.4f}\")\n",
    "    print(f\"  {'Max abs difference':<25} {np.max(np.abs(local_scores - api_scores)):>10.6f}\")\n",
    "    print(f\"  {'Correlation':<25} {np.corrcoef(local_scores, api_scores)[0,1]:>10.6f}\")\n",
    "\n",
    "    # Scatter plot: local vs API\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    ax1.scatter(local_scores, api_scores, alpha=0.5, s=20, color=\"#2196F3\")\n",
    "    ax1.plot([0, 1], [0, 1], \"r--\", linewidth=1.5, label=\"Perfect agreement\")\n",
    "    ax1.set_xlabel(\"Local Prediction Score\", fontsize=12)\n",
    "    ax1.set_ylabel(\"API Prediction Score\", fontsize=12)\n",
    "    ax1.set_title(\"Local vs API Predictions\", fontsize=13, fontweight=\"bold\")\n",
    "    ax1.legend()\n",
    "    ax1.set_xlim(-0.05, 1.05)\n",
    "    ax1.set_ylim(-0.05, 1.05)\n",
    "\n",
    "    # Histogram of differences\n",
    "    diffs = api_scores - local_scores\n",
    "    ax2.hist(diffs, bins=30, color=\"#FF9800\", edgecolor=\"white\", alpha=0.85)\n",
    "    ax2.axvline(0, color=\"red\", linestyle=\"--\", linewidth=1.5)\n",
    "    ax2.set_xlabel(\"Score Difference (API - Local)\", fontsize=12)\n",
    "    ax2.set_ylabel(\"Count\", fontsize=12)\n",
    "    ax2.set_title(f\"Prediction Differences (max |\\u0394| = {np.max(np.abs(diffs)):.4f})\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Agreement on classification\n",
    "    local_labels = (local_scores >= 0.5).astype(int)\n",
    "    api_labels = (api_scores >= 0.5).astype(int)\n",
    "    agreement = (local_labels == api_labels).mean()\n",
    "    print(f\"\\n  Classification agreement: {agreement:.1%}\")\n",
    "    print(f\"  Disagreements: {(local_labels != api_labels).sum()} out of {len(local_labels)}\")\n",
    "else:\n",
    "    print(\"API was not available. Predictions from local inference (\\u00a76) are used throughout.\")\n",
    "    print(\"In production, the API serves the same model with identical preprocessing.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "989337d5fc13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Unlabelled Production Data — Drift Analysis\n",
    "\n",
    "We now analyse **277 unlabelled contacts** — a separate batch of LinkedIn profiles with\n",
    "**no engagement outcome** (no ground truth). This is the typical production scenario:\n",
    "new contacts arrive, we score them, and we can only detect **data drift** and **prediction\n",
    "drift** (not concept drift, since we have no labels).\n",
    "\n",
    "This section demonstrates monitoring capability on purely unseen, unlabelled data."
   ]
  },
  {
   "cell_type": "code",
   "id": "d796287f446f",
   "metadata": {},
   "source": [
    "# Load unlabelled production data (needs JSON post-processing for LLM fields)\n",
    "import json as _json\n",
    "\n",
    "unlabelled_raw = pd.read_csv(PROJECT_ROOT / \"data\" / \"production\" / \"unlabelled_contacts_enriched.csv\")\n",
    "print(f\"Unlabelled data: {unlabelled_raw.shape}\")\n",
    "\n",
    "# Post-process: parse JSON strings in LLM numeric fields\n",
    "parse_map = {\n",
    "    \"llm_quality\": \"quality_score\",\n",
    "    \"llm_engagement\": \"engagement_score\",\n",
    "    \"llm_decision_maker\": \"decision_maker_score\",\n",
    "}\n",
    "\n",
    "for col, key in parse_map.items():\n",
    "    if col in unlabelled_raw.columns:\n",
    "        def _extract(val, _key=key):\n",
    "            if pd.isna(val):\n",
    "                return np.nan\n",
    "            if isinstance(val, (int, float)):\n",
    "                return val\n",
    "            try:\n",
    "                parsed = _json.loads(val) if isinstance(val, str) else val\n",
    "                if isinstance(parsed, dict):\n",
    "                    return parsed.get(_key, np.nan)\n",
    "                return float(val)\n",
    "            except (ValueError, _json.JSONDecodeError):\n",
    "                return np.nan\n",
    "        unlabelled_raw[col] = unlabelled_raw[col].apply(_extract)\n",
    "\n",
    "print(f\"  Post-processed LLM fields: {list(parse_map.keys())}\")\n",
    "\n",
    "# Select the 19 modeling columns (same schema as training data, minus 'engaged')\n",
    "model_input_cols = [c for c in production_raw.columns if c != \"engaged\"]\n",
    "unlabelled_for_model = pd.DataFrame()\n",
    "for col in model_input_cols:\n",
    "    if col in unlabelled_raw.columns:\n",
    "        unlabelled_for_model[col] = unlabelled_raw[col].values\n",
    "    else:\n",
    "        unlabelled_for_model[col] = np.nan\n",
    "\n",
    "print(f\"  Prepared for model: {unlabelled_for_model.shape}\")\n",
    "\n",
    "# Preprocess to 47-feature format\n",
    "unlabelled_features = preprocess_for_inference(\n",
    "    unlabelled_for_model,\n",
    "    target_encoder=target_encoder,\n",
    "    te_cols=te_cols,\n",
    "    feature_columns=feature_columns,\n",
    "    numeric_medians=numeric_medians,\n",
    ")\n",
    "print(f\"  Processed features: {unlabelled_features.shape}\")\n",
    "print(f\"  Columns match reference: {list(unlabelled_features.columns) == feature_columns}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "24cdf9c5a20b",
   "metadata": {},
   "source": [
    "# Data drift detection on unlabelled data\n",
    "unlabelled_drift = detector.detect_data_drift(unlabelled_features)\n",
    "\n",
    "print(\"=== UNLABELLED DATA (277 contacts) \\u2014 Drift Detection ===\\n\")\n",
    "print(f\"  Drift detected:    {unlabelled_drift['drift_detected']}\")\n",
    "print(f\"  Drift share:       {unlabelled_drift['drift_share']:.1%}\")\n",
    "print(f\"  Drifted features:  {unlabelled_drift['drifted_count']} / {unlabelled_drift['total_features']}\")\n",
    "\n",
    "if unlabelled_drift[\"drifted_features\"]:\n",
    "    print(f\"\\n  Drifted feature list:\")\n",
    "    for i, feat in enumerate(unlabelled_drift[\"drifted_features\"][:10], 1):\n",
    "        print(f\"    {i:2d}. {feat}\")\n",
    "    remaining = len(unlabelled_drift[\"drifted_features\"]) - 10\n",
    "    if remaining > 0:\n",
    "        print(f\"    ... and {remaining} more\")\n",
    "\n",
    "# Predictions (v1 model)\n",
    "unlabelled_scores = model.predict_proba(unlabelled_features[feature_columns])[:, 1]\n",
    "unlabelled_pred_drift = detector.detect_prediction_drift(ref_scores, unlabelled_scores)\n",
    "\n",
    "print(f\"\\n  Prediction drift:\")\n",
    "print(f\"    KS Statistic: {unlabelled_pred_drift['statistic']:.4f}\")\n",
    "print(f\"    p-value:      {unlabelled_pred_drift['p_value']:.4e}\")\n",
    "print(f\"    Drift:        {unlabelled_pred_drift['drift_detected']}\")\n",
    "print(f\"\\n  Unlabelled scores: mean={unlabelled_scores.mean():.3f}, std={unlabelled_scores.std():.3f}\")\n",
    "print(f\"  Predicted engaged: {(unlabelled_scores >= 0.5).sum()} / {len(unlabelled_scores)} ({(unlabelled_scores >= 0.5).mean():.1%})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0891aaeaddc4",
   "metadata": {},
   "source": [
    "# Visualize: compare reference vs labelled production vs unlabelled production\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Prediction score distributions (3-way comparison)\n",
    "ax = axes[0]\n",
    "ax.hist(ref_scores, bins=20, alpha=0.4, color=REF_COLOR, label=\"Reference (training)\", edgecolor=\"white\")\n",
    "ax.hist(prod_scores, bins=20, alpha=0.4, color=PROD_COLOR, label=\"Labelled (151)\", edgecolor=\"white\")\n",
    "ax.hist(unlabelled_scores, bins=20, alpha=0.4, color=\"#9C27B0\", label=\"Unlabelled (277)\", edgecolor=\"white\")\n",
    "ax.set_xlabel(\"Engagement Probability\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Prediction Score Distributions\", fontsize=12, fontweight=\"bold\")\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Drift share comparison\n",
    "ax = axes[1]\n",
    "names = [\"Labelled\\n(151)\", \"Unlabelled\\n(277)\"]\n",
    "shares = [prod_drift[\"drift_share\"], unlabelled_drift[\"drift_share\"]]\n",
    "colors = [ALERT_COLOR if s >= 0.5 else (PROD_COLOR if s >= 0.2 else OK_COLOR) for s in shares]\n",
    "bars = ax.bar(names, [s * 100 for s in shares], color=colors, edgecolor=\"white\", width=0.5)\n",
    "ax.axhline(50, color=ALERT_COLOR, linestyle=\"--\", linewidth=1.5, label=\"Alert (50%)\")\n",
    "ax.axhline(20, color=PROD_COLOR, linestyle=\"--\", linewidth=1.2, label=\"Investigate (20%)\")\n",
    "for bar, s in zip(bars, shares):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1.5,\n",
    "            f\"{s:.0%}\", ha=\"center\", fontsize=11, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Drift Share (%)\")\n",
    "ax.set_title(\"Data Drift: Labelled vs Unlabelled\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_ylim(0, 100)\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Key feature comparison (3-way)\n",
    "ax = axes[2]\n",
    "feat = \"llm_quality\"\n",
    "ax.hist(reference_df[feat], bins=15, alpha=0.4, color=REF_COLOR, label=\"Reference\", edgecolor=\"white\")\n",
    "ax.hist(production_features[feat], bins=15, alpha=0.4, color=PROD_COLOR, label=\"Labelled\", edgecolor=\"white\")\n",
    "ax.hist(unlabelled_features[feat], bins=15, alpha=0.4, color=\"#9C27B0\", label=\"Unlabelled\", edgecolor=\"white\")\n",
    "ax.set_xlabel(feat)\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(f\"Feature Distribution: {feat}\", fontsize=12, fontweight=\"bold\")\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "fig.suptitle(\"Production Data Overview \\u2014 Labelled (151) vs Unlabelled (277)\",\n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Labelled (151):   drift={prod_drift['drift_share']:.0%}, mean_score={prod_scores.mean():.3f}\")\n",
    "print(f\"  Unlabelled (277): drift={unlabelled_drift['drift_share']:.0%}, mean_score={unlabelled_scores.mean():.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f5717c8f8b4c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Model Retraining — v1 \\u2192 v2\n",
    "\n",
    "The drift analysis confirms significant concept drift:\n",
    "- **Data drift**: 61.7% of features drifted on labelled production data\n",
    "- **Concept drift**: F1 dropped from 0.556 (training) to 0.418 (production) \\u2014 a 25% decline\n",
    "- **Recommendation**: RETRAIN with additional labelled data\n",
    "\n",
    "We now retrain the model by merging the original training data (303 rows) with the new\n",
    "labelled production data (151 rows), giving us **454 labelled contacts**.\n",
    "\n",
    "Both model versions (v1 and v2) are logged to **MLflow** for comparison and audit trail."
   ]
  },
  {
   "cell_type": "code",
   "id": "d90ee980c6aa",
   "metadata": {},
   "source": [
    "from category_encoders import TargetEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from linkedin_lead_scoring.features import (\n",
    "    LOW_CARDINALITY_CATS, NUMERIC_COLS, TARGET_ENCODE_CATS,\n",
    "    extract_text_features,\n",
    ")\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Best hyperparameters (from Optuna tuning \\u2014 notebook 02)\n",
    "BEST_PARAMS = {\n",
    "    \"n_estimators\": 255,\n",
    "    \"max_depth\": 3,\n",
    "    \"learning_rate\": 0.121,\n",
    "    \"min_child_weight\": 7,\n",
    "    \"subsample\": 0.784,\n",
    "    \"colsample_bytree\": 0.988,\n",
    "    \"gamma\": 3.513,\n",
    "    \"scale_pos_weight\": 2.501,\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "# Load datasets\n",
    "df_original = pd.read_csv(PROJECT_ROOT / \"data\" / \"processed\" / \"linkedin_leads_clean.csv\")\n",
    "df_new = production_raw.copy()  # 151 labelled contacts from section 6\n",
    "\n",
    "# Merge\n",
    "df_merged = pd.concat([df_original, df_new], ignore_index=True)\n",
    "df_merged.to_csv(PROJECT_ROOT / \"data\" / \"processed\" / \"linkedin_leads_merged.csv\", index=False)\n",
    "\n",
    "print(\"=== Dataset Summary ===\")\n",
    "print(f\"  Original (v1):  {len(df_original)} rows \\u2014 engaged: {df_original['engaged'].sum()}, not: {(1 - df_original['engaged']).sum():.0f}\")\n",
    "print(f\"  New data:       {len(df_new)} rows \\u2014 engaged: {df_new['engaged'].sum()}, not: {(1 - df_new['engaged']).sum():.0f}\")\n",
    "print(f\"  Merged (v2):    {len(df_merged)} rows \\u2014 engaged: {df_merged['engaged'].sum()}, not: {(1 - df_merged['engaged']).sum():.0f}\")\n",
    "print(f\"\\n  Engagement rate: {df_original['engaged'].mean():.1%} (v1) \\u2192 {df_merged['engaged'].mean():.1%} (v2)\")\n",
    "print(f\"  Saved merged dataset to data/processed/linkedin_leads_merged.csv\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7fe35b7c30db",
   "metadata": {},
   "source": [
    "def train_version(df, version_name):\n",
    "    \"\"\"Train a model version and return model, metrics, preprocessor, and data splits.\"\"\"\n",
    "    df = df.copy()\n",
    "    df = extract_text_features(df)\n",
    "    y = df.pop(\"engaged\")\n",
    "    X = df\n",
    "\n",
    "    # Fill missing values\n",
    "    for col in NUMERIC_COLS:\n",
    "        if col in X.columns:\n",
    "            X[col] = X[col].fillna(X[col].median())\n",
    "    for col in LOW_CARDINALITY_CATS + TARGET_ENCODE_CATS:\n",
    "        if col in X.columns:\n",
    "            X[col] = X[col].fillna(\"UNKNOWN\")\n",
    "\n",
    "    # One-hot encode\n",
    "    ohe_cols = [c for c in LOW_CARDINALITY_CATS if c in X.columns]\n",
    "    if ohe_cols:\n",
    "        X = pd.get_dummies(X, columns=ohe_cols, drop_first=True)\n",
    "        bool_cols = X.select_dtypes(include=\"bool\").columns\n",
    "        X[bool_cols] = X[bool_cols].astype(int)\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Target encoding\n",
    "    te_cols_local = [c for c in TARGET_ENCODE_CATS if c in X_train.columns]\n",
    "    encoder = TargetEncoder(cols=te_cols_local, smoothing=2.0)\n",
    "    X_train[te_cols_local] = encoder.fit_transform(X_train[te_cols_local], y_train)\n",
    "    X_test[te_cols_local] = encoder.transform(X_test[te_cols_local])\n",
    "    preprocessor_out = {\"target_encoder\": encoder, \"te_cols\": te_cols_local}\n",
    "\n",
    "    # Drop non-numeric\n",
    "    non_numeric = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    if non_numeric:\n",
    "        X_train = X_train.drop(columns=non_numeric)\n",
    "        X_test = X_test.drop(columns=non_numeric)\n",
    "\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    # Train\n",
    "    mdl = XGBClassifier(**BEST_PARAMS)\n",
    "    mdl.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = mdl.predict(X_test)\n",
    "    metrics = {\n",
    "        \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
    "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "    }\n",
    "\n",
    "    print(f\"  {version_name}: F1={metrics['f1']:.3f}, Precision={metrics['precision']:.3f}, Recall={metrics['recall']:.3f}\")\n",
    "    print(f\"    Train: {len(X_train)} rows, Test: {len(X_test)} rows, Features: {len(X_train.columns)}\")\n",
    "\n",
    "    return mdl, metrics, preprocessor_out, X_train, X_test, y_test\n",
    "\n",
    "print(\"Training v1 (original 303 rows)...\")\n",
    "model_v1, metrics_v1, prep_v1, X_train_v1, X_test_v1, y_test_v1 = train_version(df_original, \"v1\")\n",
    "\n",
    "print(\"\\nTraining v2 (merged 454 rows)...\")\n",
    "model_v2, metrics_v2, prep_v2, X_train_v2, X_test_v2, y_test_v2 = train_version(df_merged, \"v2\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cbcf8f22d7c1",
   "metadata": {},
   "source": [
    "# Log both versions to MLflow\n",
    "mlflow_uri = f\"file://{PROJECT_ROOT / 'mlruns'}\"\n",
    "mlflow.set_tracking_uri(mlflow_uri)\n",
    "mlflow.set_experiment(\"linkedin-lead-scoring-retraining\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"v1-original-303-rows\"):\n",
    "    mlflow.set_tag(\"model_version\", \"v1\")\n",
    "    mlflow.set_tag(\"dataset\", \"original\")\n",
    "    mlflow.set_tag(\"n_samples\", str(len(df_original)))\n",
    "    mlflow.log_params(BEST_PARAMS)\n",
    "    mlflow.log_param(\"n_train\", len(X_train_v1))\n",
    "    mlflow.log_param(\"n_test\", len(X_test_v1))\n",
    "    mlflow.log_metrics(metrics_v1)\n",
    "    mlflow.sklearn.log_model(model_v1, artifact_path=\"xgboost-model\",\n",
    "                             registered_model_name=\"linkedin-lead-scoring-xgboost\")\n",
    "    v1_run_id = mlflow.active_run().info.run_id\n",
    "    print(f\"\\u2713 v1 logged to MLflow (run: {v1_run_id[:8]})\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"v2-merged-454-rows\"):\n",
    "    mlflow.set_tag(\"model_version\", \"v2\")\n",
    "    mlflow.set_tag(\"dataset\", \"merged\")\n",
    "    mlflow.set_tag(\"n_samples\", str(len(df_merged)))\n",
    "    mlflow.log_params(BEST_PARAMS)\n",
    "    mlflow.log_param(\"n_train\", len(X_train_v2))\n",
    "    mlflow.log_param(\"n_test\", len(X_test_v2))\n",
    "    mlflow.log_metrics(metrics_v2)\n",
    "    mlflow.sklearn.log_model(model_v2, artifact_path=\"xgboost-model\",\n",
    "                             registered_model_name=\"linkedin-lead-scoring-xgboost\")\n",
    "    v2_run_id = mlflow.active_run().info.run_id\n",
    "    print(f\"\\u2713 v2 logged to MLflow (run: {v2_run_id[:8]})\")\n",
    "\n",
    "# Comparison table\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  MODEL COMPARISON: v1 vs v2\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  {'Metric':<15} {'v1 (303)':>12} {'v2 (454)':>12} {'Delta':>10}\")\n",
    "print(f\"  {'-'*49}\")\n",
    "for metric in [\"f1\", \"precision\", \"recall\"]:\n",
    "    v1_val = metrics_v1[metric]\n",
    "    v2_val = metrics_v2[metric]\n",
    "    delta = v2_val - v1_val\n",
    "    sign = \"+\" if delta >= 0 else \"\"\n",
    "    print(f\"  {metric:<15} {v1_val:>12.3f} {v2_val:>12.3f} {sign}{delta:>9.3f}\")\n",
    "print(f\"  {'-'*49}\")\n",
    "print(f\"  {'Training rows':<15} {'303':>12} {'454':>12} {'+151':>10}\")\n",
    "\n",
    "f1_change = (metrics_v2[\"f1\"] - metrics_v1[\"f1\"]) / max(metrics_v1[\"f1\"], 1e-9)\n",
    "if f1_change >= 0:\n",
    "    print(f\"\\n  \\u2713 v2 F1 improved by {f1_change:.1%} \\u2014 more data helps generalization.\")\n",
    "else:\n",
    "    print(f\"\\n  Note: v2 F1 changed by {f1_change:.1%} on held-out test split.\")\n",
    "    print(f\"  Different test splits introduce variance; the key benefit of v2 is\")\n",
    "    print(f\"  better coverage of the production data distribution.\")\n",
    "\n",
    "print(f\"\\n  MLflow experiment: 'linkedin-lead-scoring-retraining'\")\n",
    "print(f\"  View runs: mlflow ui --backend-store-uri {mlflow_uri}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "447742f0bf15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Post-Retraining Evaluation\n",
    "\n",
    "With v2 trained on 454 rows (including production contacts), we evaluate whether:\n",
    "\n",
    "1. **Drift is reduced** when using v2's reference data (which now includes production patterns)\n",
    "2. **Production F1 improves** compared to v1\n",
    "3. The retraining was worthwhile and the model should be promoted to production"
   ]
  },
  {
   "cell_type": "code",
   "id": "83e2bb90c093",
   "metadata": {},
   "source": [
    "# Save v2 model artifacts to separate directory (v1 untouched)\n",
    "v2_model_dir = PROJECT_ROOT / \"model_v2\"\n",
    "v2_reference_dir = PROJECT_ROOT / \"data\" / \"reference_v2\"\n",
    "\n",
    "v2_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "v2_reference_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "joblib.dump(model_v2, v2_model_dir / \"xgboost_model.joblib\")\n",
    "joblib.dump(prep_v2, v2_model_dir / \"preprocessor.joblib\")\n",
    "\n",
    "v2_feature_columns = list(X_train_v2.columns)\n",
    "with open(v2_model_dir / \"feature_columns.json\", \"w\") as f:\n",
    "    json.dump(v2_feature_columns, f, indent=2)\n",
    "\n",
    "v2_medians = {col: float(X_train_v2[col].median()) for col in NUMERIC_COLS if col in X_train_v2.columns}\n",
    "with open(v2_model_dir / \"numeric_medians.json\", \"w\") as f:\n",
    "    json.dump(v2_medians, f, indent=2)\n",
    "\n",
    "# New reference data from v2 training set\n",
    "v2_reference = X_train_v2.head(100)\n",
    "v2_reference.to_csv(v2_reference_dir / \"training_reference.csv\", index=False)\n",
    "print(f\"\\u2713 v2 artifacts saved to {v2_model_dir}/\")\n",
    "print(f\"  Features: {len(v2_feature_columns)}\")\n",
    "\n",
    "# Create v2 drift detector\n",
    "detector_v2 = DriftDetector(reference_data=v2_reference)\n",
    "\n",
    "# Preprocess production data with v2's pipeline\n",
    "production_features_v2 = preprocess_for_inference(\n",
    "    production_raw.drop(columns=[\"engaged\"]),\n",
    "    target_encoder=prep_v2[\"target_encoder\"],\n",
    "    te_cols=prep_v2[\"te_cols\"],\n",
    "    feature_columns=v2_feature_columns,\n",
    "    numeric_medians=v2_medians,\n",
    ")\n",
    "\n",
    "# Drift detection with v2 reference\n",
    "prod_drift_v2 = detector_v2.detect_data_drift(production_features_v2)\n",
    "\n",
    "print(f\"\\n=== Drift Detection: v1 reference vs v2 reference ===\\n\")\n",
    "print(f\"  {'Metric':<25} {'v1 ref':>12} {'v2 ref':>12}\")\n",
    "print(f\"  {'-'*49}\")\n",
    "print(f\"  {'Drift share':<25} {prod_drift['drift_share']:>11.1%} {prod_drift_v2['drift_share']:>11.1%}\")\n",
    "print(f\"  {'Drifted features':<25} {prod_drift['drifted_count']:>12} {prod_drift_v2['drifted_count']:>12}\")\n",
    "print(f\"  {'Drift detected':<25} {str(prod_drift['drift_detected']):>12} {str(prod_drift_v2['drift_detected']):>12}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a4d8ac08827d",
   "metadata": {},
   "source": [
    "EM_DASH = '\\u2014'\n",
    "# Production predictions with v2\n",
    "v2_prod_scores = model_v2.predict_proba(production_features_v2[v2_feature_columns])[:, 1]\n",
    "v2_prod_binary = (v2_prod_scores >= 0.5).astype(int)\n",
    "\n",
    "v2_f1 = f1_score(actuals, v2_prod_binary)\n",
    "v2_precision = precision_score(actuals, v2_prod_binary)\n",
    "v2_recall = recall_score(actuals, v2_prod_binary)\n",
    "\n",
    "print(\"=== PRODUCTION PERFORMANCE: v1 vs v2 ===\\n\")\n",
    "print(f\"  {'Metric':<20} {'Training':>10} {'v1 on prod':>12} {'v2 on prod':>12}\")\n",
    "print(f\"  {'-'*54}\")\n",
    "print(f\"  {'F1':<20} {'0.556':>10} {f1:>12.3f} {v2_f1:>12.3f}\")\n",
    "print(f\"  {'Precision':<20} {'---':>10} {precision:>12.3f} {v2_precision:>12.3f}\")\n",
    "print(f\"  {'Recall':<20} {'---':>10} {recall:>12.3f} {v2_recall:>12.3f}\")\n",
    "\n",
    "# Visualization: side-by-side comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Drift share comparison\n",
    "ax = axes[0]\n",
    "names_cmp = [\"v1 model\", \"v2 model\"]\n",
    "shares_cmp = [prod_drift[\"drift_share\"], prod_drift_v2[\"drift_share\"]]\n",
    "colors_cmp = [ALERT_COLOR if s >= 0.5 else (PROD_COLOR if s >= 0.2 else OK_COLOR) for s in shares_cmp]\n",
    "bars = ax.bar(names_cmp, [s * 100 for s in shares_cmp], color=colors_cmp, edgecolor=\"white\", width=0.4)\n",
    "ax.axhline(50, color=ALERT_COLOR, linestyle=\"--\", linewidth=1.2, label=\"Alert (50%)\")\n",
    "ax.axhline(20, color=PROD_COLOR, linestyle=\"--\", linewidth=1.2, label=\"Investigate (20%)\")\n",
    "for bar, s in zip(bars, shares_cmp):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1.5,\n",
    "            f\"{s:.0%}\", ha=\"center\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Drift Share (%)\")\n",
    "ax.set_title(\"Data Drift: v1 vs v2 Reference\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_ylim(0, 100)\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# F1 comparison\n",
    "ax = axes[1]\n",
    "f1_values = [0.556, f1, v2_f1]\n",
    "f1_labels = [\"Training\\nbaseline\", \"v1 on\\nproduction\", \"v2 on\\nproduction\"]\n",
    "f1_colors = [REF_COLOR, ALERT_COLOR if f1 < 0.45 else PROD_COLOR, OK_COLOR if v2_f1 > f1 else PROD_COLOR]\n",
    "bars = ax.bar(f1_labels, f1_values, color=f1_colors, edgecolor=\"white\", width=0.5)\n",
    "for bar, val in zip(bars, f1_values):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
    "            f\"{val:.3f}\", ha=\"center\", fontsize=11, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"F1 Score\")\n",
    "ax.set_title(\"F1 Score: Training \\u2192 v1 \\u2192 v2\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_ylim(0, 0.8)\n",
    "\n",
    "# Prediction score distributions\n",
    "ax = axes[2]\n",
    "ax.hist(ref_scores, bins=20, alpha=0.35, color=REF_COLOR, label=\"Reference (training)\", edgecolor=\"white\")\n",
    "ax.hist(prod_scores, bins=20, alpha=0.35, color=ALERT_COLOR, label=\"v1 predictions\", edgecolor=\"white\")\n",
    "ax.hist(v2_prod_scores, bins=20, alpha=0.35, color=OK_COLOR, label=\"v2 predictions\", edgecolor=\"white\")\n",
    "ax.set_xlabel(\"Engagement Probability\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Prediction Distributions\", fontsize=12, fontweight=\"bold\")\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "fig.suptitle(\"Model Retraining Impact \\u2014 v1 (303 rows) vs v2 (454 rows)\",\n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "drift_reduction = prod_drift[\"drift_share\"] - prod_drift_v2[\"drift_share\"]\n",
    "f1_improvement = v2_f1 - f1\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  RETRAINING IMPACT SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Drift share: {prod_drift['drift_share']:.0%} \\u2192 {prod_drift_v2['drift_share']:.0%} (\\u0394 = {drift_reduction:+.0%})\")\n",
    "print(f\"  F1 on production: {f1:.3f} \\u2192 {v2_f1:.3f} (\\u0394 = {f1_improvement:+.3f})\")\n",
    "print(f\"  Training data: 303 \\u2192 454 rows (+{len(df_new)} new labelled contacts)\")\n",
    "if f1_improvement > 0:\n",
    "    print(f\"\\n  \\u2713 Retraining improved production F1 by {f1_improvement:.3f}\")\n",
    "else:\n",
    "    print(f\"\\n  Note: F1 may not improve immediately due to test split variance.\")\n",
    "    print(f\"  However, v2 has better coverage of the production data distribution.\")\n",
    "print(f\"{'='*60}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "kxlppwbh2y",
   "source": [
    "---\n",
    "\n",
    "## 12. Monitoring Recommendations & Decision Framework\n",
    "\n",
    "### Summary of Findings\n",
    "\n",
    "| Scenario | Data Drift Share | Prediction Drift | Action |\n",
    "|----------|:----------------:|:-----------------:|--------|\n",
    "| No-Drift Baseline | ~2% | No | Monitor (all clear) |\n",
    "| Sector Shift | High | Check results above | Investigate if new verticals are permanent |\n",
    "| Seniority Shift | High | Check results above | Investigate targeting strategy change |\n",
    "| Geography Shift | Moderate-High | Check results above | Assess expansion impact |\n",
    "| Quality Degradation | High | Check results above | Alert -- fix data pipeline first |\n",
    "\n",
    "### Decision Framework: When to Retrain\n",
    "\n",
    "| Metric | Threshold | Action |\n",
    "|--------|-----------|--------|\n",
    "| **Drift share** < 20% | Green | Continue monitoring. Normal variation. |\n",
    "| **Drift share** 20-50% | Yellow | Investigate which features drifted. If business-relevant, plan retraining. |\n",
    "| **Drift share** >= 50% | Red | Alert. Consider immediate retraining or rollback. |\n",
    "| **Prediction drift** p-value < 0.01 | Red | Model output distribution has shifted significantly. Evaluate real-world impact. |\n",
    "| **F1 on holdout** drops > 20% from baseline (0.556) | Red | Concept drift confirmed. Retrain with new labelled data. |\n",
    "\n",
    "### Operational Monitoring Stack\n",
    "\n",
    "- **Streamlit Dashboard**: Real-time drift metrics and prediction distributions.\n",
    "  Deployed on Hugging Face Spaces for continuous visibility.\n",
    "- **Evidently Reports**: Full HTML drift reports generated via `DriftDetector.generate_report()`.\n",
    "  Stored per batch for audit trail.\n",
    "- **Retraining Script**: `scripts/export_model.py` re-exports the model with updated data.\n",
    "  MLflow tracks experiment versions and model registry manages production deployment.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Deploy monitoring dashboard to production\n",
    "2. Establish ground truth feedback loop (record outreach outcomes)\n",
    "3. Set up automated drift detection on weekly batches\n",
    "4. Define SLA: drift report generated within 24h of new batch ingestion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4n6i67x5uol",
   "source": [
    "# Final consolidated summary — all data and both model versions\n",
    "print(\"=\" * 80)\n",
    "print(\"  CONSOLIDATED MONITORING & RETRAINING REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n  Reference data:  {reference_df.shape[0]} samples, {len(feature_columns)} features\")\n",
    "print(f\"  Model v1:        XGBoost trained on 303 rows (F1={metrics_v1['f1']:.3f} on test)\")\n",
    "print(f\"  Model v2:        XGBoost trained on 454 rows (F1={metrics_v2['f1']:.3f} on test)\")\n",
    "print(f\"  Drift detector:  Evidently AI (KS test for numeric, chi-squared for categorical)\")\n",
    "print()\n",
    "\n",
    "# Synthetic scenarios\n",
    "print(\"  --- SYNTHETIC SCENARIOS (v1 model) ---\")\n",
    "for name in all_scenario_names:\n",
    "    dr = drift_results[name]\n",
    "    pr = prediction_results[name]\n",
    "    if dr[\"drift_share\"] >= 0.5 or pr[\"drift_detected\"]:\n",
    "        level = \"RED\"\n",
    "    elif dr[\"drift_share\"] >= 0.2:\n",
    "        level = \"YELLOW\"\n",
    "    else:\n",
    "        level = \"GREEN\"\n",
    "    print(f\"    {name:<25} [{level:<6}] drift={dr['drift_share']:.1%}, pred_drift={pr['drift_detected']}\")\n",
    "\n",
    "# Real production data\n",
    "print(f\"\\n  --- REAL PRODUCTION DATA ---\")\n",
    "print(f\"    Labelled (151):    drift={prod_drift['drift_share']:.1%}, v1 F1={f1:.3f}, v2 F1={v2_f1:.3f}\")\n",
    "print(f\"    Unlabelled (277):  drift={unlabelled_drift['drift_share']:.1%}, no ground truth\")\n",
    "\n",
    "# Retraining impact\n",
    "print(f\"\\n  --- RETRAINING IMPACT ---\")\n",
    "print(f\"    Drift share:   {prod_drift['drift_share']:.0%} (v1 ref) -> {prod_drift_v2['drift_share']:.0%} (v2 ref)\")\n",
    "print(f\"    Production F1: {f1:.3f} (v1) -> {v2_f1:.3f} (v2)\")\n",
    "print(f\"    MLflow:        Both versions registered in 'linkedin-lead-scoring-retraining'\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"  End of drift monitoring & retraining analysis.\")\n",
    "print(f\"{'='*80}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}