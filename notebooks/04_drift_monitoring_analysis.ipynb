{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5tf714p48ew",
   "source": "# 04 — Data Drift Monitoring Analysis\n\n## LinkedIn Lead Scoring Model — MLOps Monitoring\n\nThis notebook demonstrates **data drift detection** for the LinkedIn Lead Scoring model.\nIt is a key deliverable for evaluating the operational monitoring capabilities of the ML pipeline.\n\n**Objectives:**\n1. Understand the training distribution (reference data)\n2. Detect data drift using Evidently AI on synthetic scenarios\n3. Analyse prediction drift when input distributions shift\n4. Establish monitoring thresholds and a decision framework for retraining\n\n**Model context:**\n- XGBoost classifier trained on 1,910 LemList campaign contacts\n- 47 features (19 base numeric + 28 one-hot encoded)\n- Best F1 score on validation: **0.556**\n- Drift detection powered by [Evidently AI](https://www.evidentlyai.com/)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "542v2yo0b99",
   "source": "## 1. Setup & Imports",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ltr2831e9nd",
   "source": "import sys\nimport os\nimport json\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport warnings\n\n# Add src to path for project imports\nPROJECT_ROOT = Path(os.getcwd()).parent\nsys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n\nfrom linkedin_lead_scoring.monitoring.drift import DriftDetector\n\n# Plotting configuration\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams[\"figure.dpi\"] = 100\nplt.rcParams[\"font.size\"] = 11\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)  # suppress scipy/numpy division warnings\n\n# Consistent colour palette\nREF_COLOR = \"#2196F3\"    # Blue for reference\nPROD_COLOR = \"#FF9800\"   # Orange for production/scenario\nALERT_COLOR = \"#F44336\"  # Red for alerts\nOK_COLOR = \"#4CAF50\"     # Green for no-drift\n\nprint(f\"Project root: {PROJECT_ROOT}\")\nprint(\"Setup complete.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "kg3sfwiz8qh",
   "source": "# Load reference data, model, and configuration\nreference_df = pd.read_csv(PROJECT_ROOT / \"data\" / \"reference\" / \"training_reference.csv\")\nmodel = joblib.load(PROJECT_ROOT / \"model\" / \"xgboost_model.joblib\")\n\npreprocessor_dict = joblib.load(PROJECT_ROOT / \"model\" / \"preprocessor.joblib\")\ntarget_encoder = preprocessor_dict[\"target_encoder\"]\nte_cols = preprocessor_dict[\"te_cols\"]\n\nwith open(PROJECT_ROOT / \"model\" / \"feature_columns.json\") as f:\n    feature_columns = json.load(f)\n\nwith open(PROJECT_ROOT / \"model\" / \"numeric_medians.json\") as f:\n    numeric_medians = json.load(f)\n\n# Import feature engineering for processing raw production data\nfrom linkedin_lead_scoring.features import preprocess_for_inference\n\n# Initialize drift detector with reference data\ndetector = DriftDetector(reference_data=reference_df)\n\nprint(f\"Reference data shape: {reference_df.shape}\")\nprint(f\"Number of features:   {len(feature_columns)}\")\nprint(f\"Model type:           {type(model).__name__}\")\nprint(f\"Target encoder cols:  {te_cols}\")\nprint(f\"Numeric medians:      {numeric_medians}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2m01lt6wyla",
   "source": "---\n\n## 2. Understanding the Training Distribution\n\nBefore monitoring for drift, we must understand the **reference distribution** -- the statistical\nprofile of features the model was trained on. Any future data that deviates significantly from\nthis baseline may indicate that the model's assumptions no longer hold.\n\nWe examine:\n- **Numeric features**: LLM quality scores, engagement scores, company founding year\n- **One-hot encoded features**: seniority level, geography, business type, company size",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "n6hjtn5s5o",
   "source": "# Summary statistics of the reference data\nreference_df.describe().round(3)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5ffvfa021cy",
   "source": "# Distribution of key numeric features\nkey_numeric = [\"llm_quality\", \"llm_engagement\", \"llm_decision_maker\", \"companyfoundedon\"]\n\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\nfor ax, col in zip(axes, key_numeric):\n    reference_df[col].hist(bins=20, ax=ax, color=REF_COLOR, edgecolor=\"white\", alpha=0.85)\n    ax.set_title(col, fontsize=12, fontweight=\"bold\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Count\")\n    ax.axvline(reference_df[col].median(), color=\"red\", linestyle=\"--\", linewidth=1.2, label=\"Median\")\n    ax.legend(fontsize=9)\n\nfig.suptitle(\"Reference Data — Key Numeric Feature Distributions\", fontsize=14, fontweight=\"bold\", y=1.03)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0y5luo3rakkn",
   "source": "# Categorical (OHE) feature distributions — grouped by category prefix\nohe_groups = {\n    \"Seniority (llm_seniority)\": [c for c in feature_columns if c.startswith(\"llm_seniority_\")],\n    \"Geography (llm_geography)\": [c for c in feature_columns if c.startswith(\"llm_geography_\")],\n    \"Business Type (llm_business_type)\": [c for c in feature_columns if c.startswith(\"llm_business_type_\")],\n    \"Company Size\": [c for c in feature_columns if c.startswith(\"companysize_\")],\n}\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\naxes = axes.flatten()\n\nfor ax, (group_name, cols) in zip(axes, ohe_groups.items()):\n    # Compute proportion of 1s in each OHE column (= category prevalence)\n    proportions = reference_df[cols].mean()\n    short_labels = [c.split(\"_\", 2)[-1] if \"_\" in c else c for c in cols]\n    bars = ax.bar(short_labels, proportions, color=REF_COLOR, edgecolor=\"white\", alpha=0.85)\n    ax.set_title(group_name, fontsize=12, fontweight=\"bold\")\n    ax.set_ylabel(\"Proportion\")\n    ax.set_ylim(0, 1.0)\n    ax.tick_params(axis=\"x\", rotation=30)\n    # Add value labels on bars\n    for bar, val in zip(bars, proportions):\n        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.02,\n                f\"{val:.0%}\", ha=\"center\", fontsize=9)\n\nfig.suptitle(\"Reference Data — Categorical Feature Distributions (OHE Prevalence)\",\n             fontsize=14, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "hfp6ngadmcg",
   "source": "---\n\n## 3. Data Drift Detection — Baseline (No Drift)\n\nWe begin with a **no-drift baseline** scenario: synthetic data generated from the same\ndistribution as the training set. This validates that the drift detector does **not produce\nfalse positives** when data is statistically similar to the reference.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "257e6b5l1ve",
   "source": "# Load baseline scenario\nbaseline_df = pd.read_csv(PROJECT_ROOT / \"data\" / \"drift_scenarios\" / \"no_drift_baseline.csv\")\nprint(f\"Baseline data shape: {baseline_df.shape}\")\n\n# Run drift detection\nbaseline_result = detector.detect_data_drift(baseline_df)\n\nprint(\"\\n--- Baseline Drift Detection Results ---\")\nprint(f\"  Drift detected:    {baseline_result['drift_detected']}\")\nprint(f\"  Drifted features:  {baseline_result['drifted_count']} / {baseline_result['total_features']}\")\nprint(f\"  Drift share:       {baseline_result['drift_share']:.1%}\")\nif baseline_result[\"drifted_features\"]:\n    print(f\"  Drifted columns:   {baseline_result['drifted_features']}\")\nelse:\n    print(\"  Drifted columns:   (none)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "y56z8atkq2p",
   "source": "# Side-by-side distributions: reference vs baseline for key features\ncompare_features = [\"llm_quality\", \"llm_engagement\", \"llm_decision_maker\"]\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\nfor ax, col in zip(axes, compare_features):\n    ax.hist(reference_df[col], bins=15, alpha=0.6, color=REF_COLOR, label=\"Reference\", edgecolor=\"white\")\n    ax.hist(baseline_df[col], bins=15, alpha=0.6, color=PROD_COLOR, label=\"Baseline\", edgecolor=\"white\")\n    ax.set_title(col, fontsize=12, fontweight=\"bold\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Count\")\n    ax.legend(fontsize=9)\n\nfig.suptitle(\"Reference vs No-Drift Baseline — Overlapping Distributions\",\n             fontsize=14, fontweight=\"bold\", y=1.03)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "mem7x1xfxy",
   "source": "**Interpretation:** The baseline scenario produces very low drift share and no overall drift alarm.\nThe distributions overlap closely with the reference, confirming that the detector is well-calibrated\nand does not trigger false positives on in-distribution data.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "4qrgapw9ow",
   "source": "---\n\n## 4. Data Drift Detection — Synthetic Scenarios\n\nWe now test four **intentionally drifted** scenarios, each simulating a realistic production shift:\n\n| Scenario | Description |\n|----------|-------------|\n| **Sector Shift** | New industries (healthcare, government) absent from training data |\n| **Seniority Shift** | Mostly junior profiles instead of the mid/senior mix in training |\n| **Geography Shift** | Contacts from different geographic regions |\n| **Quality Degradation** | Incomplete profiles (missing summaries, low LLM scores) |\n\nFor each scenario, we run Evidently's drift detection and examine which features are most affected.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "fotzl0bwzu9",
   "source": "# Load all drift scenarios\nscenario_files = {\n    \"Sector Shift\": \"drift_sector_shift.csv\",\n    \"Seniority Shift\": \"drift_seniority_shift.csv\",\n    \"Geography Shift\": \"drift_geography_shift.csv\",\n    \"Quality Degradation\": \"drift_quality_degradation.csv\",\n}\n\nscenarios = {}\nfor name, filename in scenario_files.items():\n    df = pd.read_csv(PROJECT_ROOT / \"data\" / \"drift_scenarios\" / filename)\n    scenarios[name] = df\n    print(f\"Loaded '{name}': {df.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "yzm8dj4p7kn",
   "source": "# Run drift detection on each scenario and collect results\ndrift_results = {}\nfor name, df in scenarios.items():\n    result = detector.detect_data_drift(df)\n    drift_results[name] = result\n\n# Also include the baseline for comparison\ndrift_results[\"No Drift (Baseline)\"] = baseline_result\n\n# Summary table\nsummary_rows = []\nfor name in [\"No Drift (Baseline)\"] + list(scenarios.keys()):\n    r = drift_results[name]\n    summary_rows.append({\n        \"Scenario\": name,\n        \"Drift Detected\": r[\"drift_detected\"],\n        \"Drifted Features\": r[\"drifted_count\"],\n        \"Total Features\": r[\"total_features\"],\n        \"Drift Share\": f\"{r['drift_share']:.1%}\",\n    })\n\nsummary_df = pd.DataFrame(summary_rows)\nprint(\"=== Data Drift Detection Summary ===\\n\")\nprint(summary_df.to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "obg9pb4cyy",
   "source": "# Visualize drift share across scenarios\nscenario_names = [r[\"Scenario\"] for r in summary_rows]\ndrift_shares = [drift_results[name][\"drift_share\"] for name in\n                [\"No Drift (Baseline)\"] + list(scenarios.keys())]\n\nfig, ax = plt.subplots(figsize=(10, 5))\ncolors = [OK_COLOR if ds < 0.2 else (PROD_COLOR if ds < 0.5 else ALERT_COLOR)\n          for ds in drift_shares]\nbars = ax.barh(scenario_names, [ds * 100 for ds in drift_shares], color=colors, edgecolor=\"white\")\n\n# Add threshold lines\nax.axvline(20, color=PROD_COLOR, linestyle=\"--\", linewidth=1.5, label=\"Investigate (20%)\")\nax.axvline(50, color=ALERT_COLOR, linestyle=\"--\", linewidth=1.5, label=\"Alert / Retrain (50%)\")\n\n# Labels on bars\nfor bar, ds in zip(bars, drift_shares):\n    ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height() / 2,\n            f\"{ds:.1%}\", va=\"center\", fontsize=10, fontweight=\"bold\")\n\nax.set_xlabel(\"Drift Share (%)\", fontsize=12)\nax.set_title(\"Data Drift Share by Scenario\", fontsize=14, fontweight=\"bold\")\nax.legend(loc=\"lower right\", fontsize=10)\nax.set_xlim(0, 100)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "r3pt14t2n4",
   "source": "# Detailed drifted features for each scenario\nfor name in scenarios.keys():\n    result = drift_results[name]\n    drifted = result[\"drifted_features\"]\n    print(f\"\\n{'='*60}\")\n    print(f\"  {name}\")\n    print(f\"  Drifted: {result['drifted_count']}/{result['total_features']} ({result['drift_share']:.1%})\")\n    print(f\"{'='*60}\")\n    if drifted:\n        for i, feat in enumerate(drifted[:10], 1):\n            print(f\"  {i:2d}. {feat}\")\n        if len(drifted) > 10:\n            print(f\"  ... and {len(drifted) - 10} more\")\n    else:\n        print(\"  (no individually drifted features detected)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "w9b0ay7t14e",
   "source": "# Visualize one key drifted feature per scenario (overlapping histograms)\n# We pick the most interpretable feature for each scenario type\nscenario_highlight_features = {\n    \"Sector Shift\": \"llm_industry\",\n    \"Seniority Shift\": \"llm_decision_maker\",\n    \"Geography Shift\": \"companylocation\",\n    \"Quality Degradation\": \"llm_quality\",\n}\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\naxes = axes.flatten()\n\nfor ax, (scenario_name, feat) in zip(axes, scenario_highlight_features.items()):\n    ref_vals = reference_df[feat]\n    scen_vals = scenarios[scenario_name][feat]\n\n    ax.hist(ref_vals, bins=15, alpha=0.6, color=REF_COLOR, label=\"Reference\", edgecolor=\"white\")\n    ax.hist(scen_vals, bins=15, alpha=0.6, color=PROD_COLOR, label=scenario_name, edgecolor=\"white\")\n\n    ax.set_title(f\"{scenario_name}\\nFeature: {feat}\", fontsize=11, fontweight=\"bold\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Count\")\n    ax.legend(fontsize=9)\n\nfig.suptitle(\"Key Drifted Feature per Scenario — Reference vs Scenario\",\n             fontsize=14, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "kn5vo95873b",
   "source": "### Scenario Interpretations\n\n- **Sector Shift**: The industry-related features (target-encoded `llm_industry`, `industry`, `companyindustry`) drift because the synthetic data introduces sectors (e.g., healthcare, government) that were underrepresented or absent in training. This simulates a business expansion into new verticals.\n\n- **Seniority Shift**: Features like `llm_decision_maker`, `llm_quality`, and the `llm_seniority_*` OHE columns shift drastically. This simulates targeting more junior contacts who have different profile completeness patterns.\n\n- **Geography Shift**: Location-related features (`companylocation`, `location`, `llm_geography_*`) shift as contacts come from different regions. This is common when expanding to new markets.\n\n- **Quality Degradation**: `llm_quality` drops, `has_summary` goes to 0, and `skills_count` decreases. This simulates a scenario where enrichment quality degrades (e.g., LLM API issues, scraping failures).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "3xwc9dzj70m",
   "source": "---\n\n## 5. Prediction Drift Analysis\n\nData drift does not always translate into prediction drift. A model may be robust to certain\ndistributional shifts if the affected features are not strongly weighted.\n\nHere we:\n1. Generate model predictions (probability scores) for the reference and each scenario\n2. Use a **Kolmogorov-Smirnov (KS) test** to compare prediction score distributions\n3. Visualize how predicted lead scores shift across scenarios",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2dyxqr86ms6",
   "source": "# Generate predictions on reference data\n# The data is already in the 47-feature format, so we can predict directly\nref_features = reference_df[feature_columns]\nref_scores = model.predict_proba(ref_features)[:, 1]\n\nprint(f\"Reference prediction scores:\")\nprint(f\"  Mean:   {ref_scores.mean():.3f}\")\nprint(f\"  Median: {np.median(ref_scores):.3f}\")\nprint(f\"  Std:    {ref_scores.std():.3f}\")\nprint(f\"  Range:  [{ref_scores.min():.3f}, {ref_scores.max():.3f}]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "pogqvbiho7q",
   "source": "# Prediction drift analysis for each scenario\nall_scenario_names = [\"No Drift (Baseline)\"] + list(scenarios.keys())\nall_scenario_data = {\"No Drift (Baseline)\": baseline_df, **scenarios}\n\nprediction_results = {}\nfor name in all_scenario_names:\n    df = all_scenario_data[name]\n    scen_features = df[feature_columns]\n    scen_scores = model.predict_proba(scen_features)[:, 1]\n\n    pred_drift = detector.detect_prediction_drift(ref_scores, scen_scores)\n    prediction_results[name] = {\n        \"scores\": scen_scores,\n        **pred_drift,\n    }\n\n# Summary table\npred_rows = []\nfor name in all_scenario_names:\n    r = prediction_results[name]\n    pred_rows.append({\n        \"Scenario\": name,\n        \"KS Statistic\": f\"{r['statistic']:.4f}\",\n        \"p-value\": f\"{r['p_value']:.4e}\" if r[\"p_value\"] < 0.001 else f\"{r['p_value']:.4f}\",\n        \"Drift Detected\": r[\"drift_detected\"],\n        \"Mean Score\": f\"{r['scores'].mean():.3f}\",\n    })\n\npred_summary_df = pd.DataFrame(pred_rows)\nprint(\"=== Prediction Drift Summary (KS Test, threshold=0.05) ===\\n\")\nprint(pred_summary_df.to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ee1awqvlfok",
   "source": "# Overlapping histograms of prediction score distributions\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\naxes = axes.flatten()\n\ndrift_scenario_names = list(scenarios.keys())\nfor ax, name in zip(axes, drift_scenario_names):\n    r = prediction_results[name]\n    color = ALERT_COLOR if r[\"drift_detected\"] else PROD_COLOR\n\n    ax.hist(ref_scores, bins=20, alpha=0.5, color=REF_COLOR, label=\"Reference\", edgecolor=\"white\")\n    ax.hist(r[\"scores\"], bins=20, alpha=0.5, color=color, label=name, edgecolor=\"white\")\n\n    status = \"DRIFT\" if r[\"drift_detected\"] else \"OK\"\n    ax.set_title(f\"{name}\\nKS={r['statistic']:.3f}, p={r['p_value']:.3e} [{status}]\",\n                 fontsize=10, fontweight=\"bold\")\n    ax.set_xlabel(\"Predicted Probability (engagement)\")\n    ax.set_ylabel(\"Count\")\n    ax.legend(fontsize=9)\n\nfig.suptitle(\"Prediction Score Distributions — Reference vs Each Scenario\",\n             fontsize=14, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "sv2mvm59oy",
   "source": "### Key Observations\n\n- **Data drift does not always cause prediction drift.** Some feature shifts may affect columns\n  the model does not weight heavily.\n- **Prediction drift is the stronger signal** for deciding when to retrain, because it directly\n  measures whether the model's output behaviour has changed.\n- Scenarios with both high data drift *and* prediction drift (p < 0.05) are the most concerning\n  and should trigger a retraining investigation.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "sw8wrwgzgc",
   "source": "---\n\n## 6. Real Production Data — Drift Analysis\n\nWe now apply drift detection on **real LinkedIn contacts** that were processed through the same\nLLM enrichment pipeline (notebook 01) as the training data. These are actual business development\ncontacts — not synthetic data.\n\n**Data source:** 151 labelled contacts from 3 LemList campaign exports, enriched via gpt-4o-mini,\nwith known engagement outcomes (47.7% engagement rate).\n\nThis is the most important section — it shows whether our model faces **real drift** in production.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "91y9jscbaee",
   "source": "# Load real production data (20-column enriched format, same as training)\nproduction_raw = pd.read_csv(PROJECT_ROOT / \"data\" / \"production\" / \"new_contacts_enriched.csv\")\nprint(f\"Production data: {production_raw.shape}\")\nprint(f\"Engagement rate: {production_raw['engaged'].mean():.1%}\")\nprint(f\"  Engaged: {production_raw['engaged'].sum()}, Not engaged: {(1 - production_raw['engaged']).sum():.0f}\")\n\n# Preprocess to 47-feature format (same as training_reference.csv)\nproduction_features = preprocess_for_inference(\n    production_raw.drop(columns=[\"engaged\"]),\n    target_encoder=target_encoder,\n    te_cols=te_cols,\n    feature_columns=feature_columns,\n    numeric_medians=numeric_medians,\n)\nprint(f\"\\nProcessed features: {production_features.shape}\")\nprint(f\"Columns match reference: {list(production_features.columns) == feature_columns}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "31oatn5iu3m",
   "source": "# Data drift detection on real production data\nprod_drift = detector.detect_data_drift(production_features)\n\nprint(\"=== REAL PRODUCTION DATA — Drift Detection ===\\n\")\nprint(f\"  Drift detected:    {prod_drift['drift_detected']}\")\nprint(f\"  Drift share:       {prod_drift['drift_share']:.1%}\")\nprint(f\"  Drifted features:  {prod_drift['drifted_count']} / {prod_drift['total_features']}\")\nprint(f\"\\n  Drifted feature list:\")\nfor i, feat in enumerate(prod_drift[\"drifted_features\"], 1):\n    print(f\"    {i:2d}. {feat}\")\n\n# Prediction drift on real data\nprod_scores = model.predict_proba(production_features[feature_columns])[:, 1]\nprod_pred_drift = detector.detect_prediction_drift(ref_scores, prod_scores)\n\nprint(f\"\\n=== Prediction Drift ===\")\nprint(f\"  KS Statistic:  {prod_pred_drift['statistic']:.4f}\")\nprint(f\"  p-value:       {prod_pred_drift['p_value']:.4e}\")\nprint(f\"  Drift:         {prod_pred_drift['drift_detected']}\")\nprint(f\"\\n  Production scores: mean={prod_scores.mean():.3f}, std={prod_scores.std():.3f}\")\nprint(f\"  Reference scores:  mean={ref_scores.mean():.3f}, std={ref_scores.std():.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qhqxys6kmnc",
   "source": "# Visualize real production drift\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\n# Top row: key numeric feature distributions (reference vs production)\ntop_features = [\"llm_quality\", \"llm_engagement\", \"llm_decision_maker\"]\nfor ax, col in zip(axes[0], top_features):\n    drifted = col in prod_drift[\"drifted_features\"]\n    border_color = ALERT_COLOR if drifted else OK_COLOR\n    ax.hist(reference_df[col], bins=15, alpha=0.5, color=REF_COLOR, label=\"Reference (training)\", edgecolor=\"white\")\n    ax.hist(production_features[col], bins=15, alpha=0.5, color=PROD_COLOR, label=\"Production (real)\", edgecolor=\"white\")\n    status = \"DRIFTED\" if drifted else \"OK\"\n    ax.set_title(f\"{col} [{status}]\", fontsize=11, fontweight=\"bold\", color=border_color)\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Count\")\n    ax.legend(fontsize=9)\n\n# Bottom left: prediction score distributions\nax = axes[1][0]\nax.hist(ref_scores, bins=20, alpha=0.5, color=REF_COLOR, label=\"Reference\", edgecolor=\"white\")\nax.hist(prod_scores, bins=20, alpha=0.5, color=PROD_COLOR, label=\"Production\", edgecolor=\"white\")\ndrift_label = \"DRIFT\" if prod_pred_drift[\"drift_detected\"] else \"OK\"\nax.set_title(f\"Prediction Scores [{drift_label}]\\nKS={prod_pred_drift['statistic']:.3f}, p={prod_pred_drift['p_value']:.3e}\",\n             fontsize=11, fontweight=\"bold\")\nax.set_xlabel(\"Engagement Probability\")\nax.set_ylabel(\"Count\")\nax.legend(fontsize=9)\n\n# Bottom middle: drift share comparison (production vs synthetic scenarios)\nax = axes[1][1]\nall_names = [\"Baseline\", \"Production\\n(REAL)\", \"Sector\", \"Seniority\", \"Geography\", \"Quality\"]\nall_shares = [\n    baseline_result[\"drift_share\"],\n    prod_drift[\"drift_share\"],\n    drift_results[\"Sector Shift\"][\"drift_share\"],\n    drift_results[\"Seniority Shift\"][\"drift_share\"],\n    drift_results[\"Geography Shift\"][\"drift_share\"],\n    drift_results[\"Quality Degradation\"][\"drift_share\"],\n]\ncolors = [OK_COLOR if s < 0.2 else (PROD_COLOR if s < 0.5 else ALERT_COLOR) for s in all_shares]\ncolors[1] = \"#E91E63\"  # Highlight production in pink\nbars = ax.bar(all_names, [s * 100 for s in all_shares], color=colors, edgecolor=\"white\")\nax.axhline(50, color=ALERT_COLOR, linestyle=\"--\", linewidth=1.5, label=\"Alert threshold (50%)\")\nax.axhline(20, color=PROD_COLOR, linestyle=\"--\", linewidth=1.2, label=\"Investigate (20%)\")\nfor bar, s in zip(bars, all_shares):\n    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f\"{s:.0%}\",\n            ha=\"center\", fontsize=9, fontweight=\"bold\")\nax.set_ylabel(\"Drift Share (%)\")\nax.set_title(\"Drift Share Comparison\", fontsize=11, fontweight=\"bold\")\nax.legend(fontsize=8)\nax.set_ylim(0, 100)\nax.tick_params(axis=\"x\", rotation=15)\n\n# Bottom right: OHE category comparison\nax = axes[1][2]\nohe_cats = [\"companysize_UNKNOWN\", \"companytype_Privately Held\", \"companytype_UNKNOWN\",\n            \"llm_seniority_Senior\", \"llm_business_type_others\"]\nref_vals = reference_df[ohe_cats].mean()\nprod_vals = production_features[ohe_cats].mean()\nx = np.arange(len(ohe_cats))\nshort_labels = [c.split(\"_\", 1)[-1][:20] for c in ohe_cats]\nax.bar(x - 0.2, ref_vals, 0.35, color=REF_COLOR, label=\"Reference\", edgecolor=\"white\")\nax.bar(x + 0.2, prod_vals, 0.35, color=PROD_COLOR, label=\"Production\", edgecolor=\"white\")\nax.set_xticks(x)\nax.set_xticklabels(short_labels, fontsize=8, rotation=30, ha=\"right\")\nax.set_ylabel(\"Proportion\")\nax.set_title(\"Key Category Shifts\", fontsize=11, fontweight=\"bold\")\nax.legend(fontsize=9)\n\nfig.suptitle(\"REAL PRODUCTION DATA — Drift Analysis (151 contacts)\",\n             fontsize=15, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "r1sxo38igy",
   "source": "---\n\n## 7. Concept Drift — Ground Truth Analysis\n\n**Concept drift** differs from data drift: the **relationship between features and the target**\nmay have changed, even if feature distributions are stable.\n\nWe have **151 labelled contacts** with actual engagement outcomes. This allows us to:\n1. Compare the model's predictions against ground truth\n2. Measure real-world F1, precision, recall on unseen data\n3. Determine whether concept drift has occurred (F1 drop from training baseline of 0.556)\n\nThis is the strongest evidence for or against retraining.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "zov1rxvnvc",
   "source": "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n\n# Ground truth from the production data\nactuals = production_raw[\"engaged\"].values\npredictions_proba = prod_scores\npredictions_binary = (predictions_proba >= 0.5).astype(int)\n\n# Performance metrics\nf1 = f1_score(actuals, predictions_binary)\nprecision = precision_score(actuals, predictions_binary)\nrecall = recall_score(actuals, predictions_binary)\n\nprint(\"=== MODEL PERFORMANCE ON REAL PRODUCTION DATA ===\\n\")\nprint(f\"  Contacts evaluated:  {len(actuals)}\")\nprint(f\"  Actual engaged:      {actuals.sum()} ({actuals.mean():.1%})\")\nprint(f\"  Predicted engaged:   {predictions_binary.sum()} ({predictions_binary.mean():.1%})\")\nprint()\nprint(f\"  {'Metric':<15} {'Training':>10} {'Production':>12} {'Change':>10}\")\nprint(f\"  {'-'*47}\")\nprint(f\"  {'F1 Score':<15} {'0.556':>10} {f1:>12.3f} {(f1 - 0.556) / 0.556:>+10.1%}\")\nprint(f\"  {'Precision':<15} {'—':>10} {precision:>12.3f}\")\nprint(f\"  {'Recall':<15} {'—':>10} {recall:>12.3f}\")\n\n# Concept drift assessment\nf1_drop = (0.556 - f1) / 0.556\nif f1_drop > 0.20:\n    print(f\"\\n  ⚠️  ALERT: F1 dropped {f1_drop:.0%} from training baseline.\")\n    print(f\"     This indicates concept drift. Retraining is recommended.\")\nelif f1_drop > 0.10:\n    print(f\"\\n  ⚠️  WARNING: F1 dropped {f1_drop:.0%}. Monitor closely.\")\nelse:\n    print(f\"\\n  ✅ F1 is within acceptable range of training performance.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ofbhe4l5lhi",
   "source": "# Confusion matrix and detailed classification report\nprint(\"=== Confusion Matrix ===\\n\")\ncm = confusion_matrix(actuals, predictions_binary)\nprint(f\"                    Predicted\")\nprint(f\"                 Not Eng.  Engaged\")\nprint(f\"  Actual Not Eng.  {cm[0][0]:>5}    {cm[0][1]:>5}\")\nprint(f\"  Actual Engaged   {cm[1][0]:>5}    {cm[1][1]:>5}\")\nprint()\nprint(classification_report(actuals, predictions_binary, target_names=[\"Not Engaged\", \"Engaged\"]))\n\n# Visualization: confusion matrix + score distribution by actual label\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Confusion matrix heatmap\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1,\n            xticklabels=[\"Not Engaged\", \"Engaged\"],\n            yticklabels=[\"Not Engaged\", \"Engaged\"])\nax1.set_xlabel(\"Predicted\", fontsize=12)\nax1.set_ylabel(\"Actual\", fontsize=12)\nax1.set_title(\"Confusion Matrix — Production Data\", fontsize=13, fontweight=\"bold\")\n\n# Score distribution by actual label\nengaged_scores = predictions_proba[actuals == 1]\nnot_engaged_scores = predictions_proba[actuals == 0]\nax2.hist(not_engaged_scores, bins=15, alpha=0.6, color=\"#F44336\", label=\"Actually Not Engaged\", edgecolor=\"white\")\nax2.hist(engaged_scores, bins=15, alpha=0.6, color=\"#4CAF50\", label=\"Actually Engaged\", edgecolor=\"white\")\nax2.axvline(0.5, color=\"black\", linestyle=\"--\", linewidth=2, label=\"Decision threshold (0.5)\")\nax2.set_xlabel(\"Predicted Engagement Probability\", fontsize=12)\nax2.set_ylabel(\"Count\", fontsize=12)\nax2.set_title(\"Score Distribution by Actual Outcome\", fontsize=13, fontweight=\"bold\")\nax2.legend(fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nKey insight: The model scores actually-engaged contacts at {engaged_scores.mean():.3f} on average,\")\nprint(f\"vs {not_engaged_scores.mean():.3f} for not-engaged. Separation exists but is weaker than training.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "kxlppwbh2y",
   "source": "---\n\n## 8. Monitoring Recommendations & Decision Framework\n\n### Summary of Findings\n\n| Scenario | Data Drift Share | Prediction Drift | Action |\n|----------|:----------------:|:-----------------:|--------|\n| No-Drift Baseline | ~2% | No | Monitor (all clear) |\n| Sector Shift | High | Check results above | Investigate if new verticals are permanent |\n| Seniority Shift | High | Check results above | Investigate targeting strategy change |\n| Geography Shift | Moderate-High | Check results above | Assess expansion impact |\n| Quality Degradation | High | Check results above | Alert -- fix data pipeline first |\n\n### Decision Framework: When to Retrain\n\n| Metric | Threshold | Action |\n|--------|-----------|--------|\n| **Drift share** < 20% | Green | Continue monitoring. Normal variation. |\n| **Drift share** 20-50% | Yellow | Investigate which features drifted. If business-relevant, plan retraining. |\n| **Drift share** >= 50% | Red | Alert. Consider immediate retraining or rollback. |\n| **Prediction drift** p-value < 0.01 | Red | Model output distribution has shifted significantly. Evaluate real-world impact. |\n| **F1 on holdout** drops > 20% from baseline (0.556) | Red | Concept drift confirmed. Retrain with new labelled data. |\n\n### Operational Monitoring Stack\n\n- **Streamlit Dashboard**: Real-time drift metrics and prediction distributions.\n  Deployed on Hugging Face Spaces for continuous visibility.\n- **Evidently Reports**: Full HTML drift reports generated via `DriftDetector.generate_report()`.\n  Stored per batch for audit trail.\n- **Retraining Script**: `scripts/export_model.py` re-exports the model with updated data.\n  MLflow tracks experiment versions and model registry manages production deployment.\n\n### Next Steps\n\n1. Deploy monitoring dashboard to production\n2. Establish ground truth feedback loop (record outreach outcomes)\n3. Set up automated drift detection on weekly batches\n4. Define SLA: drift report generated within 24h of new batch ingestion",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4n6i67x5uol",
   "source": "# Final consolidated summary — combined data drift + prediction drift\nprint(\"=\" * 80)\nprint(\"  CONSOLIDATED DRIFT MONITORING REPORT\")\nprint(\"=\" * 80)\nprint(f\"\\n  Reference data:  {reference_df.shape[0]} samples, {len(feature_columns)} features\")\nprint(f\"  Model:           XGBoost (F1=0.556 on validation)\")\nprint(f\"  Drift detector:  Evidently AI (KS test for numeric, chi-squared for categorical)\")\nprint()\n\nfor name in all_scenario_names:\n    dr = drift_results[name]\n    pr = prediction_results[name]\n\n    # Determine alert level\n    if dr[\"drift_share\"] >= 0.5 or pr[\"drift_detected\"]:\n        level = \"RED\"\n    elif dr[\"drift_share\"] >= 0.2:\n        level = \"YELLOW\"\n    else:\n        level = \"GREEN\"\n\n    print(f\"  --- {name} [{level}] ---\")\n    print(f\"    Data drift:       {dr['drifted_count']}/{dr['total_features']} features ({dr['drift_share']:.1%})\")\n    print(f\"    Prediction drift: KS={pr['statistic']:.4f}, p={pr['p_value']:.4e}, detected={pr['drift_detected']}\")\n    print(f\"    Mean score:       {pr['scores'].mean():.3f} (ref: {ref_scores.mean():.3f})\")\n    print()\n\nprint(\"=\" * 80)\nprint(\"  End of drift monitoring analysis.\")\nprint(\"=\" * 80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}