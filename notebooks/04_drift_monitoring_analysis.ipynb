{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5tf714p48ew",
   "source": "# 04 — Data Drift Monitoring Analysis\n\n## LinkedIn Lead Scoring Model — MLOps Monitoring\n\nThis notebook demonstrates **data drift detection** for the LinkedIn Lead Scoring model.\nIt is a key deliverable for evaluating the operational monitoring capabilities of the ML pipeline.\n\n**Objectives:**\n1. Understand the training distribution (reference data)\n2. Detect data drift using Evidently AI on synthetic scenarios\n3. Analyse prediction drift when input distributions shift\n4. Establish monitoring thresholds and a decision framework for retraining\n\n**Model context:**\n- XGBoost classifier trained on 1,910 LemList campaign contacts\n- 47 features (19 base numeric + 28 one-hot encoded)\n- Best F1 score on validation: **0.556**\n- Drift detection powered by [Evidently AI](https://www.evidentlyai.com/)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "542v2yo0b99",
   "source": "## 1. Setup & Imports",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ltr2831e9nd",
   "source": "import sys\nimport os\nimport json\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport warnings\n\n# Add src to path for project imports\nPROJECT_ROOT = Path(os.getcwd()).parent\nsys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n\nfrom linkedin_lead_scoring.monitoring.drift import DriftDetector\n\n# Plotting configuration\nplt.style.use(\"seaborn-v0_8-whitegrid\")\nplt.rcParams[\"figure.dpi\"] = 100\nplt.rcParams[\"font.size\"] = 11\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)  # suppress scipy/numpy division warnings\n\n# Consistent colour palette\nREF_COLOR = \"#2196F3\"    # Blue for reference\nPROD_COLOR = \"#FF9800\"   # Orange for production/scenario\nALERT_COLOR = \"#F44336\"  # Red for alerts\nOK_COLOR = \"#4CAF50\"     # Green for no-drift\n\nprint(f\"Project root: {PROJECT_ROOT}\")\nprint(\"Setup complete.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "kg3sfwiz8qh",
   "source": "# Load reference data, model, and configuration\nreference_df = pd.read_csv(PROJECT_ROOT / \"data\" / \"reference\" / \"training_reference.csv\")\nmodel = joblib.load(PROJECT_ROOT / \"model\" / \"xgboost_model.joblib\")\n\nwith open(PROJECT_ROOT / \"model\" / \"feature_columns.json\") as f:\n    feature_columns = json.load(f)\n\nwith open(PROJECT_ROOT / \"model\" / \"numeric_medians.json\") as f:\n    numeric_medians = json.load(f)\n\n# Initialize drift detector with reference data\ndetector = DriftDetector(reference_data=reference_df)\n\nprint(f\"Reference data shape: {reference_df.shape}\")\nprint(f\"Number of features:   {len(feature_columns)}\")\nprint(f\"Model type:           {type(model).__name__}\")\nprint(f\"Numeric medians:      {numeric_medians}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2m01lt6wyla",
   "source": "---\n\n## 2. Understanding the Training Distribution\n\nBefore monitoring for drift, we must understand the **reference distribution** -- the statistical\nprofile of features the model was trained on. Any future data that deviates significantly from\nthis baseline may indicate that the model's assumptions no longer hold.\n\nWe examine:\n- **Numeric features**: LLM quality scores, engagement scores, company founding year\n- **One-hot encoded features**: seniority level, geography, business type, company size",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "n6hjtn5s5o",
   "source": "# Summary statistics of the reference data\nreference_df.describe().round(3)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5ffvfa021cy",
   "source": "# Distribution of key numeric features\nkey_numeric = [\"llm_quality\", \"llm_engagement\", \"llm_decision_maker\", \"companyfoundedon\"]\n\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\nfor ax, col in zip(axes, key_numeric):\n    reference_df[col].hist(bins=20, ax=ax, color=REF_COLOR, edgecolor=\"white\", alpha=0.85)\n    ax.set_title(col, fontsize=12, fontweight=\"bold\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Count\")\n    ax.axvline(reference_df[col].median(), color=\"red\", linestyle=\"--\", linewidth=1.2, label=\"Median\")\n    ax.legend(fontsize=9)\n\nfig.suptitle(\"Reference Data — Key Numeric Feature Distributions\", fontsize=14, fontweight=\"bold\", y=1.03)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0y5luo3rakkn",
   "source": "# Categorical (OHE) feature distributions — grouped by category prefix\nohe_groups = {\n    \"Seniority (llm_seniority)\": [c for c in feature_columns if c.startswith(\"llm_seniority_\")],\n    \"Geography (llm_geography)\": [c for c in feature_columns if c.startswith(\"llm_geography_\")],\n    \"Business Type (llm_business_type)\": [c for c in feature_columns if c.startswith(\"llm_business_type_\")],\n    \"Company Size\": [c for c in feature_columns if c.startswith(\"companysize_\")],\n}\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\naxes = axes.flatten()\n\nfor ax, (group_name, cols) in zip(axes, ohe_groups.items()):\n    # Compute proportion of 1s in each OHE column (= category prevalence)\n    proportions = reference_df[cols].mean()\n    short_labels = [c.split(\"_\", 2)[-1] if \"_\" in c else c for c in cols]\n    bars = ax.bar(short_labels, proportions, color=REF_COLOR, edgecolor=\"white\", alpha=0.85)\n    ax.set_title(group_name, fontsize=12, fontweight=\"bold\")\n    ax.set_ylabel(\"Proportion\")\n    ax.set_ylim(0, 1.0)\n    ax.tick_params(axis=\"x\", rotation=30)\n    # Add value labels on bars\n    for bar, val in zip(bars, proportions):\n        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.02,\n                f\"{val:.0%}\", ha=\"center\", fontsize=9)\n\nfig.suptitle(\"Reference Data — Categorical Feature Distributions (OHE Prevalence)\",\n             fontsize=14, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "hfp6ngadmcg",
   "source": "---\n\n## 3. Data Drift Detection — Baseline (No Drift)\n\nWe begin with a **no-drift baseline** scenario: synthetic data generated from the same\ndistribution as the training set. This validates that the drift detector does **not produce\nfalse positives** when data is statistically similar to the reference.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "257e6b5l1ve",
   "source": "# Load baseline scenario\nbaseline_df = pd.read_csv(PROJECT_ROOT / \"data\" / \"drift_scenarios\" / \"no_drift_baseline.csv\")\nprint(f\"Baseline data shape: {baseline_df.shape}\")\n\n# Run drift detection\nbaseline_result = detector.detect_data_drift(baseline_df)\n\nprint(\"\\n--- Baseline Drift Detection Results ---\")\nprint(f\"  Drift detected:    {baseline_result['drift_detected']}\")\nprint(f\"  Drifted features:  {baseline_result['drifted_count']} / {baseline_result['total_features']}\")\nprint(f\"  Drift share:       {baseline_result['drift_share']:.1%}\")\nif baseline_result[\"drifted_features\"]:\n    print(f\"  Drifted columns:   {baseline_result['drifted_features']}\")\nelse:\n    print(\"  Drifted columns:   (none)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "y56z8atkq2p",
   "source": "# Side-by-side distributions: reference vs baseline for key features\ncompare_features = [\"llm_quality\", \"llm_engagement\", \"llm_decision_maker\"]\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\nfor ax, col in zip(axes, compare_features):\n    ax.hist(reference_df[col], bins=15, alpha=0.6, color=REF_COLOR, label=\"Reference\", edgecolor=\"white\")\n    ax.hist(baseline_df[col], bins=15, alpha=0.6, color=PROD_COLOR, label=\"Baseline\", edgecolor=\"white\")\n    ax.set_title(col, fontsize=12, fontweight=\"bold\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Count\")\n    ax.legend(fontsize=9)\n\nfig.suptitle(\"Reference vs No-Drift Baseline — Overlapping Distributions\",\n             fontsize=14, fontweight=\"bold\", y=1.03)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "mem7x1xfxy",
   "source": "**Interpretation:** The baseline scenario produces very low drift share and no overall drift alarm.\nThe distributions overlap closely with the reference, confirming that the detector is well-calibrated\nand does not trigger false positives on in-distribution data.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "4qrgapw9ow",
   "source": "---\n\n## 4. Data Drift Detection — Synthetic Scenarios\n\nWe now test four **intentionally drifted** scenarios, each simulating a realistic production shift:\n\n| Scenario | Description |\n|----------|-------------|\n| **Sector Shift** | New industries (healthcare, government) absent from training data |\n| **Seniority Shift** | Mostly junior profiles instead of the mid/senior mix in training |\n| **Geography Shift** | Contacts from different geographic regions |\n| **Quality Degradation** | Incomplete profiles (missing summaries, low LLM scores) |\n\nFor each scenario, we run Evidently's drift detection and examine which features are most affected.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "fotzl0bwzu9",
   "source": "# Load all drift scenarios\nscenario_files = {\n    \"Sector Shift\": \"drift_sector_shift.csv\",\n    \"Seniority Shift\": \"drift_seniority_shift.csv\",\n    \"Geography Shift\": \"drift_geography_shift.csv\",\n    \"Quality Degradation\": \"drift_quality_degradation.csv\",\n}\n\nscenarios = {}\nfor name, filename in scenario_files.items():\n    df = pd.read_csv(PROJECT_ROOT / \"data\" / \"drift_scenarios\" / filename)\n    scenarios[name] = df\n    print(f\"Loaded '{name}': {df.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "yzm8dj4p7kn",
   "source": "# Run drift detection on each scenario and collect results\ndrift_results = {}\nfor name, df in scenarios.items():\n    result = detector.detect_data_drift(df)\n    drift_results[name] = result\n\n# Also include the baseline for comparison\ndrift_results[\"No Drift (Baseline)\"] = baseline_result\n\n# Summary table\nsummary_rows = []\nfor name in [\"No Drift (Baseline)\"] + list(scenarios.keys()):\n    r = drift_results[name]\n    summary_rows.append({\n        \"Scenario\": name,\n        \"Drift Detected\": r[\"drift_detected\"],\n        \"Drifted Features\": r[\"drifted_count\"],\n        \"Total Features\": r[\"total_features\"],\n        \"Drift Share\": f\"{r['drift_share']:.1%}\",\n    })\n\nsummary_df = pd.DataFrame(summary_rows)\nprint(\"=== Data Drift Detection Summary ===\\n\")\nprint(summary_df.to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "obg9pb4cyy",
   "source": "# Visualize drift share across scenarios\nscenario_names = [r[\"Scenario\"] for r in summary_rows]\ndrift_shares = [drift_results[name][\"drift_share\"] for name in\n                [\"No Drift (Baseline)\"] + list(scenarios.keys())]\n\nfig, ax = plt.subplots(figsize=(10, 5))\ncolors = [OK_COLOR if ds < 0.2 else (PROD_COLOR if ds < 0.5 else ALERT_COLOR)\n          for ds in drift_shares]\nbars = ax.barh(scenario_names, [ds * 100 for ds in drift_shares], color=colors, edgecolor=\"white\")\n\n# Add threshold lines\nax.axvline(20, color=PROD_COLOR, linestyle=\"--\", linewidth=1.5, label=\"Investigate (20%)\")\nax.axvline(50, color=ALERT_COLOR, linestyle=\"--\", linewidth=1.5, label=\"Alert / Retrain (50%)\")\n\n# Labels on bars\nfor bar, ds in zip(bars, drift_shares):\n    ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height() / 2,\n            f\"{ds:.1%}\", va=\"center\", fontsize=10, fontweight=\"bold\")\n\nax.set_xlabel(\"Drift Share (%)\", fontsize=12)\nax.set_title(\"Data Drift Share by Scenario\", fontsize=14, fontweight=\"bold\")\nax.legend(loc=\"lower right\", fontsize=10)\nax.set_xlim(0, 100)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "r3pt14t2n4",
   "source": "# Detailed drifted features for each scenario\nfor name in scenarios.keys():\n    result = drift_results[name]\n    drifted = result[\"drifted_features\"]\n    print(f\"\\n{'='*60}\")\n    print(f\"  {name}\")\n    print(f\"  Drifted: {result['drifted_count']}/{result['total_features']} ({result['drift_share']:.1%})\")\n    print(f\"{'='*60}\")\n    if drifted:\n        for i, feat in enumerate(drifted[:10], 1):\n            print(f\"  {i:2d}. {feat}\")\n        if len(drifted) > 10:\n            print(f\"  ... and {len(drifted) - 10} more\")\n    else:\n        print(\"  (no individually drifted features detected)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "w9b0ay7t14e",
   "source": "# Visualize one key drifted feature per scenario (overlapping histograms)\n# We pick the most interpretable feature for each scenario type\nscenario_highlight_features = {\n    \"Sector Shift\": \"llm_industry\",\n    \"Seniority Shift\": \"llm_decision_maker\",\n    \"Geography Shift\": \"companylocation\",\n    \"Quality Degradation\": \"llm_quality\",\n}\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\naxes = axes.flatten()\n\nfor ax, (scenario_name, feat) in zip(axes, scenario_highlight_features.items()):\n    ref_vals = reference_df[feat]\n    scen_vals = scenarios[scenario_name][feat]\n\n    ax.hist(ref_vals, bins=15, alpha=0.6, color=REF_COLOR, label=\"Reference\", edgecolor=\"white\")\n    ax.hist(scen_vals, bins=15, alpha=0.6, color=PROD_COLOR, label=scenario_name, edgecolor=\"white\")\n\n    ax.set_title(f\"{scenario_name}\\nFeature: {feat}\", fontsize=11, fontweight=\"bold\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Count\")\n    ax.legend(fontsize=9)\n\nfig.suptitle(\"Key Drifted Feature per Scenario — Reference vs Scenario\",\n             fontsize=14, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "kn5vo95873b",
   "source": "### Scenario Interpretations\n\n- **Sector Shift**: The industry-related features (target-encoded `llm_industry`, `industry`, `companyindustry`) drift because the synthetic data introduces sectors (e.g., healthcare, government) that were underrepresented or absent in training. This simulates a business expansion into new verticals.\n\n- **Seniority Shift**: Features like `llm_decision_maker`, `llm_quality`, and the `llm_seniority_*` OHE columns shift drastically. This simulates targeting more junior contacts who have different profile completeness patterns.\n\n- **Geography Shift**: Location-related features (`companylocation`, `location`, `llm_geography_*`) shift as contacts come from different regions. This is common when expanding to new markets.\n\n- **Quality Degradation**: `llm_quality` drops, `has_summary` goes to 0, and `skills_count` decreases. This simulates a scenario where enrichment quality degrades (e.g., LLM API issues, scraping failures).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "3xwc9dzj70m",
   "source": "---\n\n## 5. Prediction Drift Analysis\n\nData drift does not always translate into prediction drift. A model may be robust to certain\ndistributional shifts if the affected features are not strongly weighted.\n\nHere we:\n1. Generate model predictions (probability scores) for the reference and each scenario\n2. Use a **Kolmogorov-Smirnov (KS) test** to compare prediction score distributions\n3. Visualize how predicted lead scores shift across scenarios",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2dyxqr86ms6",
   "source": "# Generate predictions on reference data\n# The data is already in the 47-feature format, so we can predict directly\nref_features = reference_df[feature_columns]\nref_scores = model.predict_proba(ref_features)[:, 1]\n\nprint(f\"Reference prediction scores:\")\nprint(f\"  Mean:   {ref_scores.mean():.3f}\")\nprint(f\"  Median: {np.median(ref_scores):.3f}\")\nprint(f\"  Std:    {ref_scores.std():.3f}\")\nprint(f\"  Range:  [{ref_scores.min():.3f}, {ref_scores.max():.3f}]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "pogqvbiho7q",
   "source": "# Prediction drift analysis for each scenario\nall_scenario_names = [\"No Drift (Baseline)\"] + list(scenarios.keys())\nall_scenario_data = {\"No Drift (Baseline)\": baseline_df, **scenarios}\n\nprediction_results = {}\nfor name in all_scenario_names:\n    df = all_scenario_data[name]\n    scen_features = df[feature_columns]\n    scen_scores = model.predict_proba(scen_features)[:, 1]\n\n    pred_drift = detector.detect_prediction_drift(ref_scores, scen_scores)\n    prediction_results[name] = {\n        \"scores\": scen_scores,\n        **pred_drift,\n    }\n\n# Summary table\npred_rows = []\nfor name in all_scenario_names:\n    r = prediction_results[name]\n    pred_rows.append({\n        \"Scenario\": name,\n        \"KS Statistic\": f\"{r['statistic']:.4f}\",\n        \"p-value\": f\"{r['p_value']:.4e}\" if r[\"p_value\"] < 0.001 else f\"{r['p_value']:.4f}\",\n        \"Drift Detected\": r[\"drift_detected\"],\n        \"Mean Score\": f\"{r['scores'].mean():.3f}\",\n    })\n\npred_summary_df = pd.DataFrame(pred_rows)\nprint(\"=== Prediction Drift Summary (KS Test, threshold=0.05) ===\\n\")\nprint(pred_summary_df.to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ee1awqvlfok",
   "source": "# Overlapping histograms of prediction score distributions\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\naxes = axes.flatten()\n\ndrift_scenario_names = list(scenarios.keys())\nfor ax, name in zip(axes, drift_scenario_names):\n    r = prediction_results[name]\n    color = ALERT_COLOR if r[\"drift_detected\"] else PROD_COLOR\n\n    ax.hist(ref_scores, bins=20, alpha=0.5, color=REF_COLOR, label=\"Reference\", edgecolor=\"white\")\n    ax.hist(r[\"scores\"], bins=20, alpha=0.5, color=color, label=name, edgecolor=\"white\")\n\n    status = \"DRIFT\" if r[\"drift_detected\"] else \"OK\"\n    ax.set_title(f\"{name}\\nKS={r['statistic']:.3f}, p={r['p_value']:.3e} [{status}]\",\n                 fontsize=10, fontweight=\"bold\")\n    ax.set_xlabel(\"Predicted Probability (engagement)\")\n    ax.set_ylabel(\"Count\")\n    ax.legend(fontsize=9)\n\nfig.suptitle(\"Prediction Score Distributions — Reference vs Each Scenario\",\n             fontsize=14, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "sv2mvm59oy",
   "source": "### Key Observations\n\n- **Data drift does not always cause prediction drift.** Some feature shifts may affect columns\n  the model does not weight heavily.\n- **Prediction drift is the stronger signal** for deciding when to retrain, because it directly\n  measures whether the model's output behaviour has changed.\n- Scenarios with both high data drift *and* prediction drift (p < 0.05) are the most concerning\n  and should trigger a retraining investigation.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "sw8wrwgzgc",
   "source": "---\n\n## 6. Real Production Data (Placeholder)\n\nThis section is reserved for analysis on **real production data** from LinkedIn contacts\nprocessed through the live API. In production, new contacts flow through:\n\n1. **Notebook 01** (LLM enrichment) -- raw LinkedIn profiles are enriched with quality/seniority/industry scores\n2. **Feature engineering** -- `preprocess_for_inference()` transforms raw profiles into the 47-feature format\n3. **Prediction API** -- the FastAPI endpoint returns engagement probability scores\n4. **Drift monitoring** -- this analysis is applied to batches of production predictions\n\nOnce real data is available, this section can be populated with live drift analysis.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "91y9jscbaee",
   "source": "# -----------------------------------------------------------------------\n# TODO: Replace with real production data path when available\n# -----------------------------------------------------------------------\n\n# production_data_path = PROJECT_ROOT / \"data\" / \"production\" / \"new_contacts_enriched.csv\"\n# production_df = pd.read_csv(production_data_path)\n#\n# # Run data drift detection\n# prod_drift = detector.detect_data_drift(production_df)\n# print(f\"Production drift share: {prod_drift['drift_share']:.1%}\")\n# print(f\"Drifted features: {prod_drift['drifted_features']}\")\n#\n# # Run prediction drift\n# prod_features = production_df[feature_columns]\n# prod_scores = model.predict_proba(prod_features)[:, 1]\n# prod_pred_drift = detector.detect_prediction_drift(ref_scores, prod_scores)\n# print(f\"Prediction KS statistic: {prod_pred_drift['statistic']:.4f}\")\n# print(f\"Prediction p-value: {prod_pred_drift['p_value']:.4e}\")\n#\n# # Visualize\n# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n# ax1.hist(ref_scores, bins=20, alpha=0.6, color=REF_COLOR, label=\"Reference\")\n# ax1.hist(prod_scores, bins=20, alpha=0.6, color=PROD_COLOR, label=\"Production\")\n# ax1.set_title(\"Prediction Drift — Reference vs Production\")\n# ax1.legend()\n# plt.tight_layout()\n# plt.show()\n\nprint(\"Section 6: Awaiting real production data.\"\n      \" Real data will be preprocessed through notebook 01 (LLM enrichment) first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "r1sxo38igy",
   "source": "---\n\n## 7. Concept Drift — Ground Truth Analysis (Placeholder)\n\n**Concept drift** differs from data drift: the input distribution may remain stable, but the\n**relationship between features and the target** changes. For example, contacts with high\n`llm_quality` may have replied well during the initial campaign but stop engaging over time as\nmarket conditions evolve.\n\nDetecting concept drift requires **ground truth labels** -- actual engagement outcomes for new\ncontacts. This is only possible after a feedback loop is established:\n\n1. The model scores new contacts\n2. Sales outreach is conducted\n3. Engagement outcomes (reply, meeting, no response) are recorded\n4. Predicted vs actual performance is compared\n\nThe metrics to track:\n- **F1 score** (current training benchmark: 0.556)\n- **Precision** (are high-score leads actually engaging?)\n- **Recall** (are we missing engaged leads scored too low?)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "zov1rxvnvc",
   "source": "# -----------------------------------------------------------------------\n# TODO: Load labelled holdout data when feedback loop is established\n# -----------------------------------------------------------------------\n\n# from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n#\n# holdout = pd.read_csv(PROJECT_ROOT / \"data\" / \"production\" / \"labelled_holdout.csv\")\n# holdout_features = holdout[feature_columns]\n# holdout_labels = holdout[\"engaged\"]\n#\n# # Model predictions on holdout\n# holdout_proba = model.predict_proba(holdout_features)[:, 1]\n# holdout_preds = (holdout_proba >= 0.5).astype(int)\n#\n# # Performance metrics\n# f1 = f1_score(holdout_labels, holdout_preds)\n# precision = precision_score(holdout_labels, holdout_preds)\n# recall = recall_score(holdout_labels, holdout_preds)\n#\n# print(f\"Holdout F1:        {f1:.3f}  (training: 0.556)\")\n# print(f\"Holdout Precision: {precision:.3f}\")\n# print(f\"Holdout Recall:    {recall:.3f}\")\n# print()\n# print(classification_report(holdout_labels, holdout_preds, target_names=[\"Not Engaged\", \"Engaged\"]))\n#\n# # Concept drift alarm: significant drop in F1\n# if f1 < 0.45:\n#     print(\"ALERT: F1 dropped > 20% from training baseline. Investigate concept drift.\")\n# elif f1 < 0.50:\n#     print(\"WARNING: F1 is declining. Monitor closely.\")\n# else:\n#     print(\"OK: F1 is within acceptable range of training performance.\")\n\nprint(\"Section 7: Awaiting labelled holdout data for concept drift analysis.\"\n      \" Requires a closed feedback loop (outreach outcomes recorded).\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "kxlppwbh2y",
   "source": "---\n\n## 8. Monitoring Recommendations & Decision Framework\n\n### Summary of Findings\n\n| Scenario | Data Drift Share | Prediction Drift | Action |\n|----------|:----------------:|:-----------------:|--------|\n| No-Drift Baseline | ~2% | No | Monitor (all clear) |\n| Sector Shift | High | Check results above | Investigate if new verticals are permanent |\n| Seniority Shift | High | Check results above | Investigate targeting strategy change |\n| Geography Shift | Moderate-High | Check results above | Assess expansion impact |\n| Quality Degradation | High | Check results above | Alert -- fix data pipeline first |\n\n### Decision Framework: When to Retrain\n\n| Metric | Threshold | Action |\n|--------|-----------|--------|\n| **Drift share** < 20% | Green | Continue monitoring. Normal variation. |\n| **Drift share** 20-50% | Yellow | Investigate which features drifted. If business-relevant, plan retraining. |\n| **Drift share** >= 50% | Red | Alert. Consider immediate retraining or rollback. |\n| **Prediction drift** p-value < 0.01 | Red | Model output distribution has shifted significantly. Evaluate real-world impact. |\n| **F1 on holdout** drops > 20% from baseline (0.556) | Red | Concept drift confirmed. Retrain with new labelled data. |\n\n### Operational Monitoring Stack\n\n- **Streamlit Dashboard**: Real-time drift metrics and prediction distributions.\n  Deployed on Hugging Face Spaces for continuous visibility.\n- **Evidently Reports**: Full HTML drift reports generated via `DriftDetector.generate_report()`.\n  Stored per batch for audit trail.\n- **Retraining Script**: `scripts/export_model.py` re-exports the model with updated data.\n  MLflow tracks experiment versions and model registry manages production deployment.\n\n### Next Steps\n\n1. Deploy monitoring dashboard to production\n2. Establish ground truth feedback loop (record outreach outcomes)\n3. Set up automated drift detection on weekly batches\n4. Define SLA: drift report generated within 24h of new batch ingestion",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4n6i67x5uol",
   "source": "# Final consolidated summary — combined data drift + prediction drift\nprint(\"=\" * 80)\nprint(\"  CONSOLIDATED DRIFT MONITORING REPORT\")\nprint(\"=\" * 80)\nprint(f\"\\n  Reference data:  {reference_df.shape[0]} samples, {len(feature_columns)} features\")\nprint(f\"  Model:           XGBoost (F1=0.556 on validation)\")\nprint(f\"  Drift detector:  Evidently AI (KS test for numeric, chi-squared for categorical)\")\nprint()\n\nfor name in all_scenario_names:\n    dr = drift_results[name]\n    pr = prediction_results[name]\n\n    # Determine alert level\n    if dr[\"drift_share\"] >= 0.5 or pr[\"drift_detected\"]:\n        level = \"RED\"\n    elif dr[\"drift_share\"] >= 0.2:\n        level = \"YELLOW\"\n    else:\n        level = \"GREEN\"\n\n    print(f\"  --- {name} [{level}] ---\")\n    print(f\"    Data drift:       {dr['drifted_count']}/{dr['total_features']} features ({dr['drift_share']:.1%})\")\n    print(f\"    Prediction drift: KS={pr['statistic']:.4f}, p={pr['p_value']:.4e}, detected={pr['drift_detected']}\")\n    print(f\"    Mean score:       {pr['scores'].mean():.3f} (ref: {ref_scores.mean():.3f})\")\n    print()\n\nprint(\"=\" * 80)\nprint(\"  End of drift monitoring analysis.\")\nprint(\"=\" * 80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}