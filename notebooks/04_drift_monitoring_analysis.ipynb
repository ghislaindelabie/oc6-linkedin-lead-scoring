{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5tf714p48ew",
   "metadata": {},
   "source": [
    "# 04 — Data Drift Monitoring Analysis\n",
    "\n",
    "## LinkedIn Lead Scoring Model — MLOps Monitoring\n",
    "\n",
    "This notebook demonstrates **data drift detection** for the LinkedIn Lead Scoring model.\n",
    "It is a key deliverable for evaluating the operational monitoring capabilities of the ML pipeline.\n",
    "\n",
    "**Objectives:**\n",
    "1. Understand the training distribution (reference data)\n",
    "2. Detect data drift using Evidently AI on synthetic scenarios\n",
    "3. Analyse prediction drift when input distributions shift\n",
    "4. Establish monitoring thresholds and a decision framework for retraining\n",
    "\n",
    "**Model context:**\n",
    "- XGBoost classifier trained on 1,910 LemList campaign contacts\n",
    "- 47 features (19 base numeric + 28 one-hot encoded)\n",
    "- Best F1 score on validation: **0.556**\n",
    "- Drift detection powered by [Evidently AI](https://www.evidentlyai.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542v2yo0b99",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ltr2831e9nd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "# Add src to path for project imports\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
    "\n",
    "from linkedin_lead_scoring.monitoring.drift import DriftDetector\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"font.size\"] = 11\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)  # suppress scipy/numpy division warnings\n",
    "\n",
    "# Consistent colour palette\n",
    "REF_COLOR = \"#2196F3\"    # Blue for reference\n",
    "PROD_COLOR = \"#FF9800\"   # Orange for production/scenario\n",
    "ALERT_COLOR = \"#F44336\"  # Red for alerts\n",
    "OK_COLOR = \"#4CAF50\"     # Green for no-drift\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kg3sfwiz8qh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference data, model, and configuration\n",
    "reference_df = pd.read_csv(PROJECT_ROOT / \"data\" / \"reference\" / \"training_reference.csv\")\n",
    "model = joblib.load(PROJECT_ROOT / \"model\" / \"xgboost_model.joblib\")\n",
    "\n",
    "preprocessor_dict = joblib.load(PROJECT_ROOT / \"model\" / \"preprocessor.joblib\")\n",
    "target_encoder = preprocessor_dict[\"target_encoder\"]\n",
    "te_cols = preprocessor_dict[\"te_cols\"]\n",
    "\n",
    "with open(PROJECT_ROOT / \"model\" / \"feature_columns.json\") as f:\n",
    "    feature_columns = json.load(f)\n",
    "\n",
    "with open(PROJECT_ROOT / \"model\" / \"numeric_medians.json\") as f:\n",
    "    numeric_medians = json.load(f)\n",
    "\n",
    "# Import feature engineering for processing raw production data\n",
    "from linkedin_lead_scoring.features import preprocess_for_inference\n",
    "\n",
    "# Initialize drift detector with reference data\n",
    "detector = DriftDetector(reference_data=reference_df)\n",
    "\n",
    "print(f\"Reference data shape: {reference_df.shape}\")\n",
    "print(f\"Number of features:   {len(feature_columns)}\")\n",
    "print(f\"Model type:           {type(model).__name__}\")\n",
    "print(f\"Target encoder cols:  {te_cols}\")\n",
    "print(f\"Numeric medians:      {numeric_medians}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2m01lt6wyla",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Understanding the Training Distribution\n",
    "\n",
    "Before monitoring for drift, we must understand the **reference distribution** -- the statistical\n",
    "profile of features the model was trained on. Any future data that deviates significantly from\n",
    "this baseline may indicate that the model's assumptions no longer hold.\n",
    "\n",
    "We examine:\n",
    "- **Numeric features**: LLM quality scores, engagement scores, company founding year\n",
    "- **One-hot encoded features**: seniority level, geography, business type, company size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n6hjtn5s5o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics of the reference data\n",
    "reference_df.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffvfa021cy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of key numeric features\n",
    "key_numeric = [\"llm_quality\", \"llm_engagement\", \"llm_decision_maker\", \"companyfoundedon\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for ax, col in zip(axes, key_numeric):\n",
    "    reference_df[col].hist(bins=20, ax=ax, color=REF_COLOR, edgecolor=\"white\", alpha=0.85)\n",
    "    ax.set_title(col, fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Value\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.axvline(reference_df[col].median(), color=\"red\", linestyle=\"--\", linewidth=1.2, label=\"Median\")\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "fig.suptitle(\"Reference Data — Key Numeric Feature Distributions\", fontsize=14, fontweight=\"bold\", y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0y5luo3rakkn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical (OHE) feature distributions — grouped by category prefix\n",
    "ohe_groups = {\n",
    "    \"Seniority (llm_seniority)\": [c for c in feature_columns if c.startswith(\"llm_seniority_\")],\n",
    "    \"Geography (llm_geography)\": [c for c in feature_columns if c.startswith(\"llm_geography_\")],\n",
    "    \"Business Type (llm_business_type)\": [c for c in feature_columns if c.startswith(\"llm_business_type_\")],\n",
    "    \"Company Size\": [c for c in feature_columns if c.startswith(\"companysize_\")],\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, (group_name, cols) in zip(axes, ohe_groups.items()):\n",
    "    # Compute proportion of 1s in each OHE column (= category prevalence)\n",
    "    proportions = reference_df[cols].mean()\n",
    "    short_labels = [c.split(\"_\", 2)[-1] if \"_\" in c else c for c in cols]\n",
    "    bars = ax.bar(short_labels, proportions, color=REF_COLOR, edgecolor=\"white\", alpha=0.85)\n",
    "    ax.set_title(group_name, fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Proportion\")\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    ax.tick_params(axis=\"x\", rotation=30)\n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, proportions):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.02,\n",
    "                f\"{val:.0%}\", ha=\"center\", fontsize=9)\n",
    "\n",
    "fig.suptitle(\"Reference Data — Categorical Feature Distributions (OHE Prevalence)\",\n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hfp6ngadmcg",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Drift Detection — Baseline (No Drift)\n",
    "\n",
    "We begin with a **no-drift baseline** scenario: synthetic data generated from the same\n",
    "distribution as the training set. This validates that the drift detector does **not produce\n",
    "false positives** when data is statistically similar to the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257e6b5l1ve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline scenario\n",
    "baseline_df = pd.read_csv(PROJECT_ROOT / \"data\" / \"drift_scenarios\" / \"no_drift_baseline.csv\")\n",
    "print(f\"Baseline data shape: {baseline_df.shape}\")\n",
    "\n",
    "# Run drift detection\n",
    "baseline_result = detector.detect_data_drift(baseline_df)\n",
    "\n",
    "print(\"\\n--- Baseline Drift Detection Results ---\")\n",
    "print(f\"  Drift detected:    {baseline_result['drift_detected']}\")\n",
    "print(f\"  Drifted features:  {baseline_result['drifted_count']} / {baseline_result['total_features']}\")\n",
    "print(f\"  Drift share:       {baseline_result['drift_share']:.1%}\")\n",
    "if baseline_result[\"drifted_features\"]:\n",
    "    print(f\"  Drifted columns:   {baseline_result['drifted_features']}\")\n",
    "else:\n",
    "    print(\"  Drifted columns:   (none)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y56z8atkq2p",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side distributions: reference vs baseline for key features\n",
    "compare_features = [\"llm_quality\", \"llm_engagement\", \"llm_decision_maker\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for ax, col in zip(axes, compare_features):\n",
    "    ax.hist(reference_df[col], bins=15, alpha=0.6, color=REF_COLOR, label=\"Reference\", edgecolor=\"white\")\n",
    "    ax.hist(baseline_df[col], bins=15, alpha=0.6, color=PROD_COLOR, label=\"Baseline\", edgecolor=\"white\")\n",
    "    ax.set_title(col, fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Value\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "fig.suptitle(\"Reference vs No-Drift Baseline — Overlapping Distributions\",\n",
    "             fontsize=14, fontweight=\"bold\", y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mem7x1xfxy",
   "metadata": {},
   "source": [
    "**Interpretation:** The baseline scenario produces very low drift share and no overall drift alarm.\n",
    "The distributions overlap closely with the reference, confirming that the detector is well-calibrated\n",
    "and does not trigger false positives on in-distribution data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4qrgapw9ow",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Data Drift Detection — Synthetic Scenarios\n",
    "\n",
    "We now test four **intentionally drifted** scenarios, each simulating a realistic production shift:\n",
    "\n",
    "| Scenario | Description |\n",
    "|----------|-------------|\n",
    "| **Sector Shift** | New industries (healthcare, government) absent from training data |\n",
    "| **Seniority Shift** | Mostly junior profiles instead of the mid/senior mix in training |\n",
    "| **Geography Shift** | Contacts from different geographic regions |\n",
    "| **Quality Degradation** | Incomplete profiles (missing summaries, low LLM scores) |\n",
    "\n",
    "For each scenario, we run Evidently's drift detection and examine which features are most affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fotzl0bwzu9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all drift scenarios\n",
    "scenario_files = {\n",
    "    \"Sector Shift\": \"drift_sector_shift.csv\",\n",
    "    \"Seniority Shift\": \"drift_seniority_shift.csv\",\n",
    "    \"Geography Shift\": \"drift_geography_shift.csv\",\n",
    "    \"Quality Degradation\": \"drift_quality_degradation.csv\",\n",
    "}\n",
    "\n",
    "scenarios = {}\n",
    "for name, filename in scenario_files.items():\n",
    "    df = pd.read_csv(PROJECT_ROOT / \"data\" / \"drift_scenarios\" / filename)\n",
    "    scenarios[name] = df\n",
    "    print(f\"Loaded '{name}': {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yzm8dj4p7kn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run drift detection on each scenario and collect results\n",
    "drift_results = {}\n",
    "for name, df in scenarios.items():\n",
    "    result = detector.detect_data_drift(df)\n",
    "    drift_results[name] = result\n",
    "\n",
    "# Also include the baseline for comparison\n",
    "drift_results[\"No Drift (Baseline)\"] = baseline_result\n",
    "\n",
    "# Summary table\n",
    "summary_rows = []\n",
    "for name in [\"No Drift (Baseline)\"] + list(scenarios.keys()):\n",
    "    r = drift_results[name]\n",
    "    summary_rows.append({\n",
    "        \"Scenario\": name,\n",
    "        \"Drift Detected\": r[\"drift_detected\"],\n",
    "        \"Drifted Features\": r[\"drifted_count\"],\n",
    "        \"Total Features\": r[\"total_features\"],\n",
    "        \"Drift Share\": f\"{r['drift_share']:.1%}\",\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "print(\"=== Data Drift Detection Summary ===\\n\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obg9pb4cyy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize drift share across scenarios\n",
    "scenario_names = [r[\"Scenario\"] for r in summary_rows]\n",
    "drift_shares = [drift_results[name][\"drift_share\"] for name in\n",
    "                [\"No Drift (Baseline)\"] + list(scenarios.keys())]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = [OK_COLOR if ds < 0.2 else (PROD_COLOR if ds < 0.5 else ALERT_COLOR)\n",
    "          for ds in drift_shares]\n",
    "bars = ax.barh(scenario_names, [ds * 100 for ds in drift_shares], color=colors, edgecolor=\"white\")\n",
    "\n",
    "# Add threshold lines\n",
    "ax.axvline(20, color=PROD_COLOR, linestyle=\"--\", linewidth=1.5, label=\"Investigate (20%)\")\n",
    "ax.axvline(50, color=ALERT_COLOR, linestyle=\"--\", linewidth=1.5, label=\"Alert / Retrain (50%)\")\n",
    "\n",
    "# Labels on bars\n",
    "for bar, ds in zip(bars, drift_shares):\n",
    "    ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height() / 2,\n",
    "            f\"{ds:.1%}\", va=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "ax.set_xlabel(\"Drift Share (%)\", fontsize=12)\n",
    "ax.set_title(\"Data Drift Share by Scenario\", fontsize=14, fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower right\", fontsize=10)\n",
    "ax.set_xlim(0, 100)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r3pt14t2n4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed drifted features for each scenario\n",
    "for name in scenarios.keys():\n",
    "    result = drift_results[name]\n",
    "    drifted = result[\"drifted_features\"]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  {name}\")\n",
    "    print(f\"  Drifted: {result['drifted_count']}/{result['total_features']} ({result['drift_share']:.1%})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    if drifted:\n",
    "        for i, feat in enumerate(drifted[:10], 1):\n",
    "            print(f\"  {i:2d}. {feat}\")\n",
    "        if len(drifted) > 10:\n",
    "            print(f\"  ... and {len(drifted) - 10} more\")\n",
    "    else:\n",
    "        print(\"  (no individually drifted features detected)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w9b0ay7t14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize one key drifted feature per scenario (overlapping histograms)\n",
    "# We pick the most interpretable feature for each scenario type\n",
    "scenario_highlight_features = {\n",
    "    \"Sector Shift\": \"llm_industry\",\n",
    "    \"Seniority Shift\": \"llm_decision_maker\",\n",
    "    \"Geography Shift\": \"companylocation\",\n",
    "    \"Quality Degradation\": \"llm_quality\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, (scenario_name, feat) in zip(axes, scenario_highlight_features.items()):\n",
    "    ref_vals = reference_df[feat]\n",
    "    scen_vals = scenarios[scenario_name][feat]\n",
    "\n",
    "    ax.hist(ref_vals, bins=15, alpha=0.6, color=REF_COLOR, label=\"Reference\", edgecolor=\"white\")\n",
    "    ax.hist(scen_vals, bins=15, alpha=0.6, color=PROD_COLOR, label=scenario_name, edgecolor=\"white\")\n",
    "\n",
    "    ax.set_title(f\"{scenario_name}\\nFeature: {feat}\", fontsize=11, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Value\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "fig.suptitle(\"Key Drifted Feature per Scenario — Reference vs Scenario\",\n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kn5vo95873b",
   "metadata": {},
   "source": [
    "### Scenario Interpretations\n",
    "\n",
    "- **Sector Shift**: The industry-related features (target-encoded `llm_industry`, `industry`, `companyindustry`) drift because the synthetic data introduces sectors (e.g., healthcare, government) that were underrepresented or absent in training. This simulates a business expansion into new verticals.\n",
    "\n",
    "- **Seniority Shift**: Features like `llm_decision_maker`, `llm_quality`, and the `llm_seniority_*` OHE columns shift drastically. This simulates targeting more junior contacts who have different profile completeness patterns.\n",
    "\n",
    "- **Geography Shift**: Location-related features (`companylocation`, `location`, `llm_geography_*`) shift as contacts come from different regions. This is common when expanding to new markets.\n",
    "\n",
    "- **Quality Degradation**: `llm_quality` drops, `has_summary` goes to 0, and `skills_count` decreases. This simulates a scenario where enrichment quality degrades (e.g., LLM API issues, scraping failures)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3xwc9dzj70m",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Prediction Drift Analysis\n",
    "\n",
    "Data drift does not always translate into prediction drift. A model may be robust to certain\n",
    "distributional shifts if the affected features are not strongly weighted.\n",
    "\n",
    "Here we:\n",
    "1. Generate model predictions (probability scores) for the reference and each scenario\n",
    "2. Use a **Kolmogorov-Smirnov (KS) test** to compare prediction score distributions\n",
    "3. Visualize how predicted lead scores shift across scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dyxqr86ms6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on reference data\n",
    "# The data is already in the 47-feature format, so we can predict directly\n",
    "ref_features = reference_df[feature_columns]\n",
    "ref_scores = model.predict_proba(ref_features)[:, 1]\n",
    "\n",
    "print(f\"Reference prediction scores:\")\n",
    "print(f\"  Mean:   {ref_scores.mean():.3f}\")\n",
    "print(f\"  Median: {np.median(ref_scores):.3f}\")\n",
    "print(f\"  Std:    {ref_scores.std():.3f}\")\n",
    "print(f\"  Range:  [{ref_scores.min():.3f}, {ref_scores.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pogqvbiho7q",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction drift analysis for each scenario\n",
    "all_scenario_names = [\"No Drift (Baseline)\"] + list(scenarios.keys())\n",
    "all_scenario_data = {\"No Drift (Baseline)\": baseline_df, **scenarios}\n",
    "\n",
    "prediction_results = {}\n",
    "for name in all_scenario_names:\n",
    "    df = all_scenario_data[name]\n",
    "    scen_features = df[feature_columns]\n",
    "    scen_scores = model.predict_proba(scen_features)[:, 1]\n",
    "\n",
    "    pred_drift = detector.detect_prediction_drift(ref_scores, scen_scores)\n",
    "    prediction_results[name] = {\n",
    "        \"scores\": scen_scores,\n",
    "        **pred_drift,\n",
    "    }\n",
    "\n",
    "# Summary table\n",
    "pred_rows = []\n",
    "for name in all_scenario_names:\n",
    "    r = prediction_results[name]\n",
    "    pred_rows.append({\n",
    "        \"Scenario\": name,\n",
    "        \"KS Statistic\": f\"{r['statistic']:.4f}\",\n",
    "        \"p-value\": f\"{r['p_value']:.4e}\" if r[\"p_value\"] < 0.001 else f\"{r['p_value']:.4f}\",\n",
    "        \"Drift Detected\": r[\"drift_detected\"],\n",
    "        \"Mean Score\": f\"{r['scores'].mean():.3f}\",\n",
    "    })\n",
    "\n",
    "pred_summary_df = pd.DataFrame(pred_rows)\n",
    "print(\"=== Prediction Drift Summary (KS Test, threshold=0.05) ===\\n\")\n",
    "print(pred_summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1awqvlfok",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlapping histograms of prediction score distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "drift_scenario_names = list(scenarios.keys())\n",
    "for ax, name in zip(axes, drift_scenario_names):\n",
    "    r = prediction_results[name]\n",
    "    color = ALERT_COLOR if r[\"drift_detected\"] else PROD_COLOR\n",
    "\n",
    "    ax.hist(ref_scores, bins=20, alpha=0.5, color=REF_COLOR, label=\"Reference\", edgecolor=\"white\")\n",
    "    ax.hist(r[\"scores\"], bins=20, alpha=0.5, color=color, label=name, edgecolor=\"white\")\n",
    "\n",
    "    status = \"DRIFT\" if r[\"drift_detected\"] else \"OK\"\n",
    "    ax.set_title(f\"{name}\\nKS={r['statistic']:.3f}, p={r['p_value']:.3e} [{status}]\",\n",
    "                 fontsize=10, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Predicted Probability (engagement)\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "fig.suptitle(\"Prediction Score Distributions — Reference vs Each Scenario\",\n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sv2mvm59oy",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "- **Data drift does not always cause prediction drift.** Some feature shifts may affect columns\n",
    "  the model does not weight heavily.\n",
    "- **Prediction drift is the stronger signal** for deciding when to retrain, because it directly\n",
    "  measures whether the model's output behaviour has changed.\n",
    "- Scenarios with both high data drift *and* prediction drift (p < 0.05) are the most concerning\n",
    "  and should trigger a retraining investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sw8wrwgzgc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Real Production Data — Drift Analysis\n",
    "\n",
    "We now apply drift detection on **real LinkedIn contacts** that were processed through the same\n",
    "LLM enrichment pipeline (notebook 01) as the training data. These are actual business development\n",
    "contacts — not synthetic data.\n",
    "\n",
    "**Data source:** 151 labelled contacts from 3 LemList campaign exports, enriched via gpt-4o-mini,\n",
    "with known engagement outcomes (47.7% engagement rate).\n",
    "\n",
    "This is the most important section — it shows whether our model faces **real drift** in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91y9jscbaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real production data (20-column enriched format, same as training)\n",
    "production_raw = pd.read_csv(PROJECT_ROOT / \"data\" / \"production\" / \"new_contacts_enriched.csv\")\n",
    "print(f\"Production data: {production_raw.shape}\")\n",
    "print(f\"Engagement rate: {production_raw['engaged'].mean():.1%}\")\n",
    "print(f\"  Engaged: {production_raw['engaged'].sum()}, Not engaged: {(1 - production_raw['engaged']).sum():.0f}\")\n",
    "\n",
    "# Preprocess to 47-feature format (same as training_reference.csv)\n",
    "production_features = preprocess_for_inference(\n",
    "    production_raw.drop(columns=[\"engaged\"]),\n",
    "    target_encoder=target_encoder,\n",
    "    te_cols=te_cols,\n",
    "    feature_columns=feature_columns,\n",
    "    numeric_medians=numeric_medians,\n",
    ")\n",
    "print(f\"\\nProcessed features: {production_features.shape}\")\n",
    "print(f\"Columns match reference: {list(production_features.columns) == feature_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31oatn5iu3m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data drift detection on real production data\n",
    "prod_drift = detector.detect_data_drift(production_features)\n",
    "\n",
    "print(\"=== REAL PRODUCTION DATA — Drift Detection ===\\n\")\n",
    "print(f\"  Drift detected:    {prod_drift['drift_detected']}\")\n",
    "print(f\"  Drift share:       {prod_drift['drift_share']:.1%}\")\n",
    "print(f\"  Drifted features:  {prod_drift['drifted_count']} / {prod_drift['total_features']}\")\n",
    "print(f\"\\n  Drifted feature list:\")\n",
    "for i, feat in enumerate(prod_drift[\"drifted_features\"], 1):\n",
    "    print(f\"    {i:2d}. {feat}\")\n",
    "\n",
    "# Prediction drift on real data\n",
    "prod_scores = model.predict_proba(production_features[feature_columns])[:, 1]\n",
    "prod_pred_drift = detector.detect_prediction_drift(ref_scores, prod_scores)\n",
    "\n",
    "print(f\"\\n=== Prediction Drift ===\")\n",
    "print(f\"  KS Statistic:  {prod_pred_drift['statistic']:.4f}\")\n",
    "print(f\"  p-value:       {prod_pred_drift['p_value']:.4e}\")\n",
    "print(f\"  Drift:         {prod_pred_drift['drift_detected']}\")\n",
    "print(f\"\\n  Production scores: mean={prod_scores.mean():.3f}, std={prod_scores.std():.3f}\")\n",
    "print(f\"  Reference scores:  mean={ref_scores.mean():.3f}, std={ref_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qhqxys6kmnc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize real production drift\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Top row: key numeric feature distributions (reference vs production)\n",
    "top_features = [\"llm_quality\", \"llm_engagement\", \"llm_decision_maker\"]\n",
    "for ax, col in zip(axes[0], top_features):\n",
    "    drifted = col in prod_drift[\"drifted_features\"]\n",
    "    border_color = ALERT_COLOR if drifted else OK_COLOR\n",
    "    ax.hist(reference_df[col], bins=15, alpha=0.5, color=REF_COLOR, label=\"Reference (training)\", edgecolor=\"white\")\n",
    "    ax.hist(production_features[col], bins=15, alpha=0.5, color=PROD_COLOR, label=\"Production (real)\", edgecolor=\"white\")\n",
    "    status = \"DRIFTED\" if drifted else \"OK\"\n",
    "    ax.set_title(f\"{col} [{status}]\", fontsize=11, fontweight=\"bold\", color=border_color)\n",
    "    ax.set_xlabel(\"Value\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "# Bottom left: prediction score distributions\n",
    "ax = axes[1][0]\n",
    "ax.hist(ref_scores, bins=20, alpha=0.5, color=REF_COLOR, label=\"Reference\", edgecolor=\"white\")\n",
    "ax.hist(prod_scores, bins=20, alpha=0.5, color=PROD_COLOR, label=\"Production\", edgecolor=\"white\")\n",
    "drift_label = \"DRIFT\" if prod_pred_drift[\"drift_detected\"] else \"OK\"\n",
    "ax.set_title(f\"Prediction Scores [{drift_label}]\\nKS={prod_pred_drift['statistic']:.3f}, p={prod_pred_drift['p_value']:.3e}\",\n",
    "             fontsize=11, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Engagement Probability\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Bottom middle: drift share comparison (production vs synthetic scenarios)\n",
    "ax = axes[1][1]\n",
    "all_names = [\"Baseline\", \"Production\\n(REAL)\", \"Sector\", \"Seniority\", \"Geography\", \"Quality\"]\n",
    "all_shares = [\n",
    "    baseline_result[\"drift_share\"],\n",
    "    prod_drift[\"drift_share\"],\n",
    "    drift_results[\"Sector Shift\"][\"drift_share\"],\n",
    "    drift_results[\"Seniority Shift\"][\"drift_share\"],\n",
    "    drift_results[\"Geography Shift\"][\"drift_share\"],\n",
    "    drift_results[\"Quality Degradation\"][\"drift_share\"],\n",
    "]\n",
    "colors = [OK_COLOR if s < 0.2 else (PROD_COLOR if s < 0.5 else ALERT_COLOR) for s in all_shares]\n",
    "colors[1] = \"#E91E63\"  # Highlight production in pink\n",
    "bars = ax.bar(all_names, [s * 100 for s in all_shares], color=colors, edgecolor=\"white\")\n",
    "ax.axhline(50, color=ALERT_COLOR, linestyle=\"--\", linewidth=1.5, label=\"Alert threshold (50%)\")\n",
    "ax.axhline(20, color=PROD_COLOR, linestyle=\"--\", linewidth=1.2, label=\"Investigate (20%)\")\n",
    "for bar, s in zip(bars, all_shares):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f\"{s:.0%}\",\n",
    "            ha=\"center\", fontsize=9, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Drift Share (%)\")\n",
    "ax.set_title(\"Drift Share Comparison\", fontsize=11, fontweight=\"bold\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.tick_params(axis=\"x\", rotation=15)\n",
    "\n",
    "# Bottom right: OHE category comparison\n",
    "ax = axes[1][2]\n",
    "ohe_cats = [\"companysize_UNKNOWN\", \"companytype_Privately Held\", \"companytype_UNKNOWN\",\n",
    "            \"llm_seniority_Senior\", \"llm_business_type_others\"]\n",
    "ref_vals = reference_df[ohe_cats].mean()\n",
    "prod_vals = production_features[ohe_cats].mean()\n",
    "x = np.arange(len(ohe_cats))\n",
    "short_labels = [c.split(\"_\", 1)[-1][:20] for c in ohe_cats]\n",
    "ax.bar(x - 0.2, ref_vals, 0.35, color=REF_COLOR, label=\"Reference\", edgecolor=\"white\")\n",
    "ax.bar(x + 0.2, prod_vals, 0.35, color=PROD_COLOR, label=\"Production\", edgecolor=\"white\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(short_labels, fontsize=8, rotation=30, ha=\"right\")\n",
    "ax.set_ylabel(\"Proportion\")\n",
    "ax.set_title(\"Key Category Shifts\", fontsize=11, fontweight=\"bold\")\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "fig.suptitle(\"REAL PRODUCTION DATA — Drift Analysis (151 contacts)\",\n",
    "             fontsize=15, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r1sxo38igy",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Concept Drift — Ground Truth Analysis\n",
    "\n",
    "**Concept drift** differs from data drift: the **relationship between features and the target**\n",
    "may have changed, even if feature distributions are stable.\n",
    "\n",
    "We have **151 labelled contacts** with actual engagement outcomes. This allows us to:\n",
    "1. Compare the model's predictions against ground truth\n",
    "2. Measure real-world F1, precision, recall on unseen data\n",
    "3. Determine whether concept drift has occurred (F1 drop from training baseline of 0.556)\n",
    "\n",
    "This is the strongest evidence for or against retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zov1rxvnvc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EM_DASH = '\\u2014'\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "# Ground truth from the production data\n",
    "actuals = production_raw[\"engaged\"].values\n",
    "predictions_proba = prod_scores\n",
    "predictions_binary = (predictions_proba >= 0.5).astype(int)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(actuals, predictions_binary)\n",
    "precision = precision_score(actuals, predictions_binary)\n",
    "recall = recall_score(actuals, predictions_binary)\n",
    "\n",
    "print(\"=== MODEL PERFORMANCE ON REAL PRODUCTION DATA ===\\n\")\n",
    "print(f\"  Contacts evaluated:  {len(actuals)}\")\n",
    "print(f\"  Actual engaged:      {actuals.sum()} ({actuals.mean():.1%})\")\n",
    "print(f\"  Predicted engaged:   {predictions_binary.sum()} ({predictions_binary.mean():.1%})\")\n",
    "print()\n",
    "print(f\"  {'Metric':<15} {'Training':>10} {'Production':>12} {'Change':>10}\")\n",
    "print(f\"  {'-'*47}\")\n",
    "print(f\"  {'F1 Score':<15} {'0.556':>10} {f1:>12.3f} {(f1 - 0.556) / 0.556:>+10.1%}\")\n",
    "print(f\"  {'Precision':<15} {'---':>10} {precision:>12.3f}\")\n",
    "print(f\"  {'Recall':<15} {'---':>10} {recall:>12.3f}\")\n",
    "\n",
    "# Concept drift assessment\n",
    "f1_drop = (0.556 - f1) / 0.556\n",
    "if f1_drop > 0.20:\n",
    "    print(f\"\\n  ⚠️  ALERT: F1 dropped {f1_drop:.0%} from training baseline.\")\n",
    "    print(f\"     This indicates concept drift. Retraining is recommended.\")\n",
    "elif f1_drop > 0.10:\n",
    "    print(f\"\\n  ⚠️  WARNING: F1 dropped {f1_drop:.0%}. Monitor closely.\")\n",
    "else:\n",
    "    print(f\"\\n  ✅ F1 is within acceptable range of training performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ofbhe4l5lhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix and detailed classification report\n",
    "print(\"=== Confusion Matrix ===\\n\")\n",
    "cm = confusion_matrix(actuals, predictions_binary)\n",
    "print(f\"                    Predicted\")\n",
    "print(f\"                 Not Eng.  Engaged\")\n",
    "print(f\"  Actual Not Eng.  {cm[0][0]:>5}    {cm[0][1]:>5}\")\n",
    "print(f\"  Actual Engaged   {cm[1][0]:>5}    {cm[1][1]:>5}\")\n",
    "print()\n",
    "print(classification_report(actuals, predictions_binary, target_names=[\"Not Engaged\", \"Engaged\"]))\n",
    "\n",
    "# Visualization: confusion matrix + score distribution by actual label\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1,\n",
    "            xticklabels=[\"Not Engaged\", \"Engaged\"],\n",
    "            yticklabels=[\"Not Engaged\", \"Engaged\"])\n",
    "ax1.set_xlabel(\"Predicted\", fontsize=12)\n",
    "ax1.set_ylabel(\"Actual\", fontsize=12)\n",
    "ax1.set_title(\"Confusion Matrix — Production Data\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "# Score distribution by actual label\n",
    "engaged_scores = predictions_proba[actuals == 1]\n",
    "not_engaged_scores = predictions_proba[actuals == 0]\n",
    "ax2.hist(not_engaged_scores, bins=15, alpha=0.6, color=\"#F44336\", label=\"Actually Not Engaged\", edgecolor=\"white\")\n",
    "ax2.hist(engaged_scores, bins=15, alpha=0.6, color=\"#4CAF50\", label=\"Actually Engaged\", edgecolor=\"white\")\n",
    "ax2.axvline(0.5, color=\"black\", linestyle=\"--\", linewidth=2, label=\"Decision threshold (0.5)\")\n",
    "ax2.set_xlabel(\"Predicted Engagement Probability\", fontsize=12)\n",
    "ax2.set_ylabel(\"Count\", fontsize=12)\n",
    "ax2.set_title(\"Score Distribution by Actual Outcome\", fontsize=13, fontweight=\"bold\")\n",
    "ax2.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nKey insight: The model scores actually-engaged contacts at {engaged_scores.mean():.3f} on average,\")\n",
    "print(f\"vs {not_engaged_scores.mean():.3f} for not-engaged. Separation exists but is weaker than training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046d15b91eca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. API Inference Demo\n",
    "\n",
    "To demonstrate the deployed model works end-to-end, we send real production data through\n",
    "the **deployed API** on Hugging Face Spaces.\n",
    "\n",
    "This validates:\n",
    "1. The API accepts real LinkedIn profile data\n",
    "2. Predictions match local inference (model consistency)\n",
    "3. The batch endpoint handles production-scale requests\n",
    "\n",
    "**API endpoint:** `https://ghislaindelabie-oc6-bizdev-ml-api.hf.space/predict/batch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a818ff6865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import time\n",
    "\n",
    "API_URL = \"https://ghislaindelabie-oc6-bizdev-ml-api.hf.space\"\n",
    "BATCH_ENDPOINT = f\"{API_URL}/predict/batch\"\n",
    "\n",
    "# Prepare leads from labelled production data (151 contacts)\n",
    "# The API expects the 19 raw fields (not the 47 processed features)\n",
    "api_fields = [\n",
    "    \"jobtitle\", \"industry\", \"companyindustry\", \"companysize\", \"companytype\",\n",
    "    \"companyfoundedon\", \"location\", \"companylocation\", \"llm_seniority\",\n",
    "    \"llm_quality\", \"llm_engagement\", \"llm_decision_maker\", \"llm_company_fit\",\n",
    "    \"llm_geography\", \"llm_business_type\", \"languages\", \"summary\", \"skills\",\n",
    "    \"llm_industry\",\n",
    "]\n",
    "\n",
    "# Convert production data to API-compatible format\n",
    "leads_for_api = []\n",
    "for _, row in production_raw.iterrows():\n",
    "    lead = {}\n",
    "    for field in api_fields:\n",
    "        if field in row.index and pd.notna(row[field]):\n",
    "            val = row[field]\n",
    "            # Convert numpy types to Python types for JSON serialization\n",
    "            if hasattr(val, \"item\"):\n",
    "                val = val.item()\n",
    "            lead[field] = val\n",
    "    leads_for_api.append(lead)\n",
    "\n",
    "print(f\"Prepared {len(leads_for_api)} leads for API\")\n",
    "print(f\"Sample lead keys: {list(leads_for_api[0].keys())[:10]}...\")\n",
    "\n",
    "# Send batch request\n",
    "try:\n",
    "    print(f\"\\nSending batch to {BATCH_ENDPOINT}...\")\n",
    "    start = time.time()\n",
    "    with httpx.Client(timeout=120.0) as client:\n",
    "        response = client.post(BATCH_ENDPOINT, json={\"leads\": leads_for_api})\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        api_result = response.json()\n",
    "        api_predictions = api_result[\"predictions\"]\n",
    "        api_scores = np.array([p[\"score\"] for p in api_predictions])\n",
    "\n",
    "        print(f\"\\u2713 API responded in {elapsed:.1f}s\")\n",
    "        print(f\"  Total predictions: {api_result['total_count']}\")\n",
    "        print(f\"  Average score:     {api_result['avg_score']:.3f}\")\n",
    "        print(f\"  High engagement:   {api_result['high_engagement_count']}\")\n",
    "        API_AVAILABLE = True\n",
    "    else:\n",
    "        print(f\"\\u2717 API returned status {response.status_code}: {response.text[:200]}\")\n",
    "        API_AVAILABLE = False\n",
    "except Exception as e:\n",
    "    print(f\"\\u2717 API unavailable: {e}\")\n",
    "    print(\"  Continuing with local predictions only.\")\n",
    "    API_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e32c1cbd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare API predictions vs local inference\n",
    "if API_AVAILABLE:\n",
    "    local_scores = prod_scores  # computed in section 6\n",
    "\n",
    "    print(\"=== API vs Local Prediction Comparison ===\\n\")\n",
    "    print(f\"  {'Metric':<25} {'Local':>10} {'API':>10} {'Diff':>10}\")\n",
    "    print(f\"  {'-'*55}\")\n",
    "    print(f\"  {'Mean score':<25} {local_scores.mean():>10.4f} {api_scores.mean():>10.4f} {abs(local_scores.mean() - api_scores.mean()):>10.4f}\")\n",
    "    print(f\"  {'Std score':<25} {local_scores.std():>10.4f} {api_scores.std():>10.4f} {abs(local_scores.std() - api_scores.std()):>10.4f}\")\n",
    "    print(f\"  {'Max abs difference':<25} {np.max(np.abs(local_scores - api_scores)):>10.6f}\")\n",
    "    print(f\"  {'Correlation':<25} {np.corrcoef(local_scores, api_scores)[0,1]:>10.6f}\")\n",
    "\n",
    "    # Scatter plot: local vs API\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    ax1.scatter(local_scores, api_scores, alpha=0.5, s=20, color=\"#2196F3\")\n",
    "    ax1.plot([0, 1], [0, 1], \"r--\", linewidth=1.5, label=\"Perfect agreement\")\n",
    "    ax1.set_xlabel(\"Local Prediction Score\", fontsize=12)\n",
    "    ax1.set_ylabel(\"API Prediction Score\", fontsize=12)\n",
    "    ax1.set_title(\"Local vs API Predictions\", fontsize=13, fontweight=\"bold\")\n",
    "    ax1.legend()\n",
    "    ax1.set_xlim(-0.05, 1.05)\n",
    "    ax1.set_ylim(-0.05, 1.05)\n",
    "\n",
    "    # Histogram of differences\n",
    "    diffs = api_scores - local_scores\n",
    "    ax2.hist(diffs, bins=30, color=\"#FF9800\", edgecolor=\"white\", alpha=0.85)\n",
    "    ax2.axvline(0, color=\"red\", linestyle=\"--\", linewidth=1.5)\n",
    "    ax2.set_xlabel(\"Score Difference (API - Local)\", fontsize=12)\n",
    "    ax2.set_ylabel(\"Count\", fontsize=12)\n",
    "    ax2.set_title(f\"Prediction Differences (max |\\u0394| = {np.max(np.abs(diffs)):.4f})\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Agreement on classification\n",
    "    local_labels = (local_scores >= 0.5).astype(int)\n",
    "    api_labels = (api_scores >= 0.5).astype(int)\n",
    "    agreement = (local_labels == api_labels).mean()\n",
    "    print(f\"\\n  Classification agreement: {agreement:.1%}\")\n",
    "    print(f\"  Disagreements: {(local_labels != api_labels).sum()} out of {len(local_labels)}\")\n",
    "else:\n",
    "    print(\"API was not available. Predictions from local inference (\\u00a76) are used throughout.\")\n",
    "    print(\"In production, the API serves the same model with identical preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989337d5fc13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Unlabelled Production Data — Drift Analysis\n",
    "\n",
    "We now analyse **277 unlabelled contacts** — a separate batch of LinkedIn profiles with\n",
    "**no engagement outcome** (no ground truth). This is the typical production scenario:\n",
    "new contacts arrive, we score them, and we can only detect **data drift** and **prediction\n",
    "drift** (not concept drift, since we have no labels).\n",
    "\n",
    "This section demonstrates monitoring capability on purely unseen, unlabelled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d796287f446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unlabelled production data (needs JSON post-processing for LLM fields)\n",
    "import json as _json\n",
    "\n",
    "unlabelled_raw = pd.read_csv(PROJECT_ROOT / \"data\" / \"production\" / \"unlabelled_contacts_enriched.csv\")\n",
    "print(f\"Unlabelled data: {unlabelled_raw.shape}\")\n",
    "\n",
    "# Post-process: parse JSON strings in LLM numeric fields\n",
    "parse_map = {\n",
    "    \"llm_quality\": \"quality_score\",\n",
    "    \"llm_engagement\": \"engagement_score\",\n",
    "    \"llm_decision_maker\": \"decision_maker_score\",\n",
    "}\n",
    "\n",
    "for col, key in parse_map.items():\n",
    "    if col in unlabelled_raw.columns:\n",
    "        def _extract(val, _key=key):\n",
    "            if pd.isna(val):\n",
    "                return np.nan\n",
    "            if isinstance(val, (int, float)):\n",
    "                return val\n",
    "            try:\n",
    "                parsed = _json.loads(val) if isinstance(val, str) else val\n",
    "                if isinstance(parsed, dict):\n",
    "                    return parsed.get(_key, np.nan)\n",
    "                return float(val)\n",
    "            except (ValueError, _json.JSONDecodeError):\n",
    "                return np.nan\n",
    "        unlabelled_raw[col] = unlabelled_raw[col].apply(_extract)\n",
    "\n",
    "print(f\"  Post-processed LLM fields: {list(parse_map.keys())}\")\n",
    "\n",
    "# Select the 19 modeling columns (same schema as training data, minus 'engaged')\n",
    "model_input_cols = [c for c in production_raw.columns if c != \"engaged\"]\n",
    "unlabelled_for_model = pd.DataFrame()\n",
    "for col in model_input_cols:\n",
    "    if col in unlabelled_raw.columns:\n",
    "        unlabelled_for_model[col] = unlabelled_raw[col].values\n",
    "    else:\n",
    "        unlabelled_for_model[col] = np.nan\n",
    "\n",
    "print(f\"  Prepared for model: {unlabelled_for_model.shape}\")\n",
    "\n",
    "# Preprocess to 47-feature format\n",
    "unlabelled_features = preprocess_for_inference(\n",
    "    unlabelled_for_model,\n",
    "    target_encoder=target_encoder,\n",
    "    te_cols=te_cols,\n",
    "    feature_columns=feature_columns,\n",
    "    numeric_medians=numeric_medians,\n",
    ")\n",
    "print(f\"  Processed features: {unlabelled_features.shape}\")\n",
    "print(f\"  Columns match reference: {list(unlabelled_features.columns) == feature_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cdf9c5a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data drift detection on unlabelled data\n",
    "unlabelled_drift = detector.detect_data_drift(unlabelled_features)\n",
    "\n",
    "print(\"=== UNLABELLED DATA (277 contacts) \\u2014 Drift Detection ===\\n\")\n",
    "print(f\"  Drift detected:    {unlabelled_drift['drift_detected']}\")\n",
    "print(f\"  Drift share:       {unlabelled_drift['drift_share']:.1%}\")\n",
    "print(f\"  Drifted features:  {unlabelled_drift['drifted_count']} / {unlabelled_drift['total_features']}\")\n",
    "\n",
    "if unlabelled_drift[\"drifted_features\"]:\n",
    "    print(f\"\\n  Drifted feature list:\")\n",
    "    for i, feat in enumerate(unlabelled_drift[\"drifted_features\"][:10], 1):\n",
    "        print(f\"    {i:2d}. {feat}\")\n",
    "    remaining = len(unlabelled_drift[\"drifted_features\"]) - 10\n",
    "    if remaining > 0:\n",
    "        print(f\"    ... and {remaining} more\")\n",
    "\n",
    "# Predictions (v1 model)\n",
    "unlabelled_scores = model.predict_proba(unlabelled_features[feature_columns])[:, 1]\n",
    "unlabelled_pred_drift = detector.detect_prediction_drift(ref_scores, unlabelled_scores)\n",
    "\n",
    "print(f\"\\n  Prediction drift:\")\n",
    "print(f\"    KS Statistic: {unlabelled_pred_drift['statistic']:.4f}\")\n",
    "print(f\"    p-value:      {unlabelled_pred_drift['p_value']:.4e}\")\n",
    "print(f\"    Drift:        {unlabelled_pred_drift['drift_detected']}\")\n",
    "print(f\"\\n  Unlabelled scores: mean={unlabelled_scores.mean():.3f}, std={unlabelled_scores.std():.3f}\")\n",
    "print(f\"  Predicted engaged: {(unlabelled_scores >= 0.5).sum()} / {len(unlabelled_scores)} ({(unlabelled_scores >= 0.5).mean():.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0891aaeaddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: compare reference vs labelled production vs unlabelled production\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Prediction score distributions (3-way comparison)\n",
    "ax = axes[0]\n",
    "ax.hist(ref_scores, bins=20, alpha=0.4, color=REF_COLOR, label=\"Reference (training)\", edgecolor=\"white\")\n",
    "ax.hist(prod_scores, bins=20, alpha=0.4, color=PROD_COLOR, label=\"Labelled (151)\", edgecolor=\"white\")\n",
    "ax.hist(unlabelled_scores, bins=20, alpha=0.4, color=\"#9C27B0\", label=\"Unlabelled (277)\", edgecolor=\"white\")\n",
    "ax.set_xlabel(\"Engagement Probability\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Prediction Score Distributions\", fontsize=12, fontweight=\"bold\")\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Drift share comparison\n",
    "ax = axes[1]\n",
    "names = [\"Labelled\\n(151)\", \"Unlabelled\\n(277)\"]\n",
    "shares = [prod_drift[\"drift_share\"], unlabelled_drift[\"drift_share\"]]\n",
    "colors = [ALERT_COLOR if s >= 0.5 else (PROD_COLOR if s >= 0.2 else OK_COLOR) for s in shares]\n",
    "bars = ax.bar(names, [s * 100 for s in shares], color=colors, edgecolor=\"white\", width=0.5)\n",
    "ax.axhline(50, color=ALERT_COLOR, linestyle=\"--\", linewidth=1.5, label=\"Alert (50%)\")\n",
    "ax.axhline(20, color=PROD_COLOR, linestyle=\"--\", linewidth=1.2, label=\"Investigate (20%)\")\n",
    "for bar, s in zip(bars, shares):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1.5,\n",
    "            f\"{s:.0%}\", ha=\"center\", fontsize=11, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Drift Share (%)\")\n",
    "ax.set_title(\"Data Drift: Labelled vs Unlabelled\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_ylim(0, 100)\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Key feature comparison (3-way)\n",
    "ax = axes[2]\n",
    "feat = \"llm_quality\"\n",
    "ax.hist(reference_df[feat], bins=15, alpha=0.4, color=REF_COLOR, label=\"Reference\", edgecolor=\"white\")\n",
    "ax.hist(production_features[feat], bins=15, alpha=0.4, color=PROD_COLOR, label=\"Labelled\", edgecolor=\"white\")\n",
    "ax.hist(unlabelled_features[feat], bins=15, alpha=0.4, color=\"#9C27B0\", label=\"Unlabelled\", edgecolor=\"white\")\n",
    "ax.set_xlabel(feat)\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(f\"Feature Distribution: {feat}\", fontsize=12, fontweight=\"bold\")\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "fig.suptitle(\"Production Data Overview \\u2014 Labelled (151) vs Unlabelled (277)\",\n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Labelled (151):   drift={prod_drift['drift_share']:.0%}, mean_score={prod_scores.mean():.3f}\")\n",
    "print(f\"  Unlabelled (277): drift={unlabelled_drift['drift_share']:.0%}, mean_score={unlabelled_scores.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5717c8f8b4c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Model Retraining — v1 → v1bis → v2\n",
    "\n",
    "The drift analysis confirms significant concept drift. We now retrain in **two controlled steps**\n",
    "to isolate the effect of each improvement:\n",
    "\n",
    "| Version | Data | Rows | Method | What it tests |\n",
    "|---------|------|------|--------|---------------|\n",
    "| **v1** | Original | 303 | Single 80/20 split | Baseline (deployed model) |\n",
    "| **v1bis** | Original | 303 | 5-fold stratified CV | Effect of better evaluation methodology |\n",
    "| **v2** | Original + new labelled | 454 | 5-fold stratified CV | Effect of additional training data |\n",
    "\n",
    "**Why Stratified K-Fold Cross-Validation?**\n",
    "- With only 303 samples, a single test set of ~61 rows has high variance\n",
    "- CV rotates through 5 folds so every sample is used for evaluation exactly once\n",
    "- Target encoding is fitted **inside each fold** to prevent data leakage\n",
    "- Metrics reported as mean ± std across folds (robust, not dependent on a lucky split)\n",
    "- Final production model is then trained on all available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90ee980c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from linkedin_lead_scoring.features import (\n",
    "    LOW_CARDINALITY_CATS, NUMERIC_COLS, TARGET_ENCODE_CATS,\n",
    "    extract_text_features,\n",
    ")\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Best hyperparameters (from Optuna tuning \\u2014 notebook 02)\n",
    "BEST_PARAMS = {\n",
    "    \"n_estimators\": 255,\n",
    "    \"max_depth\": 3,\n",
    "    \"learning_rate\": 0.121,\n",
    "    \"min_child_weight\": 7,\n",
    "    \"subsample\": 0.784,\n",
    "    \"colsample_bytree\": 0.988,\n",
    "    \"gamma\": 3.513,\n",
    "    \"scale_pos_weight\": 2.501,\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "# Load datasets\n",
    "df_original = pd.read_csv(PROJECT_ROOT / \"data\" / \"processed\" / \"linkedin_leads_clean.csv\")\n",
    "df_new = production_raw.copy()  # 151 labelled contacts from section 6\n",
    "\n",
    "# Merge\n",
    "df_merged = pd.concat([df_original, df_new], ignore_index=True)\n",
    "df_merged.to_csv(PROJECT_ROOT / \"data\" / \"processed\" / \"linkedin_leads_merged.csv\", index=False)\n",
    "\n",
    "print(\"=== Dataset Summary ===\")\n",
    "print(f\"  Original (v1):  {len(df_original)} rows \\u2014 engaged: {df_original['engaged'].sum()}, not: {(1 - df_original['engaged']).sum():.0f}\")\n",
    "print(f\"  New data:       {len(df_new)} rows \\u2014 engaged: {df_new['engaged'].sum()}, not: {(1 - df_new['engaged']).sum():.0f}\")\n",
    "print(f\"  Merged (v2):    {len(df_merged)} rows \\u2014 engaged: {df_merged['engaged'].sum()}, not: {(1 - df_merged['engaged']).sum():.0f}\")\n",
    "print(f\"\\n  Engagement rate: {df_original['engaged'].mean():.1%} (v1) \\u2192 {df_merged['engaged'].mean():.1%} (v2)\")\n",
    "print(f\"  Saved merged dataset to data/processed/linkedin_leads_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe35b7c30db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_version_single_split(df, version_name):\n",
    "    \"\"\"Train with single 80/20 split (v1 methodology).\"\"\"\n",
    "    df = df.copy()\n",
    "    df = extract_text_features(df)\n",
    "    y = df.pop(\"engaged\")\n",
    "    X = df\n",
    "\n",
    "    for col in NUMERIC_COLS:\n",
    "        if col in X.columns:\n",
    "            X[col] = X[col].fillna(X[col].median())\n",
    "    for col in LOW_CARDINALITY_CATS + TARGET_ENCODE_CATS:\n",
    "        if col in X.columns:\n",
    "            X[col] = X[col].fillna(\"UNKNOWN\")\n",
    "\n",
    "    ohe_cols = [c for c in LOW_CARDINALITY_CATS if c in X.columns]\n",
    "    if ohe_cols:\n",
    "        X = pd.get_dummies(X, columns=ohe_cols, drop_first=True)\n",
    "        bool_cols = X.select_dtypes(include=\"bool\").columns\n",
    "        X[bool_cols] = X[bool_cols].astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    te_cols_local = [c for c in TARGET_ENCODE_CATS if c in X_train.columns]\n",
    "    encoder = TargetEncoder(cols=te_cols_local, smoothing=2.0)\n",
    "    X_train[te_cols_local] = encoder.fit_transform(X_train[te_cols_local], y_train)\n",
    "    X_test[te_cols_local] = encoder.transform(X_test[te_cols_local])\n",
    "    preprocessor_out = {\"target_encoder\": encoder, \"te_cols\": te_cols_local}\n",
    "\n",
    "    non_numeric = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    if non_numeric:\n",
    "        X_train = X_train.drop(columns=non_numeric)\n",
    "        X_test = X_test.drop(columns=non_numeric)\n",
    "\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "    mdl = XGBClassifier(**BEST_PARAMS)\n",
    "    mdl.fit(X_train, y_train)\n",
    "    y_pred = mdl.predict(X_test)\n",
    "\n",
    "    metrics = {\n",
    "        \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
    "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "    }\n",
    "    metrics_std = {\"f1\": 0.0, \"precision\": 0.0, \"recall\": 0.0}\n",
    "\n",
    "    print(f\"  {version_name} (single split): F1={metrics['f1']:.3f}, Precision={metrics['precision']:.3f}, Recall={metrics['recall']:.3f}\")\n",
    "    print(f\"    Train: {len(X_train)}, Test: {len(X_test)}, Features: {len(X_train.columns)}\")\n",
    "\n",
    "    return mdl, metrics, metrics_std, preprocessor_out, X_train\n",
    "\n",
    "\n",
    "def train_version_cv(df, version_name, n_folds=5):\n",
    "    \"\"\"Train with Stratified K-Fold CV (improved methodology).\n",
    "    \n",
    "    Target encoding fitted INSIDE each fold to prevent leakage.\n",
    "    Final model trained on all data for deployment.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "    df = df.copy()\n",
    "    df = extract_text_features(df)\n",
    "    y = df.pop(\"engaged\")\n",
    "    X = df\n",
    "\n",
    "    for col in NUMERIC_COLS:\n",
    "        if col in X.columns:\n",
    "            X[col] = X[col].fillna(X[col].median())\n",
    "    for col in LOW_CARDINALITY_CATS + TARGET_ENCODE_CATS:\n",
    "        if col in X.columns:\n",
    "            X[col] = X[col].fillna(\"UNKNOWN\")\n",
    "\n",
    "    ohe_cols = [c for c in LOW_CARDINALITY_CATS if c in X.columns]\n",
    "    if ohe_cols:\n",
    "        X = pd.get_dummies(X, columns=ohe_cols, drop_first=True)\n",
    "        bool_cols = X.select_dtypes(include=\"bool\").columns\n",
    "        X[bool_cols] = X[bool_cols].astype(int)\n",
    "\n",
    "    # ---- Stratified K-Fold Cross-Validation ----\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    fold_metrics = {\"f1\": [], \"precision\": [], \"recall\": []}\n",
    "\n",
    "    print(f\"  {version_name}: Running {n_folds}-fold stratified cross-validation...\")\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        X_fold_train = X.iloc[train_idx].copy()\n",
    "        X_fold_val = X.iloc[val_idx].copy()\n",
    "        y_fold_train = y.iloc[train_idx]\n",
    "        y_fold_val = y.iloc[val_idx]\n",
    "\n",
    "        # Target encoding INSIDE the fold (prevents leakage)\n",
    "        te_cols_local = [c for c in TARGET_ENCODE_CATS if c in X_fold_train.columns]\n",
    "        fold_encoder = TargetEncoder(cols=te_cols_local, smoothing=2.0)\n",
    "        X_fold_train[te_cols_local] = fold_encoder.fit_transform(X_fold_train[te_cols_local], y_fold_train)\n",
    "        X_fold_val[te_cols_local] = fold_encoder.transform(X_fold_val[te_cols_local])\n",
    "\n",
    "        non_numeric = X_fold_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "        if non_numeric:\n",
    "            X_fold_train = X_fold_train.drop(columns=non_numeric)\n",
    "            X_fold_val = X_fold_val.drop(columns=non_numeric)\n",
    "\n",
    "        fold_model = XGBClassifier(**BEST_PARAMS)\n",
    "        fold_model.fit(X_fold_train, y_fold_train)\n",
    "        y_fold_pred = fold_model.predict(X_fold_val)\n",
    "\n",
    "        fold_metrics[\"f1\"].append(f1_score(y_fold_val, y_fold_pred, zero_division=0))\n",
    "        fold_metrics[\"precision\"].append(precision_score(y_fold_val, y_fold_pred, zero_division=0))\n",
    "        fold_metrics[\"recall\"].append(recall_score(y_fold_val, y_fold_pred, zero_division=0))\n",
    "\n",
    "    cv_metrics = {k: np.mean(v) for k, v in fold_metrics.items()}\n",
    "    cv_std = {k: np.std(v) for k, v in fold_metrics.items()}\n",
    "    print(f\"    CV F1: {cv_metrics['f1']:.3f} +/- {cv_std['f1']:.3f}\")\n",
    "    print(f\"    CV Precision: {cv_metrics['precision']:.3f} +/- {cv_std['precision']:.3f}\")\n",
    "    print(f\"    CV Recall: {cv_metrics['recall']:.3f} +/- {cv_std['recall']:.3f}\")\n",
    "\n",
    "    # ---- Train final model on ALL data ----\n",
    "    te_cols_all = [c for c in TARGET_ENCODE_CATS if c in X.columns]\n",
    "    final_encoder = TargetEncoder(cols=te_cols_all, smoothing=2.0)\n",
    "    X_encoded = X.copy()\n",
    "    X_encoded[te_cols_all] = final_encoder.fit_transform(X_encoded[te_cols_all], y)\n",
    "    preprocessor_out = {\"target_encoder\": final_encoder, \"te_cols\": te_cols_all}\n",
    "\n",
    "    non_numeric = X_encoded.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    if non_numeric:\n",
    "        X_encoded = X_encoded.drop(columns=non_numeric)\n",
    "\n",
    "    final_model = XGBClassifier(**BEST_PARAMS)\n",
    "    final_model.fit(X_encoded, y)\n",
    "\n",
    "    X_train_out = X_encoded.reset_index(drop=True)\n",
    "    print(f\"    Final model trained on all {len(X_train_out)} rows, {len(X_train_out.columns)} features\")\n",
    "\n",
    "    return final_model, cv_metrics, cv_std, preprocessor_out, X_train_out\n",
    "\n",
    "\n",
    "# ---- Train all three versions ----\n",
    "print(\"=\" * 60)\n",
    "print(\"  STEP 1: v1 (original methodology, single split)\")\n",
    "print(\"=\" * 60)\n",
    "model_v1, metrics_v1, std_v1, prep_v1, X_train_v1 = train_version_single_split(df_original, \"v1\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"  STEP 2: v1bis (same data, improved CV methodology)\")\n",
    "print(\"=\" * 60)\n",
    "model_v1bis, metrics_v1bis, std_v1bis, prep_v1bis, X_train_v1bis = train_version_cv(df_original, \"v1bis\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"  STEP 3: v2 (more data + CV methodology)\")\n",
    "print(\"=\" * 60)\n",
    "model_v2, metrics_v2, std_v2, prep_v2, X_train_v2 = train_version_cv(df_merged, \"v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcf8f22d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log all three versions to MLflow\n",
    "mlflow_uri = f\"file://{PROJECT_ROOT / 'mlruns'}\"\n",
    "mlflow.set_tracking_uri(mlflow_uri)\n",
    "mlflow.set_experiment(\"linkedin-lead-scoring-retraining\")\n",
    "\n",
    "versions = [\n",
    "    (\"v1\",    metrics_v1,    std_v1,    model_v1,    \"original\", 303, \"single-split\"),\n",
    "    (\"v1bis\", metrics_v1bis, std_v1bis, model_v1bis, \"original\", 303, \"5-fold-cv\"),\n",
    "    (\"v2\",    metrics_v2,    std_v2,    model_v2,    \"merged\",   454, \"5-fold-cv\"),\n",
    "]\n",
    "\n",
    "for vtag, vmet, vstd, vmod, dataset, n_samples, method in versions:\n",
    "    with mlflow.start_run(run_name=f\"{vtag}-{n_samples}rows-{method}\"):\n",
    "        mlflow.set_tag(\"model_version\", vtag)\n",
    "        mlflow.set_tag(\"dataset\", dataset)\n",
    "        mlflow.set_tag(\"n_samples\", str(n_samples))\n",
    "        mlflow.set_tag(\"evaluation\", method)\n",
    "        mlflow.log_params(BEST_PARAMS)\n",
    "        mlflow.log_param(\"n_samples\", n_samples)\n",
    "        mlflow.log_param(\"method\", method)\n",
    "        mlflow.log_metrics({\n",
    "            \"f1_mean\": vmet[\"f1\"],\n",
    "            \"f1_std\": vstd[\"f1\"],\n",
    "            \"precision_mean\": vmet[\"precision\"],\n",
    "            \"precision_std\": vstd[\"precision\"],\n",
    "            \"recall_mean\": vmet[\"recall\"],\n",
    "            \"recall_std\": vstd[\"recall\"],\n",
    "        })\n",
    "        mlflow.sklearn.log_model(vmod, artifact_path=\"xgboost-model\",\n",
    "                                 registered_model_name=\"linkedin-lead-scoring-xgboost\")\n",
    "        print(f\"\\u2713 {vtag} logged to MLflow (run: {mlflow.active_run().info.run_id[:8]})\")\n",
    "\n",
    "# Comparison table\n",
    "print(f\"\\n{'='*75}\")\n",
    "print(f\"  MODEL COMPARISON: v1 vs v1bis vs v2\")\n",
    "print(f\"{'='*75}\")\n",
    "print(f\"  {'Metric':<12} {'v1 (split)':>14} {'v1bis (CV)':>14} {'v2 (CV)':>14} {'v1->v1bis':>10} {'v1bis->v2':>10}\")\n",
    "print(f\"  {'-'*74}\")\n",
    "for metric in [\"f1\", \"precision\", \"recall\"]:\n",
    "    v1 = metrics_v1[metric]\n",
    "    v1b = metrics_v1bis[metric]\n",
    "    v1b_s = std_v1bis[metric]\n",
    "    v2_ = metrics_v2[metric]\n",
    "    v2_s = std_v2[metric]\n",
    "    d1 = v1b - v1\n",
    "    d2 = v2_ - v1b\n",
    "    s1 = \"+\" if d1 >= 0 else \"\"\n",
    "    s2 = \"+\" if d2 >= 0 else \"\"\n",
    "    v1_str = f\"{v1:.3f}\"\n",
    "    v1b_str = f\"{v1b:.3f}+/-{v1b_s:.3f}\"\n",
    "    v2_str = f\"{v2_:.3f}+/-{v2_s:.3f}\"\n",
    "    print(f\"  {metric:<12} {v1_str:>14} {v1b_str:>14} {v2_str:>14} {s1}{d1:>9.3f} {s2}{d2:>9.3f}\")\n",
    "print(f\"  {'-'*74}\")\n",
    "print(f\"  {'Data rows':<12} {'303':>14} {'303':>14} {'454':>14}\")\n",
    "print(f\"  {'Method':<12} {'single split':>14} {'5-fold CV':>14} {'5-fold CV':>14}\")\n",
    "\n",
    "print(f\"\\n  Interpretation:\")\n",
    "print(f\"    v1 -> v1bis: effect of better evaluation (CV vs single split)\")\n",
    "print(f\"    v1bis -> v2:  effect of more training data (+151 labelled contacts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447742f0bf15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Post-Retraining Evaluation\n",
    "\n",
    "With v2 trained on 454 rows (including production contacts), we evaluate whether:\n",
    "\n",
    "1. **Drift is reduced** when using v2's reference data (which now includes production patterns)\n",
    "2. **Production F1 improves** compared to v1\n",
    "3. The retraining was worthwhile and the model should be promoted to production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e2bb90c093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save v2 model artifacts to separate directory (v1 untouched)\n",
    "v2_model_dir = PROJECT_ROOT / \"model_v2\"\n",
    "v2_reference_dir = PROJECT_ROOT / \"data\" / \"reference_v2\"\n",
    "\n",
    "v2_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "v2_reference_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "joblib.dump(model_v2, v2_model_dir / \"xgboost_model.joblib\")\n",
    "joblib.dump(prep_v2, v2_model_dir / \"preprocessor.joblib\")\n",
    "\n",
    "v2_feature_columns = list(X_train_v2.columns)\n",
    "with open(v2_model_dir / \"feature_columns.json\", \"w\") as f:\n",
    "    json.dump(v2_feature_columns, f, indent=2)\n",
    "\n",
    "v2_medians = {col: float(X_train_v2[col].median()) for col in NUMERIC_COLS if col in X_train_v2.columns}\n",
    "with open(v2_model_dir / \"numeric_medians.json\", \"w\") as f:\n",
    "    json.dump(v2_medians, f, indent=2)\n",
    "\n",
    "# New reference data from v2 training set\n",
    "v2_reference = X_train_v2.head(100)\n",
    "v2_reference.to_csv(v2_reference_dir / \"training_reference.csv\", index=False)\n",
    "print(f\"\\u2713 v2 artifacts saved to {v2_model_dir}/\")\n",
    "print(f\"  Features: {len(v2_feature_columns)}\")\n",
    "\n",
    "# Create v2 drift detector\n",
    "detector_v2 = DriftDetector(reference_data=v2_reference)\n",
    "\n",
    "# Preprocess production data with v2's pipeline\n",
    "production_features_v2 = preprocess_for_inference(\n",
    "    production_raw.drop(columns=[\"engaged\"]),\n",
    "    target_encoder=prep_v2[\"target_encoder\"],\n",
    "    te_cols=prep_v2[\"te_cols\"],\n",
    "    feature_columns=v2_feature_columns,\n",
    "    numeric_medians=v2_medians,\n",
    ")\n",
    "\n",
    "# Drift detection with v2 reference\n",
    "prod_drift_v2 = detector_v2.detect_data_drift(production_features_v2)\n",
    "\n",
    "print(f\"\\n=== Drift Detection: v1 reference vs v2 reference ===\\n\")\n",
    "print(f\"  {'Metric':<25} {'v1 ref':>12} {'v2 ref':>12}\")\n",
    "print(f\"  {'-'*49}\")\n",
    "print(f\"  {'Drift share':<25} {prod_drift['drift_share']:>11.1%} {prod_drift_v2['drift_share']:>11.1%}\")\n",
    "print(f\"  {'Drifted features':<25} {prod_drift['drifted_count']:>12} {prod_drift_v2['drifted_count']:>12}\")\n",
    "print(f\"  {'Drift detected':<25} {str(prod_drift['drift_detected']):>12} {str(prod_drift_v2['drift_detected']):>12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d8ac08827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production predictions with v1bis and v2\n",
    "# v1bis: same features as v1 (47), use v1's preprocessing for fair comparison\n",
    "v1bis_prod_scores = model_v1bis.predict_proba(production_features[feature_columns])[:, 1]\n",
    "v1bis_prod_binary = (v1bis_prod_scores >= 0.5).astype(int)\n",
    "v1bis_f1 = f1_score(actuals, v1bis_prod_binary)\n",
    "v1bis_precision = precision_score(actuals, v1bis_prod_binary)\n",
    "v1bis_recall = recall_score(actuals, v1bis_prod_binary)\n",
    "\n",
    "# v2: uses its own preprocessing (may differ due to merged data OHE categories)\n",
    "v2_prod_scores = model_v2.predict_proba(production_features_v2[v2_feature_columns])[:, 1]\n",
    "v2_prod_binary = (v2_prod_scores >= 0.5).astype(int)\n",
    "v2_f1 = f1_score(actuals, v2_prod_binary)\n",
    "v2_precision = precision_score(actuals, v2_prod_binary)\n",
    "v2_recall = recall_score(actuals, v2_prod_binary)\n",
    "\n",
    "print(\"=== PRODUCTION PERFORMANCE: v1 vs v1bis vs v2 ===\\n\")\n",
    "print(f\"  {'Metric':<12} {'v1 (deployed)':>14} {'v1bis (CV)':>14} {'v2 (CV+data)':>14}\")\n",
    "print(f\"  {'-'*54}\")\n",
    "print(f\"  {'F1':<12} {f1:>14.3f} {v1bis_f1:>14.3f} {v2_f1:>14.3f}\")\n",
    "print(f\"  {'Precision':<12} {precision:>14.3f} {v1bis_precision:>14.3f} {v2_precision:>14.3f}\")\n",
    "print(f\"  {'Recall':<12} {recall:>14.3f} {v1bis_recall:>14.3f} {v2_recall:>14.3f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Drift share comparison (v1 ref vs v2 ref)\n",
    "ax = axes[0]\n",
    "names_cmp = [\"v1 ref\", \"v2 ref\"]\n",
    "shares_cmp = [prod_drift[\"drift_share\"], prod_drift_v2[\"drift_share\"]]\n",
    "colors_cmp = [ALERT_COLOR if s >= 0.5 else (PROD_COLOR if s >= 0.2 else OK_COLOR) for s in shares_cmp]\n",
    "bars = ax.bar(names_cmp, [s * 100 for s in shares_cmp], color=colors_cmp, edgecolor=\"white\", width=0.4)\n",
    "ax.axhline(50, color=ALERT_COLOR, linestyle=\"--\", linewidth=1.2, label=\"Alert (50%)\")\n",
    "ax.axhline(20, color=PROD_COLOR, linestyle=\"--\", linewidth=1.2, label=\"Investigate (20%)\")\n",
    "for bar, s in zip(bars, shares_cmp):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1.5,\n",
    "            f\"{s:.0%}\", ha=\"center\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Drift Share (%)\")\n",
    "ax.set_title(\"Data Drift: v1 vs v2 Reference\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_ylim(0, 100)\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# F1 comparison (3 versions)\n",
    "ax = axes[1]\n",
    "f1_values = [f1, v1bis_f1, v2_f1]\n",
    "f1_labels = [\"v1\\n(deployed)\", \"v1bis\\n(CV only)\", \"v2\\n(CV + data)\"]\n",
    "f1_colors = [ALERT_COLOR, PROD_COLOR, OK_COLOR]\n",
    "bars = ax.bar(f1_labels, f1_values, color=f1_colors, edgecolor=\"white\", width=0.5)\n",
    "for bar, val in zip(bars, f1_values):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
    "            f\"{val:.3f}\", ha=\"center\", fontsize=11, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"F1 Score on Production Data\")\n",
    "ax.set_title(\"Production F1: Isolating Each Improvement\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_ylim(0, 1.0)\n",
    "\n",
    "# Prediction score distributions (3 versions)\n",
    "ax = axes[2]\n",
    "ax.hist(prod_scores, bins=20, alpha=0.35, color=ALERT_COLOR, label=\"v1 (deployed)\", edgecolor=\"white\")\n",
    "ax.hist(v1bis_prod_scores, bins=20, alpha=0.35, color=PROD_COLOR, label=\"v1bis (CV)\", edgecolor=\"white\")\n",
    "ax.hist(v2_prod_scores, bins=20, alpha=0.35, color=OK_COLOR, label=\"v2 (CV + data)\", edgecolor=\"white\")\n",
    "ax.set_xlabel(\"Engagement Probability\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Prediction Distributions\", fontsize=12, fontweight=\"bold\")\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "fig.suptitle(\"Retraining Impact \\u2014 v1 vs v1bis vs v2 on Production Data (151 contacts)\",\n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  RETRAINING IMPACT SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  v1 -> v1bis (better method):  F1 {f1:.3f} -> {v1bis_f1:.3f} (delta: {v1bis_f1 - f1:+.3f})\")\n",
    "print(f\"  v1bis -> v2 (more data):      F1 {v1bis_f1:.3f} -> {v2_f1:.3f} (delta: {v2_f1 - v1bis_f1:+.3f})\")\n",
    "print(f\"  v1 -> v2 (total improvement): F1 {f1:.3f} -> {v2_f1:.3f} (delta: {v2_f1 - f1:+.3f})\")\n",
    "print(f\"\\n  Drift share: {prod_drift['drift_share']:.0%} (v1 ref) -> {prod_drift_v2['drift_share']:.0%} (v2 ref)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kxlppwbh2y",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Monitoring Recommendations & Decision Framework\n",
    "\n",
    "### Summary of Findings\n",
    "\n",
    "| Scenario | Data Drift Share | Prediction Drift | Action |\n",
    "|----------|:----------------:|:-----------------:|--------|\n",
    "| No-Drift Baseline | ~2% | No | Monitor (all clear) |\n",
    "| Sector Shift | High | Check results above | Investigate if new verticals are permanent |\n",
    "| Seniority Shift | High | Check results above | Investigate targeting strategy change |\n",
    "| Geography Shift | Moderate-High | Check results above | Assess expansion impact |\n",
    "| Quality Degradation | High | Check results above | Alert -- fix data pipeline first |\n",
    "\n",
    "### Decision Framework: When to Retrain\n",
    "\n",
    "| Metric | Threshold | Action |\n",
    "|--------|-----------|--------|\n",
    "| **Drift share** < 20% | Green | Continue monitoring. Normal variation. |\n",
    "| **Drift share** 20-50% | Yellow | Investigate which features drifted. If business-relevant, plan retraining. |\n",
    "| **Drift share** >= 50% | Red | Alert. Consider immediate retraining or rollback. |\n",
    "| **Prediction drift** p-value < 0.01 | Red | Model output distribution has shifted significantly. Evaluate real-world impact. |\n",
    "| **F1 on holdout** drops > 20% from baseline (0.556) | Red | Concept drift confirmed. Retrain with new labelled data. |\n",
    "\n",
    "### Operational Monitoring Stack\n",
    "\n",
    "- **Streamlit Dashboard**: Real-time drift metrics and prediction distributions.\n",
    "  Deployed on Hugging Face Spaces for continuous visibility.\n",
    "- **Evidently Reports**: Full HTML drift reports generated via `DriftDetector.generate_report()`.\n",
    "  Stored per batch for audit trail.\n",
    "- **Retraining Script**: `scripts/export_model.py` re-exports the model with updated data.\n",
    "  MLflow tracks experiment versions and model registry manages production deployment.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Deploy monitoring dashboard to production\n",
    "2. Establish ground truth feedback loop (record outreach outcomes)\n",
    "3. Set up automated drift detection on weekly batches\n",
    "4. Define SLA: drift report generated within 24h of new batch ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4n6i67x5uol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final consolidated summary\n",
    "print(\"=\" * 80)\n",
    "print(\"  CONSOLIDATED MONITORING & RETRAINING REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n  Reference data:  {reference_df.shape[0]} samples, {len(feature_columns)} features\")\n",
    "print(f\"  Model v1:        XGBoost, 303 rows, single split (F1={metrics_v1['f1']:.3f})\")\n",
    "print(f\"  Model v1bis:     XGBoost, 303 rows, 5-fold CV (F1={metrics_v1bis['f1']:.3f}+/-{std_v1bis['f1']:.3f})\")\n",
    "print(f\"  Model v2:        XGBoost, 454 rows, 5-fold CV (F1={metrics_v2['f1']:.3f}+/-{std_v2['f1']:.3f})\")\n",
    "print(f\"  Drift detector:  Evidently AI (KS test numeric, chi-squared categorical)\")\n",
    "print()\n",
    "\n",
    "print(\"  --- SYNTHETIC SCENARIOS (v1 model) ---\")\n",
    "for name in all_scenario_names:\n",
    "    dr = drift_results[name]\n",
    "    pr = prediction_results[name]\n",
    "    if dr[\"drift_share\"] >= 0.5 or pr[\"drift_detected\"]:\n",
    "        level = \"RED\"\n",
    "    elif dr[\"drift_share\"] >= 0.2:\n",
    "        level = \"YELLOW\"\n",
    "    else:\n",
    "        level = \"GREEN\"\n",
    "    print(f\"    {name:<25} [{level:<6}] drift={dr['drift_share']:.1%}, pred_drift={pr['drift_detected']}\")\n",
    "\n",
    "print(f\"\\n  --- REAL PRODUCTION DATA ---\")\n",
    "print(f\"    Labelled (151):    drift={prod_drift['drift_share']:.1%}\")\n",
    "print(f\"      v1 F1={f1:.3f}, v1bis F1={v1bis_f1:.3f}, v2 F1={v2_f1:.3f}\")\n",
    "print(f\"    Unlabelled (277):  drift={unlabelled_drift['drift_share']:.1%}, no ground truth\")\n",
    "\n",
    "print(f\"\\n  --- RETRAINING IMPACT (on production data) ---\")\n",
    "print(f\"    Better method alone (v1->v1bis): F1 {f1:.3f} -> {v1bis_f1:.3f} ({v1bis_f1 - f1:+.3f})\")\n",
    "print(f\"    More data on top   (v1bis->v2):  F1 {v1bis_f1:.3f} -> {v2_f1:.3f} ({v2_f1 - v1bis_f1:+.3f})\")\n",
    "print(f\"    Total improvement  (v1->v2):     F1 {f1:.3f} -> {v2_f1:.3f} ({v2_f1 - f1:+.3f})\")\n",
    "print(f\"    Drift reduction: {prod_drift['drift_share']:.0%} (v1 ref) -> {prod_drift_v2['drift_share']:.0%} (v2 ref)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"  End of drift monitoring & retraining analysis.\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (oc6)",
   "language": "python",
   "name": "oc6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}